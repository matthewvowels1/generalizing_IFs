{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ec6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8ec6e",
   "metadata": {},
   "source": [
    "## Summary of Results:\n",
    "\n",
    "$\\hat Q$ is the outcome estimator, $\\hat G$ is the propensity score estimator. Their respective columns tell us which estimators are use e.g. NN means a neural network was used.\n",
    "\n",
    "'Reduction' is the relative percent error reduction when compared against the plug-in estimator using the outcome model alone. The results are averages over 60 simulations.\n",
    "\n",
    "\n",
    "| Method | $\\hat Q$ | $\\hat G$ | Reduction $\\%$ | Rel. Error $\\%$ |\n",
    "| --- | --- | --- | --- |--- |\n",
    "| Naive | $NN$ | - |- |  4.059|\n",
    "| TMLE | $NN$ | $NN$ | 1.450 | 2.608 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1217b",
   "metadata": {},
   "source": [
    "## Problem Setup:\n",
    "\n",
    "This example is taken from https://arxiv.org/abs/2107.00681 by Hines, Dukes, Diaz-Ordaz, and Vansteelandt (2021) and the empirical evaluation follows https://onlinelibrary.wiley.com/doi/full/10.1002/sim.7628 by Miguel Angel Luque-Fernandez, Michael Schomaker, Bernard Rachet, Mireille E. Schnitzer (2018).\n",
    "\n",
    "\n",
    "The following experiments are very similar to the ones in ATE-NN.ipynb, but this time we will attempt to fit the IF during the training of the NN itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049bda1",
   "metadata": {},
   "source": [
    "## 1. Define the DGP and some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f134282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigm(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def generate_data(N, seed):\n",
    "    np.random.seed(seed=seed)\n",
    "    z1 = np.random.binomial(1, 0.5, (N,1))\n",
    "    z2 = np.random.binomial(1, 0.65, (N,1))\n",
    "    z3 = np.round(np.random.uniform(0, 4, (N,1)),3)\n",
    "    z4 = np.round(np.random.uniform(0, 5, (N,1)),3)\n",
    "    X = np.random.binomial(1, sigm(-0.4 + 0.2*z2 + 0.15*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y1 = np.random.binomial(1, sigm(-1 + 1 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y0 = np.random.binomial(1, sigm(-1 + 0 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y = Y1 * X + Y0 * (1-X)\n",
    "    Z = np.concatenate([z1,z2,z3,z4],1)\n",
    "    return Z, X, Y, Y1, Y0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed127b9",
   "metadata": {},
   "source": [
    "## 2. Define the Neural Network Objects/Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be134faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)     \n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layers_size, output_size, output_type, dropout):\n",
    "        super(QNet, self).__init__()      \n",
    "        \n",
    "        layers = []\n",
    "        layers.extend([nn.Linear(input_size, layers_size), nn.LeakyReLU()])\n",
    "        for i in range(num_layers-1):\n",
    "            layers.extend([nn.Linear(layers_size, layers_size), nn.LeakyReLU(), nn.Dropout(p=dropout)])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        pos_arm = []\n",
    "        pos_arm.extend([nn.Linear(layers_size, layers_size), nn.LeakyReLU()])\n",
    "        pos_arm.extend([nn.Linear(layers_size, output_size)])     \n",
    "        \n",
    "        neg_arm = []\n",
    "        neg_arm.extend([nn.Linear(layers_size, layers_size), nn.LeakyReLU()])\n",
    "        neg_arm.extend([nn.Linear(layers_size, output_size)])    \n",
    "        \n",
    "        if output_type == 'categorical':\n",
    "            pos_arm.append(nn.Sigmoid())\n",
    "            neg_arm.append(nn.Sigmoid())\n",
    "        elif output_type == 'continuous':\n",
    "            pass\n",
    "        self.pos_arm = nn.Sequential(*pos_arm)\n",
    "        self.neg_arm = nn.Sequential(*neg_arm)\n",
    "    \n",
    "        self.net.apply(init_weights) \n",
    "        self.neg_arm.apply(init_weights) \n",
    "        self.pos_arm.apply(init_weights) \n",
    "        \n",
    "        self.epsilon = nn.Parameter(torch.tensor([0.0]), requires_grad=True)\n",
    "\n",
    "\n",
    "    def forward(self, X, Z):\n",
    "        \n",
    "        out = self.net(torch.cat([X,Z],1))\n",
    "        out0 = self.neg_arm(out)\n",
    "        out1 = self.pos_arm(out)\n",
    "        cond = X.bool()\n",
    "        return torch.where(cond, out1, out0)\n",
    "\n",
    "\n",
    "class GNet(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layers_size, output_size, output_type, dropout):\n",
    "        super(GNet, self).__init__()      \n",
    "        \n",
    "        layers = []\n",
    "        layers.extend([nn.Linear(input_size, layers_size), nn.LeakyReLU()])\n",
    "        for i in range(num_layers-1):\n",
    "            layers.extend([nn.Linear(layers_size, layers_size), nn.LeakyReLU(), nn.Dropout(p=dropout)])\n",
    "        layers.extend([nn.Linear(layers_size, output_size)])\n",
    "\n",
    "        if output_type == 'categorical':\n",
    "            layers.append(nn.Sigmoid())\n",
    "        elif output_type == 'continuous':\n",
    "            pass\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(init_weights) \n",
    "        \n",
    "    def forward(self, Z):\n",
    "        return self.net(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7283e3",
   "metadata": {},
   "source": [
    "## 3. Create a Neural Network training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5e57e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_(p):\n",
    "    return torch.log(p / (1 - p))\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, qnet, gnet, iterations=None, outcome_type='categorical', batch_size=None, test_iter=None, lr=None, treg_lr=None):\n",
    "        self.qnet = qnet\n",
    "        self.gnet = gnet\n",
    "        self.iterations = iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.test_iter = test_iter\n",
    "        self.outcome_type = outcome_type\n",
    "        \n",
    "        if lr is not None:\n",
    "            self.treg_optimizer = optim.Adam(qnet.parameters(), lr=treg_lr)\n",
    "            self.q_optimizer = optim.Adam(qnet.parameters(), lr=lr)\n",
    "            self.g_optimizer = optim.Adam(gnet.parameters(), lr=lr)\n",
    "        self.bce_loss = nn.BCELoss(reduction='none')\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def treg(self, x, pred_x, y, pred_y): \n",
    "        pred_x = torch.clip(pred_x, 0.05, 0.99)\n",
    "        h = x / pred_x.detach() - (1 - x) / (1 - pred_x.detach())\n",
    "        y_pert = torch.sigmoid(logit_(p=pred_y) + self.qnet.epsilon * h)\n",
    "        t_reg = torch.sum(\n",
    "                - y * torch.log(y_pert) - (1 - y) * torch.log(1 - y_pert))\n",
    "        return t_reg\n",
    "        \n",
    "    def train(self, x, y, z):\n",
    "        \n",
    "        # create a small validation set\n",
    "        indices = np.arange(len(x))\n",
    "        np.random.shuffle(indices)\n",
    "        val_inds = indices[:len(x)//8]\n",
    "        train_inds = indices[len(x)//8:]\n",
    "        x_val, y_val, z_val = x[val_inds], y[val_inds], z[val_inds]\n",
    "        x_train, y_train, z_train = x[train_inds], y[train_inds], z[train_inds]\n",
    "        \n",
    "        indices = np.arange(len(x_train))\n",
    "        \n",
    "        train_losses_q = []\n",
    "        train_losses_g = []\n",
    "        test_losses_q = []\n",
    "        test_losses_g = []\n",
    "        \n",
    "        for it in range(self.iterations):\n",
    "            inds = np.random.choice(indices, self.batch_size)\n",
    "            x_batch, y_batch, z_batch = x_train[inds], y_train[inds], z_train[inds]\n",
    "            \n",
    "            x_pred = self.gnet(z_batch)\n",
    "            y_pred = self.qnet(x_batch, z_batch)\n",
    "            \n",
    "            if self.outcome_type == 'categorical':\n",
    "                q_loss = self.bce_loss(y_pred, y_batch).mean()\n",
    "            else:\n",
    "                q_loss = self.mse_loss(y_pred, y_batch)\n",
    "                       \n",
    "            weight = torch.tensor([0.7, 0.3])\n",
    "            weight_ = weight[x_batch.data.view(-1).long()].view_as(x_batch)\n",
    "            g_loss = (self.bce_loss(x_pred, x_batch) * weight_).mean()\n",
    "            \n",
    "            treg_loss = self.treg(x_batch, x_pred, y_batch, y_pred)\n",
    "            \n",
    "            treg_loss.backward(retain_graph=True)\n",
    "            q_loss.backward()\n",
    "            g_loss.backward()\n",
    "            \n",
    "            self.treg_optimizer.step()\n",
    "            self.q_optimizer.step()\n",
    "            self.g_optimizer.step()\n",
    "            self.treg_optimizer.zero_grad()\n",
    "            self.q_optimizer.zero_grad()\n",
    "            self.g_optimizer.zero_grad()\n",
    "            \n",
    "            if (it % self.test_iter == 0) or (it == (self.iterations-1)):\n",
    "                self.qnet.eval()\n",
    "                self.gnet.eval()\n",
    "                x_pred = self.gnet(z_train[:800])\n",
    "                y_pred = self.qnet(x_train[:800], z_train[:800])\n",
    "\n",
    "                if self.outcome_type == 'categorical':\n",
    "                    q_loss = self.bce_loss(y_pred, y_train[:800]).mean()\n",
    "                else:\n",
    "                    q_loss = self.mse_loss(y_pred, y_train[:800])\n",
    "                    \n",
    "                g_loss = self.bce_loss(x_pred, x_train[:800]).mean()\n",
    "                train_losses_q.append(q_loss.item())\n",
    "                train_losses_g.append(g_loss.item())\n",
    "                \n",
    "                q_loss_test, g_loss_test, _, _ = self.test(x_val, y_val, z_val)\n",
    "                test_losses_q.append(q_loss_test.item())\n",
    "                test_losses_g.append(g_loss_test.item())\n",
    "#                 print('== Iteration {} =='.format(it))\n",
    "#                 print('Test Loss Q:', q_loss_test.item(), '  Test Loss G:', g_loss_test.item())\n",
    "                \n",
    "                self.qnet.train()\n",
    "                self.gnet.train()\n",
    "        \n",
    "        return train_losses_q, train_losses_g, test_losses_q, test_losses_g\n",
    "    \n",
    "    \n",
    "    def test(self, x, y, z):\n",
    "        self.qnet.eval()\n",
    "        self.gnet.eval()\n",
    "        \n",
    "        x_pred = self.gnet(z)\n",
    "        y_pred = self.qnet(x,z)\n",
    "\n",
    "        if self.outcome_type == 'categorical':\n",
    "            q_loss = self.bce_loss(y_pred, y).mean()\n",
    "        else:\n",
    "            q_loss = self.mse_loss(y_pred, y)\n",
    "            \n",
    "        g_loss = self.bce_loss(x_pred, x).mean()\n",
    "        \n",
    "        \n",
    "        return q_loss, g_loss, x_pred, y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e2e5e6",
   "metadata": {},
   "source": [
    "## 4. Create a hyperparameter tuning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0658fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tuner(object):\n",
    "    def __init__(self, x, y, z, trials, best_params=None):\n",
    "        self.best_params = best_params\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "        self.trials = trials\n",
    "        self.test_iter = 500\n",
    "        self.best_params = best_params\n",
    "        self.qnet = None\n",
    "        self.gnet = None\n",
    "        self.best_model_q = None\n",
    "        self.best_model_g = None\n",
    "        \n",
    "    def tune(self):\n",
    "\n",
    "        output_type_Q = 'categorical'\n",
    "        output_size_Q = 1\n",
    "        output_type_G = 'categorical'\n",
    "        output_size_G = 1\n",
    "        input_size_Q = z.shape[-1] + 1  # we will concatenate the treatment var inside the qnet class\n",
    "        input_size_G = z.shape[-1]\n",
    "\n",
    "        train_loss_q = []\n",
    "        train_loss_g = []\n",
    "        val_loss_q = []\n",
    "        val_loss_g = []\n",
    "        bs_ = []\n",
    "        iters_ = []\n",
    "        lr_ = []\n",
    "        treg_lr_ = []\n",
    "        layers_ = []\n",
    "        dropout_ = []\n",
    "        layer_size_ = []\n",
    "        best_loss = 1e10\n",
    "        j = 0\n",
    "        while j < self.trials:\n",
    "            \n",
    "            try: \n",
    "                # sample hyper params and store the history\n",
    "                bs = np.random.randint(30,120) if self.best_params == None else self.best_params['batch_size']\n",
    "                bs_.append(bs)\n",
    "                iters = np.random.randint(5000,100000) if self.best_params == None else self.best_params['iters']\n",
    "                iters_.append(iters)\n",
    "                lr = np.random.uniform(0.0001, 0.005) if self.best_params == None else self.best_params['lr']\n",
    "                lr_.append(lr)\n",
    "                treg_lr = np.random.uniform(0.0001, 0.005) if self.best_params == None else self.best_params['treg_lr']\n",
    "                treg_lr_.append(treg_lr)\n",
    "                layers = np.random.randint(2, 4) if self.best_params == None else self.best_params['layers']\n",
    "                layers_.append(layers)\n",
    "                dropout = np.random.uniform(0.1,0.4) if self.best_params == None else self.best_params['dropout']\n",
    "                dropout_.append(dropout)\n",
    "                layer_size = np.random.randint(16, 32) if self.best_params == None else self.best_params['layer_size']\n",
    "                layer_size_.append(layer_size)\n",
    "                print('======== Trial {} of {} ========='.format(j, self.trials-1))\n",
    "                print('Batch size', bs, ' Iters', iters, ' Lr', lr, ' tlreg Lr', treg_lr, ' Layers', layers,\n",
    "                     ' Dropout', dropout, ' Layer Size', layer_size)\n",
    "\n",
    "\n",
    "\n",
    "                self.qnet = QNet(input_size=input_size_Q, num_layers=layers,\n",
    "                          layers_size=layer_size, output_size=output_size_Q,\n",
    "                         output_type=output_type_Q, dropout=dropout)\n",
    "\n",
    "                self.gnet = GNet(input_size=input_size_G, num_layers=layers,\n",
    "                          layers_size=layer_size, output_size=output_size_G,\n",
    "                         output_type=output_type_G, dropout=dropout)\n",
    "\n",
    "\n",
    "                trainer = Trainer(qnet=self.qnet, gnet=self.gnet, iterations=iters, outcome_type=output_type_Q,\n",
    "                              batch_size=bs, test_iter=self.test_iter, lr=lr, treg_lr=treg_lr)\n",
    "                train_loss_q_, train_loss_g_, val_loss_q_, val_loss_g_ = trainer.train(self.x,\n",
    "                                                                                      self.y,\n",
    "                                                                                      self.z)\n",
    "                train_loss_q.append(train_loss_q_[-1])\n",
    "                train_loss_g.append(train_loss_g_[-1])\n",
    "                val_loss_q.append(val_loss_q_[-1])\n",
    "                val_loss_g.append(val_loss_g_[-1])\n",
    "\n",
    "                total_val_loss = val_loss_q_[-1] + val_loss_g_[-1]\n",
    "\n",
    "                if total_val_loss < best_loss:\n",
    "                    print('epsilon:', self.qnet.epsilon.item())\n",
    "                    print('old loss:', best_loss)\n",
    "                    print('new loss:', total_val_loss)\n",
    "                    print('best model updated')\n",
    "                    best_loss = total_val_loss\n",
    "                    self.best_model_q = self.qnet\n",
    "                    self.best_model_g = self.gnet\n",
    "                j += 1\n",
    "            except:\n",
    "                print('Error at trial {}:', j)\n",
    "\n",
    "        tuning_dict = {'batch_size': bs_, 'layers':layers_, 'dropout':dropout_,\n",
    "                      'layer_size':layer_size_,'lr':lr_, 'iters':iters_, 'treg_lr':treg_lr_,\n",
    "                      'train_loss_q':train_loss_q, 'train_loss_g':train_loss_g,\n",
    "                      'val_loss_q':val_loss_q, 'val_loss_g':val_loss_g}\n",
    "        \n",
    "        return tuning_dict, self.best_model_q, self.best_model_g\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6fc57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "129eeea7",
   "metadata": {},
   "source": [
    "## 5. Run Hyperparameter Search\n",
    "\n",
    "Now we have everything we need, we can initialize the neural networks, run hyperparameter search to identify the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "475fa678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Trial 0 of 59 =========\n",
      "Batch size 78  Iters 43624  Lr 0.0003733993182942967  tlreg Lr 0.004495198013541288  Layers 2  Dropout 0.3225257649898322  Layer Size 19\n",
      "epsilon: -0.05302128940820694\n",
      "old loss: 10000000000.0\n",
      "new loss: 1.2240602374076843\n",
      "best model updated\n",
      "======== Trial 1 of 59 =========\n",
      "Batch size 115  Iters 16712  Lr 0.002682077660233514  tlreg Lr 0.00420576216575615  Layers 3  Dropout 0.33822910047391863  Layer Size 19\n",
      "======== Trial 2 of 59 =========\n",
      "Batch size 106  Iters 37992  Lr 0.0002518623386282587  tlreg Lr 0.003420400889094646  Layers 3  Dropout 0.12461431079113315  Layer Size 21\n",
      "======== Trial 3 of 59 =========\n",
      "Batch size 66  Iters 60646  Lr 0.0009308256863955367  tlreg Lr 0.0005458583177761694  Layers 3  Dropout 0.16532860546429634  Layer Size 31\n",
      "======== Trial 4 of 59 =========\n",
      "Batch size 66  Iters 74285  Lr 0.00356284820658788  tlreg Lr 0.001050807657046484  Layers 3  Dropout 0.1045584928737894  Layer Size 17\n",
      "======== Trial 5 of 59 =========\n",
      "Batch size 41  Iters 9828  Lr 0.0048158086010098985  tlreg Lr 0.00013241558545377678  Layers 3  Dropout 0.1645543948149314  Layer Size 24\n",
      "======== Trial 6 of 59 =========\n",
      "Batch size 114  Iters 92925  Lr 0.0042919854831173394  tlreg Lr 0.0037524082243857912  Layers 2  Dropout 0.18270200834360967  Layer Size 30\n",
      "Error at trial {}: 6\n",
      "======== Trial 6 of 59 =========\n",
      "Batch size 112  Iters 83660  Lr 0.002646023478818186  tlreg Lr 0.004115471468076054  Layers 2  Dropout 0.21028660135860333  Layer Size 17\n",
      "Error at trial {}: 6\n",
      "======== Trial 6 of 59 =========\n",
      "Batch size 35  Iters 34211  Lr 0.0013895898373133347  tlreg Lr 0.0021998540118786966  Layers 3  Dropout 0.3066209290483919  Layer Size 28\n",
      "======== Trial 7 of 59 =========\n",
      "Batch size 56  Iters 78525  Lr 0.0007533668584327701  tlreg Lr 0.0027263639410421655  Layers 3  Dropout 0.13570440054584448  Layer Size 19\n",
      "Error at trial {}: 7\n",
      "======== Trial 7 of 59 =========\n",
      "Batch size 68  Iters 36093  Lr 0.0008628940758412191  tlreg Lr 0.0015361149834747648  Layers 2  Dropout 0.2877862456141692  Layer Size 20\n",
      "epsilon: -0.03774789720773697\n",
      "old loss: 1.2240602374076843\n",
      "new loss: 1.1856747269630432\n",
      "best model updated\n",
      "======== Trial 8 of 59 =========\n",
      "Batch size 81  Iters 78182  Lr 0.004308389603374283  tlreg Lr 0.002089429498542638  Layers 3  Dropout 0.24866250388439207  Layer Size 28\n",
      "Error at trial {}: 8\n",
      "======== Trial 8 of 59 =========\n",
      "Batch size 41  Iters 86718  Lr 0.0021015660749092854  tlreg Lr 0.004976611715281796  Layers 2  Dropout 0.13338075644544858  Layer Size 27\n",
      "Error at trial {}: 8\n",
      "======== Trial 8 of 59 =========\n",
      "Batch size 40  Iters 30494  Lr 0.0009479070795281125  tlreg Lr 0.00259287944227049  Layers 3  Dropout 0.15109044155640483  Layer Size 17\n",
      "======== Trial 9 of 59 =========\n",
      "Batch size 49  Iters 23944  Lr 0.004600426314121013  tlreg Lr 0.0048279347739559764  Layers 2  Dropout 0.2667490285594105  Layer Size 30\n",
      "======== Trial 10 of 59 =========\n",
      "Batch size 87  Iters 81083  Lr 0.002962498746219101  tlreg Lr 0.004645523853801184  Layers 2  Dropout 0.15583712203216493  Layer Size 24\n",
      "Error at trial {}: 10\n",
      "======== Trial 10 of 59 =========\n",
      "Batch size 78  Iters 20128  Lr 0.0025034407987986774  tlreg Lr 0.0023134113175212784  Layers 2  Dropout 0.28228955639298947  Layer Size 27\n",
      "======== Trial 11 of 59 =========\n",
      "Batch size 76  Iters 83132  Lr 0.004735780239342929  tlreg Lr 0.004687911135456249  Layers 3  Dropout 0.3007486576250742  Layer Size 22\n",
      "Error at trial {}: 11\n",
      "======== Trial 11 of 59 =========\n",
      "Batch size 32  Iters 71913  Lr 0.002152232177917312  tlreg Lr 0.0012501058784153275  Layers 3  Dropout 0.11192321321273531  Layer Size 22\n",
      "======== Trial 12 of 59 =========\n",
      "Batch size 60  Iters 13932  Lr 0.003933430643992343  tlreg Lr 0.0008218335428256925  Layers 2  Dropout 0.31330748289704535  Layer Size 24\n",
      "======== Trial 13 of 59 =========\n",
      "Batch size 70  Iters 74033  Lr 0.004814344096561251  tlreg Lr 0.00015481021243949396  Layers 2  Dropout 0.1994385066482478  Layer Size 22\n",
      "Error at trial {}: 13\n",
      "======== Trial 13 of 59 =========\n",
      "Batch size 44  Iters 96122  Lr 0.001699779650276872  tlreg Lr 0.0031151955623374256  Layers 3  Dropout 0.11969058565725427  Layer Size 28\n",
      "Error at trial {}: 13\n",
      "======== Trial 13 of 59 =========\n",
      "Batch size 76  Iters 96733  Lr 0.003490007755069896  tlreg Lr 0.004071865040562754  Layers 2  Dropout 0.3444930956949579  Layer Size 28\n",
      "Error at trial {}: 13\n",
      "======== Trial 13 of 59 =========\n",
      "Batch size 50  Iters 31149  Lr 0.0031592883638806614  tlreg Lr 0.0034520760312332036  Layers 2  Dropout 0.34100618610281447  Layer Size 16\n",
      "======== Trial 14 of 59 =========\n",
      "Batch size 116  Iters 19053  Lr 0.002117005324297046  tlreg Lr 0.0017747289061595985  Layers 2  Dropout 0.22151993701450637  Layer Size 16\n",
      "======== Trial 15 of 59 =========\n",
      "Batch size 31  Iters 57051  Lr 0.003870548954675611  tlreg Lr 0.0029858698623306677  Layers 3  Dropout 0.3072799737118588  Layer Size 28\n",
      "Error at trial {}: 15\n",
      "======== Trial 15 of 59 =========\n",
      "Batch size 35  Iters 79594  Lr 0.004213020257316781  tlreg Lr 0.002568623233186569  Layers 3  Dropout 0.2674287452847389  Layer Size 25\n",
      "Error at trial {}: 15\n",
      "======== Trial 15 of 59 =========\n",
      "Batch size 54  Iters 8271  Lr 0.004474806218783017  tlreg Lr 0.002115458425144496  Layers 2  Dropout 0.399908615809872  Layer Size 19\n",
      "======== Trial 16 of 59 =========\n",
      "Batch size 88  Iters 90487  Lr 0.002365585732675545  tlreg Lr 0.0034242754062426346  Layers 3  Dropout 0.20347725686260426  Layer Size 23\n",
      "Error at trial {}: 16\n",
      "======== Trial 16 of 59 =========\n",
      "Batch size 101  Iters 44284  Lr 0.0002751150634752086  tlreg Lr 0.0019523626983947249  Layers 2  Dropout 0.34425633107810616  Layer Size 31\n",
      "======== Trial 17 of 59 =========\n",
      "Batch size 41  Iters 56363  Lr 0.0013968323589750482  tlreg Lr 0.002377694853164658  Layers 3  Dropout 0.2871560588531993  Layer Size 26\n",
      "Error at trial {}: 17\n",
      "======== Trial 17 of 59 =========\n",
      "Batch size 93  Iters 42781  Lr 0.00010706412956694131  tlreg Lr 0.0018494522727688408  Layers 2  Dropout 0.21082084663999462  Layer Size 30\n",
      "======== Trial 18 of 59 =========\n",
      "Batch size 83  Iters 48131  Lr 0.0011604384022019367  tlreg Lr 0.001213664066802279  Layers 3  Dropout 0.16880650054340443  Layer Size 17\n",
      "======== Trial 19 of 59 =========\n",
      "Batch size 116  Iters 78893  Lr 0.0012450656316278954  tlreg Lr 0.0015896874808920846  Layers 2  Dropout 0.1709125073073111  Layer Size 17\n",
      "Error at trial {}: 19\n",
      "======== Trial 19 of 59 =========\n",
      "Batch size 73  Iters 15947  Lr 0.0006024713402007313  tlreg Lr 0.0016496853714962638  Layers 3  Dropout 0.13748916274152273  Layer Size 22\n",
      "======== Trial 20 of 59 =========\n",
      "Batch size 92  Iters 99938  Lr 0.004106403791312473  tlreg Lr 0.004216382942948861  Layers 3  Dropout 0.367300850982333  Layer Size 16\n",
      "Error at trial {}: 20\n",
      "======== Trial 20 of 59 =========\n",
      "Batch size 78  Iters 83851  Lr 0.003331486453358096  tlreg Lr 0.0020470515188141898  Layers 2  Dropout 0.1110874380384219  Layer Size 24\n",
      "Error at trial {}: 20\n",
      "======== Trial 20 of 59 =========\n",
      "Batch size 93  Iters 41203  Lr 0.0020235034419054266  tlreg Lr 0.004573247153042589  Layers 2  Dropout 0.3014722389523087  Layer Size 25\n",
      "======== Trial 21 of 59 =========\n",
      "Batch size 86  Iters 37387  Lr 0.0001805833111117229  tlreg Lr 0.002493130777689103  Layers 3  Dropout 0.29325615763289264  Layer Size 30\n",
      "======== Trial 22 of 59 =========\n",
      "Batch size 72  Iters 74072  Lr 0.0008474089244162126  tlreg Lr 0.0033217797238777216  Layers 3  Dropout 0.13808530460038354  Layer Size 16\n",
      "Error at trial {}: 22\n",
      "======== Trial 22 of 59 =========\n",
      "Batch size 41  Iters 68672  Lr 0.002160481403076996  tlreg Lr 0.0023697844060223553  Layers 2  Dropout 0.39295980335934466  Layer Size 31\n",
      "======== Trial 23 of 59 =========\n",
      "Batch size 33  Iters 70514  Lr 0.0012321646647586696  tlreg Lr 0.004927530162357846  Layers 3  Dropout 0.28079793853385016  Layer Size 23\n",
      "Error at trial {}: 23\n",
      "======== Trial 23 of 59 =========\n",
      "Batch size 66  Iters 10604  Lr 0.004039836862749958  tlreg Lr 0.004255294878999507  Layers 3  Dropout 0.2558149263454728  Layer Size 26\n",
      "======== Trial 24 of 59 =========\n",
      "Batch size 88  Iters 12755  Lr 0.0010886119945622153  tlreg Lr 0.002410067462519999  Layers 3  Dropout 0.1252825522840773  Layer Size 25\n",
      "======== Trial 25 of 59 =========\n",
      "Batch size 87  Iters 34057  Lr 0.0025026202911613287  tlreg Lr 0.001303685143350728  Layers 2  Dropout 0.39923320357751  Layer Size 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Trial 26 of 59 =========\n",
      "Batch size 82  Iters 44086  Lr 0.0004173980574009601  tlreg Lr 0.002225389107623747  Layers 3  Dropout 0.3555496966066629  Layer Size 21\n",
      "======== Trial 27 of 59 =========\n",
      "Batch size 75  Iters 32394  Lr 0.0004673959770171352  tlreg Lr 0.004183748692291637  Layers 3  Dropout 0.197529351017468  Layer Size 28\n",
      "Error at trial {}: 27\n",
      "======== Trial 27 of 59 =========\n",
      "Batch size 83  Iters 9078  Lr 0.0010920617380885882  tlreg Lr 0.004397415580614684  Layers 2  Dropout 0.21131258286108978  Layer Size 19\n",
      "======== Trial 28 of 59 =========\n",
      "Batch size 73  Iters 36453  Lr 0.0024159189565179573  tlreg Lr 0.004745579382886737  Layers 2  Dropout 0.13434110266876273  Layer Size 16\n",
      "======== Trial 29 of 59 =========\n",
      "Batch size 59  Iters 96293  Lr 0.002242574385689902  tlreg Lr 0.00030159591941358985  Layers 2  Dropout 0.19071508603954257  Layer Size 16\n",
      "======== Trial 30 of 59 =========\n",
      "Batch size 36  Iters 58343  Lr 0.0031387193092859283  tlreg Lr 0.002774892916405316  Layers 2  Dropout 0.12891424775825344  Layer Size 27\n",
      "Error at trial {}: 30\n",
      "======== Trial 30 of 59 =========\n",
      "Batch size 96  Iters 41982  Lr 0.0021040370751510508  tlreg Lr 0.0028776784497805247  Layers 3  Dropout 0.15760555048553643  Layer Size 19\n",
      "======== Trial 31 of 59 =========\n",
      "Batch size 100  Iters 76157  Lr 0.0017009478638401923  tlreg Lr 0.0030876060672616035  Layers 2  Dropout 0.35274334061711665  Layer Size 28\n",
      "Error at trial {}: 31\n",
      "======== Trial 31 of 59 =========\n",
      "Batch size 90  Iters 88311  Lr 0.0010947824743683997  tlreg Lr 0.0015463954735552487  Layers 3  Dropout 0.18109351896493536  Layer Size 24\n",
      "Error at trial {}: 31\n",
      "======== Trial 31 of 59 =========\n",
      "Batch size 74  Iters 24351  Lr 0.002659714058256209  tlreg Lr 0.0023884022722248824  Layers 3  Dropout 0.25193681986449734  Layer Size 29\n",
      "======== Trial 32 of 59 =========\n",
      "Batch size 95  Iters 87429  Lr 0.003055978606879113  tlreg Lr 0.0019098575144451356  Layers 2  Dropout 0.1407729894074269  Layer Size 20\n",
      "Error at trial {}: 32\n",
      "======== Trial 32 of 59 =========\n",
      "Batch size 86  Iters 67207  Lr 0.0010473447141886607  tlreg Lr 0.002350313221634349  Layers 3  Dropout 0.2791294971694221  Layer Size 18\n",
      "Error at trial {}: 32\n",
      "======== Trial 32 of 59 =========\n",
      "Batch size 81  Iters 43495  Lr 0.003508489021418073  tlreg Lr 0.003653526956403684  Layers 3  Dropout 0.16372504410794286  Layer Size 29\n",
      "Error at trial {}: 32\n",
      "======== Trial 32 of 59 =========\n",
      "Batch size 36  Iters 33978  Lr 0.0008209944199169014  tlreg Lr 0.0014846843720113785  Layers 3  Dropout 0.29609860827179224  Layer Size 19\n",
      "======== Trial 33 of 59 =========\n",
      "Batch size 82  Iters 36819  Lr 0.0005554575125772699  tlreg Lr 0.00279196137304477  Layers 3  Dropout 0.23889036451913867  Layer Size 19\n",
      "======== Trial 34 of 59 =========\n",
      "Batch size 111  Iters 36061  Lr 0.00018313248132245636  tlreg Lr 0.0031696463041839228  Layers 2  Dropout 0.17544146544233963  Layer Size 29\n",
      "Error at trial {}: 34\n",
      "======== Trial 34 of 59 =========\n",
      "Batch size 108  Iters 61841  Lr 0.0021657463473294027  tlreg Lr 0.004527181291827872  Layers 2  Dropout 0.3680352134557714  Layer Size 17\n",
      "Error at trial {}: 34\n",
      "======== Trial 34 of 59 =========\n",
      "Batch size 86  Iters 75731  Lr 0.0038099366440810254  tlreg Lr 0.00021317575383524316  Layers 2  Dropout 0.1091451589223989  Layer Size 23\n",
      "Error at trial {}: 34\n",
      "======== Trial 34 of 59 =========\n",
      "Batch size 96  Iters 26254  Lr 0.0006480736832225471  tlreg Lr 0.0009637775148890034  Layers 3  Dropout 0.34831303220640664  Layer Size 17\n",
      "======== Trial 35 of 59 =========\n",
      "Batch size 57  Iters 72159  Lr 0.0041944988348668905  tlreg Lr 0.004325790356635249  Layers 2  Dropout 0.20380145033369185  Layer Size 30\n",
      "Error at trial {}: 35\n",
      "======== Trial 35 of 59 =========\n",
      "Batch size 112  Iters 13915  Lr 0.0033560240121510067  tlreg Lr 0.0023401032329739627  Layers 2  Dropout 0.367114596931178  Layer Size 21\n",
      "======== Trial 36 of 59 =========\n",
      "Batch size 30  Iters 49299  Lr 0.0006863883002187918  tlreg Lr 0.002062210283265581  Layers 3  Dropout 0.30959214085264414  Layer Size 17\n",
      "======== Trial 37 of 59 =========\n",
      "Batch size 72  Iters 82626  Lr 0.0008314510864185984  tlreg Lr 0.0012779216217475914  Layers 2  Dropout 0.23640695123980568  Layer Size 26\n",
      "Error at trial {}: 37\n",
      "======== Trial 37 of 59 =========\n",
      "Batch size 49  Iters 55843  Lr 0.0032215931542703244  tlreg Lr 0.004666794357255035  Layers 3  Dropout 0.1092415326139502  Layer Size 18\n",
      "Error at trial {}: 37\n",
      "======== Trial 37 of 59 =========\n",
      "Batch size 58  Iters 92883  Lr 0.0009769356692597652  tlreg Lr 0.003638426484231214  Layers 2  Dropout 0.3878251950959263  Layer Size 16\n",
      "======== Trial 38 of 59 =========\n",
      "Batch size 116  Iters 92207  Lr 0.003551539885795402  tlreg Lr 0.0034737199887380156  Layers 3  Dropout 0.18779068042805752  Layer Size 27\n",
      "Error at trial {}: 38\n",
      "======== Trial 38 of 59 =========\n",
      "Batch size 77  Iters 68860  Lr 0.003120876853509084  tlreg Lr 0.0030224980746745393  Layers 2  Dropout 0.3623286179179802  Layer Size 28\n",
      "Error at trial {}: 38\n",
      "======== Trial 38 of 59 =========\n",
      "Batch size 110  Iters 43272  Lr 0.0037433186711021385  tlreg Lr 0.0020514682390108696  Layers 2  Dropout 0.30329856057945603  Layer Size 24\n",
      "Error at trial {}: 38\n",
      "======== Trial 38 of 59 =========\n",
      "Batch size 44  Iters 39786  Lr 0.004068698834311912  tlreg Lr 0.00185657945063696  Layers 3  Dropout 0.1528615310683505  Layer Size 18\n",
      "======== Trial 39 of 59 =========\n",
      "Batch size 97  Iters 90636  Lr 0.003967176456841415  tlreg Lr 0.0005336413289168516  Layers 3  Dropout 0.29601994522716213  Layer Size 23\n",
      "Error at trial {}: 39\n",
      "======== Trial 39 of 59 =========\n",
      "Batch size 98  Iters 13239  Lr 0.0045204596460467735  tlreg Lr 0.0015539147210826998  Layers 2  Dropout 0.21826273387163397  Layer Size 18\n",
      "epsilon: -0.06078452989459038\n",
      "old loss: 1.1856747269630432\n",
      "new loss: 1.1848841905593872\n",
      "best model updated\n",
      "======== Trial 40 of 59 =========\n",
      "Batch size 80  Iters 96860  Lr 0.001343184365972972  tlreg Lr 0.004057081523041473  Layers 2  Dropout 0.3356557826663468  Layer Size 20\n",
      "======== Trial 41 of 59 =========\n",
      "Batch size 77  Iters 21785  Lr 0.0005593345733138055  tlreg Lr 0.0019730637794437415  Layers 3  Dropout 0.33588830453596774  Layer Size 28\n",
      "======== Trial 42 of 59 =========\n",
      "Batch size 91  Iters 88144  Lr 0.0020535762259911126  tlreg Lr 0.004031846390415705  Layers 3  Dropout 0.34456441872284216  Layer Size 23\n",
      "Error at trial {}: 42\n",
      "======== Trial 42 of 59 =========\n",
      "Batch size 96  Iters 47877  Lr 0.0019601011982681264  tlreg Lr 0.0011563473331923522  Layers 3  Dropout 0.3182833371802255  Layer Size 21\n",
      "======== Trial 43 of 59 =========\n",
      "Batch size 50  Iters 21925  Lr 0.0033195979035873383  tlreg Lr 0.0031687357979177234  Layers 2  Dropout 0.34089138046687417  Layer Size 16\n",
      "======== Trial 44 of 59 =========\n",
      "Batch size 37  Iters 46269  Lr 0.0011700207606275938  tlreg Lr 0.0003471930705887689  Layers 3  Dropout 0.3761694982100321  Layer Size 26\n",
      "======== Trial 45 of 59 =========\n",
      "Batch size 72  Iters 76702  Lr 0.0008478970619732138  tlreg Lr 0.0018628438800471717  Layers 3  Dropout 0.38456780593881823  Layer Size 22\n",
      "======== Trial 46 of 59 =========\n",
      "Batch size 55  Iters 68738  Lr 0.0006748716932120147  tlreg Lr 0.002715353050086925  Layers 2  Dropout 0.21726673566033516  Layer Size 23\n",
      "Error at trial {}: 46\n",
      "======== Trial 46 of 59 =========\n",
      "Batch size 109  Iters 23263  Lr 0.003981490735291276  tlreg Lr 0.004243744245352417  Layers 3  Dropout 0.3059297570960563  Layer Size 16\n",
      "======== Trial 47 of 59 =========\n",
      "Batch size 70  Iters 65005  Lr 0.002727943100408306  tlreg Lr 0.0003548236341623637  Layers 3  Dropout 0.38809987636120025  Layer Size 29\n",
      "======== Trial 48 of 59 =========\n",
      "Batch size 36  Iters 83410  Lr 0.00019046748341691692  tlreg Lr 0.0008332332856566539  Layers 2  Dropout 0.2733050357112652  Layer Size 21\n",
      "======== Trial 49 of 59 =========\n",
      "Batch size 93  Iters 6239  Lr 0.0005949253253290969  tlreg Lr 0.003905478764377925  Layers 3  Dropout 0.23127849117030563  Layer Size 19\n",
      "======== Trial 50 of 59 =========\n",
      "Batch size 111  Iters 82231  Lr 0.004553086902432247  tlreg Lr 0.0005020506866569087  Layers 3  Dropout 0.22539244502349823  Layer Size 25\n",
      "Error at trial {}: 50\n",
      "======== Trial 50 of 59 =========\n",
      "Batch size 103  Iters 62175  Lr 0.00020786203870198003  tlreg Lr 0.0008747984444353773  Layers 2  Dropout 0.36492158146047016  Layer Size 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Trial 51 of 59 =========\n",
      "Batch size 53  Iters 67419  Lr 0.001817597188798792  tlreg Lr 0.003978974173343584  Layers 3  Dropout 0.23900659020376588  Layer Size 27\n",
      "Error at trial {}: 51\n",
      "======== Trial 51 of 59 =========\n",
      "Batch size 89  Iters 30167  Lr 0.003385654400760559  tlreg Lr 0.0016584953996469861  Layers 3  Dropout 0.26142818419802905  Layer Size 20\n",
      "======== Trial 52 of 59 =========\n",
      "Batch size 66  Iters 60194  Lr 0.001607084061873295  tlreg Lr 0.0002746588151037081  Layers 2  Dropout 0.3704963698643874  Layer Size 17\n",
      "======== Trial 53 of 59 =========\n",
      "Batch size 103  Iters 36289  Lr 0.0024182545270691434  tlreg Lr 0.0004696498500233234  Layers 2  Dropout 0.26031168874859156  Layer Size 19\n",
      "======== Trial 54 of 59 =========\n",
      "Batch size 107  Iters 31501  Lr 0.004405111097911275  tlreg Lr 0.0031031193099003607  Layers 3  Dropout 0.37016295267981214  Layer Size 19\n",
      "======== Trial 55 of 59 =========\n",
      "Batch size 68  Iters 17418  Lr 0.0010031635190881683  tlreg Lr 0.0013432215529990464  Layers 2  Dropout 0.19647375425279506  Layer Size 26\n",
      "======== Trial 56 of 59 =========\n",
      "Batch size 59  Iters 73559  Lr 0.0028569563863496353  tlreg Lr 0.001526297516520008  Layers 3  Dropout 0.13233202452130452  Layer Size 31\n",
      "Error at trial {}: 56\n",
      "======== Trial 56 of 59 =========\n",
      "Batch size 40  Iters 94433  Lr 0.0017479355778052981  tlreg Lr 0.0007285988886900095  Layers 2  Dropout 0.16029372533361647  Layer Size 27\n",
      "Error at trial {}: 56\n",
      "======== Trial 56 of 59 =========\n",
      "Batch size 94  Iters 40951  Lr 0.003802417754313909  tlreg Lr 0.003686692329427612  Layers 3  Dropout 0.17342794688166896  Layer Size 20\n",
      "Error at trial {}: 56\n",
      "======== Trial 56 of 59 =========\n",
      "Batch size 115  Iters 33270  Lr 0.002021659979102117  tlreg Lr 0.0017538304736386672  Layers 3  Dropout 0.16644870319941657  Layer Size 21\n",
      "======== Trial 57 of 59 =========\n",
      "Batch size 101  Iters 99677  Lr 0.003476436089982839  tlreg Lr 0.004793618626396772  Layers 3  Dropout 0.35768556595178524  Layer Size 27\n",
      "Error at trial {}: 57\n",
      "======== Trial 57 of 59 =========\n",
      "Batch size 82  Iters 5504  Lr 0.0027564501373283632  tlreg Lr 0.0036357344539937365  Layers 3  Dropout 0.15894747381001745  Layer Size 23\n",
      "======== Trial 58 of 59 =========\n",
      "Batch size 90  Iters 15096  Lr 0.00034229916421183164  tlreg Lr 0.0012772559886493416  Layers 3  Dropout 0.2677207650825209  Layer Size 16\n",
      "======== Trial 59 of 59 =========\n",
      "Batch size 39  Iters 38392  Lr 0.0042344594658650266  tlreg Lr 0.0042770726294387865  Layers 2  Dropout 0.24258971426605122  Layer Size 25\n",
      "Error at trial {}: 59\n",
      "======== Trial 59 of 59 =========\n",
      "Batch size 87  Iters 20767  Lr 0.003591603229100544  tlreg Lr 0.0037770056512238396  Layers 2  Dropout 0.2780675537461613  Layer Size 23\n"
     ]
    }
   ],
   "source": [
    "# First establish ground truth treatment effect:\n",
    "N = 5000000\n",
    "Z, x, y, Y1, Y0 = generate_data(N, seed=0)\n",
    "true_psi = (Y1-Y0).mean()\n",
    "\n",
    "\n",
    "# Set some params\n",
    "N = 10000\n",
    "seed = 0\n",
    "num_tuning_trials = 60\n",
    "\n",
    "# data generation:\n",
    "z, x, y, _, _ = generate_data(N, 0)\n",
    "x = torch.tensor(x).type(torch.float32)\n",
    "z = torch.tensor(z).type(torch.float32)\n",
    "y = torch.tensor(y).type(torch.float32)\n",
    "    \n",
    "tuner = Tuner(x=x,y=y,z=z,trials=num_tuning_trials)\n",
    "tuning_history, best_q, best_g = tuner.tune()\n",
    "\n",
    "total_losses = np.asarray(tuning_history['val_loss_g']) + np.asarray(tuning_history['val_loss_q'])\n",
    "best_index = np.argmin(total_losses)\n",
    "\n",
    "best_params = {}\n",
    "for key in tuning_history.keys():\n",
    "    best_params[key] = tuning_history[key][best_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4090404",
   "metadata": {},
   "source": [
    "## 6. Run Simulation\n",
    "\n",
    "Now we have the best hyperparameters, we will run the simulations accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9709b014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 72, 'layers': 3, 'dropout': 0.13808530460038354, 'layer_size': 16, 'lr': 0.0008474089244162126, 'iters': 74072, 'treg_lr': 0.0033217797238777216, 'train_loss_q': 0.5656583309173584, 'train_loss_g': 0.665006160736084, 'val_loss_q': 0.5169773697853088, 'val_loss_g': 0.6679068207740784}\n",
      "=====================RUN 0===================\n",
      "Problem with run 0\n",
      "=====================RUN 0===================\n",
      "=====================RUN 1===================\n",
      "=====================RUN 2===================\n",
      "=====================RUN 3===================\n",
      "=====================RUN 4===================\n",
      "=====================RUN 5===================\n",
      "=====================RUN 6===================\n",
      "Problem with run 6\n",
      "=====================RUN 6===================\n",
      "=====================RUN 7===================\n",
      "=====================RUN 8===================\n",
      "=====================RUN 9===================\n",
      "Problem with run 9\n",
      "=====================RUN 9===================\n",
      "=====================RUN 10===================\n",
      "=====================RUN 11===================\n",
      "Problem with run 11\n",
      "=====================RUN 11===================\n",
      "=====================RUN 12===================\n",
      "Problem with run 12\n",
      "=====================RUN 12===================\n",
      "=====================RUN 13===================\n",
      "=====================RUN 14===================\n",
      "Problem with run 14\n",
      "=====================RUN 14===================\n",
      "Problem with run 14\n",
      "=====================RUN 14===================\n",
      "=====================RUN 15===================\n",
      "=====================RUN 16===================\n",
      "=====================RUN 17===================\n",
      "=====================RUN 18===================\n",
      "Problem with run 18\n",
      "=====================RUN 18===================\n",
      "=====================RUN 19===================\n",
      "=====================RUN 20===================\n",
      "=====================RUN 21===================\n",
      "Problem with run 21\n",
      "=====================RUN 21===================\n",
      "Problem with run 21\n",
      "=====================RUN 21===================\n",
      "Problem with run 21\n",
      "=====================RUN 21===================\n",
      "=====================RUN 22===================\n",
      "Problem with run 22\n",
      "=====================RUN 22===================\n",
      "=====================RUN 23===================\n",
      "=====================RUN 24===================\n",
      "=====================RUN 25===================\n",
      "=====================RUN 26===================\n",
      "Problem with run 26\n",
      "=====================RUN 26===================\n",
      "=====================RUN 27===================\n",
      "=====================RUN 28===================\n",
      "Problem with run 28\n",
      "=====================RUN 28===================\n",
      "=====================RUN 29===================\n",
      "Problem with run 29\n",
      "=====================RUN 29===================\n",
      "=====================RUN 30===================\n",
      "=====================RUN 31===================\n",
      "Problem with run 31\n",
      "=====================RUN 31===================\n",
      "=====================RUN 32===================\n",
      "=====================RUN 33===================\n",
      "Problem with run 33\n",
      "=====================RUN 33===================\n",
      "=====================RUN 34===================\n",
      "Problem with run 34\n",
      "=====================RUN 34===================\n",
      "=====================RUN 35===================\n",
      "Problem with run 35\n",
      "=====================RUN 35===================\n",
      "=====================RUN 36===================\n",
      "=====================RUN 37===================\n",
      "=====================RUN 38===================\n",
      "=====================RUN 39===================\n",
      "=====================RUN 40===================\n",
      "Problem with run 40\n",
      "=====================RUN 40===================\n",
      "=====================RUN 41===================\n",
      "=====================RUN 42===================\n",
      "Problem with run 42\n",
      "=====================RUN 42===================\n",
      "Problem with run 42\n",
      "=====================RUN 42===================\n",
      "Problem with run 42\n",
      "=====================RUN 42===================\n",
      "=====================RUN 43===================\n",
      "=====================RUN 44===================\n",
      "=====================RUN 45===================\n",
      "Problem with run 45\n",
      "=====================RUN 45===================\n",
      "=====================RUN 46===================\n",
      "Problem with run 46\n",
      "=====================RUN 46===================\n",
      "=====================RUN 47===================\n",
      "=====================RUN 48===================\n",
      "Problem with run 48\n",
      "=====================RUN 48===================\n",
      "=====================RUN 49===================\n",
      "Problem with run 49\n",
      "=====================RUN 49===================\n",
      "Problem with run 49\n",
      "=====================RUN 49===================\n",
      "Problem with run 49\n",
      "=====================RUN 49===================\n",
      "=====================RUN 50===================\n",
      "=====================RUN 51===================\n",
      "=====================RUN 52===================\n",
      "Problem with run 52\n",
      "=====================RUN 52===================\n",
      "=====================RUN 53===================\n",
      "=====================RUN 54===================\n",
      "=====================RUN 55===================\n",
      "=====================RUN 56===================\n",
      "=====================RUN 57===================\n",
      "=====================RUN 58===================\n",
      "=====================RUN 59===================\n"
     ]
    }
   ],
   "source": [
    "print(best_params)\n",
    "N = 10000\n",
    "seed = 0\n",
    "num_runs = 60\n",
    "\n",
    "output_type_Q = 'categorical'\n",
    "output_size_Q = 1\n",
    "output_type_G = 'categorical'\n",
    "output_size_G = 1\n",
    "input_size_Q = z.shape[-1] + 1  # we will concatenate the treatment var inside the qnet class\n",
    "input_size_G = z.shape[-1]\n",
    "layers = best_params['layers']\n",
    "dropout = best_params['dropout']\n",
    "layer_size = best_params['layer_size']\n",
    "iters = best_params['iters']\n",
    "lr = best_params['lr']\n",
    "lr = best_params['lr']\n",
    "treg_lr = best_params['treg_lr']\n",
    "batch_size = best_params['batch_size']\n",
    "\n",
    "estimates_upd = []\n",
    "i = 0\n",
    "while i < num_runs:\n",
    "    try:\n",
    "        print('=====================RUN {}==================='.format(i))\n",
    "        seed += 1\n",
    "        # data generation:\n",
    "        z, x, y, _, _ = generate_data(N, seed=seed)\n",
    "        x = torch.tensor(x).type(torch.float32)\n",
    "        z = torch.tensor(z).type(torch.float32)\n",
    "        y = torch.tensor(y).type(torch.float32)\n",
    "        x_int1 = torch.ones_like(x)  # this is the 'intervention data'\n",
    "        x_int0 = torch.zeros_like(x)    \n",
    "\n",
    "        qnet = QNet(input_size=input_size_Q, num_layers=layers,\n",
    "                              layers_size=layer_size, output_size=output_size_Q,\n",
    "                             output_type=output_type_Q, dropout=dropout)\n",
    "\n",
    "        gnet = GNet(input_size=input_size_G, num_layers=layers,\n",
    "                              layers_size=layer_size, output_size=output_size_G,\n",
    "                             output_type=output_type_G, dropout=dropout)\n",
    "\n",
    "\n",
    "        trainer = Trainer(qnet=qnet, gnet=gnet, iterations=iters, outcome_type=output_type_Q,\n",
    "                          batch_size=batch_size, test_iter=500, lr=lr, treg_lr=treg_lr)\n",
    "\n",
    "        train_loss_q_, train_loss_g_, val_loss_q_, val_loss_g_ = trainer.train(x, y, z)\n",
    "\n",
    "        _, _, x_pred, y_pred = trainer.test(x, y, z)\n",
    "        x_pred, y_pred = x_pred.detach().numpy(), y_pred.detach().numpy()\n",
    "\n",
    "        _, _, G10, Q1 = trainer.test(x_int1, y, z)\n",
    "        _, _, _, Q0 = trainer.test(x_int0, y, z)\n",
    "\n",
    "        Q1 = Q1.detach().numpy()\n",
    "        Q0 = Q0.detach().numpy()\n",
    "\n",
    "        upd_psi = (Q1 - Q0).mean()\n",
    "\n",
    "\n",
    "        estimates_upd.append(upd_psi)\n",
    "        i += 1\n",
    "    except:\n",
    "        print('Problem with run {}'.format(i))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e37c9e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True psi:  0.1956508\n",
      "updated TMLE psi:  0.24102733  relative bias: 23.192609175004343 %\n",
      "updated psi var: 0.0007042262\n"
     ]
    }
   ],
   "source": [
    "estimates_upd = np.asarray(estimates_upd)\n",
    "\n",
    "\n",
    "print('True psi: ', true_psi)\n",
    "print('updated TMLE psi: ', estimates_upd.mean(), ' relative bias:',\n",
    "      (estimates_upd.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('updated psi var:', estimates_upd.var())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a92660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef13a45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
