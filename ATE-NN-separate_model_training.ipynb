{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ec6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8ec6e",
   "metadata": {},
   "source": [
    "## Summary of Results:\n",
    "\n",
    "$\\hat Q$ is the outcome estimator, $\\hat G$ is the propensity score estimator. Their respective columns tell us which estimators are use e.g. NN means a neural network was used.\n",
    "\n",
    "'Reduction' is the relative percent error reduction when compared against the plug-in estimator using the outcome model alone. The results are averages over 60 simulations.\n",
    "\n",
    "\n",
    "| Method | $\\hat Q$ | $\\hat G$ | Reduction $\\%$ | Rel. Error $\\%$ |\n",
    "| --- | --- | --- | --- |--- |\n",
    "| Naive | $NN$ | - |- |  4.059|\n",
    "| TMLE | $NN$ | $NN$ | 1.450 | 2.608 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1217b",
   "metadata": {},
   "source": [
    "## Problem Setup:\n",
    "\n",
    "This example is taken from https://arxiv.org/abs/2107.00681 by Hines, Dukes, Diaz-Ordaz, and Vansteelandt (2021) and the empirical evaluation follows https://onlinelibrary.wiley.com/doi/full/10.1002/sim.7628 by Miguel Angel Luque-Fernandez, Michael Schomaker, Bernard Rachet, Mireille E. Schnitzer (2018).\n",
    "\n",
    "\n",
    "The following experiments are very similar to the ones in ATE.ipynb, but this time we will fit the estimators using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049bda1",
   "metadata": {},
   "source": [
    "## 1. Define the DGP and some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f134282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigm(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def inv_sigm(x):\n",
    "    return np.log(x/(1-x))\n",
    "\n",
    "def generate_data(N, seed):\n",
    "    np.random.seed(seed=seed)\n",
    "    z1 = np.random.binomial(1, 0.5, (N,1))\n",
    "    z2 = np.random.binomial(1, 0.65, (N,1))\n",
    "    z3 = np.round(np.random.uniform(0, 4, (N,1)),3)\n",
    "    z4 = np.round(np.random.uniform(0, 5, (N,1)),3)\n",
    "    X = np.random.binomial(1, sigm(-0.4 + 0.2*z2 + 0.15*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y1 = np.random.binomial(1, sigm(-1 + 1 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y0 = np.random.binomial(1, sigm(-1 + 0 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y = Y1 * X + Y0 * (1-X)\n",
    "    Z = np.concatenate([z1,z2,z3,z4],1)\n",
    "    return Z, X, Y, Y1, Y0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed127b9",
   "metadata": {},
   "source": [
    "## 2. Define the Neural Network Objects/Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be134faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)     \n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layers_size, output_size, output_type, dropout):\n",
    "        super(QNet, self).__init__()      \n",
    "        \n",
    "        layers = []\n",
    "        layers.extend([nn.Linear(input_size, layers_size), nn.ReLU()])\n",
    "        for i in range(num_layers-1):\n",
    "            layers.extend([nn.Linear(layers_size, layers_size), nn.ReLU(), nn.Dropout(p=dropout)])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        pos_arm = []\n",
    "        pos_arm.extend([nn.Linear(layers_size, layers_size), nn.ReLU()])\n",
    "        pos_arm.extend([nn.Linear(layers_size, output_size)])     \n",
    "        \n",
    "        neg_arm = []\n",
    "        neg_arm.extend([nn.Linear(layers_size, layers_size), nn.ReLU()])\n",
    "        neg_arm.extend([nn.Linear(layers_size, output_size)])    \n",
    "        \n",
    "        if output_type == 'categorical':\n",
    "            pos_arm.append(nn.Sigmoid())\n",
    "            neg_arm.append(nn.Sigmoid())\n",
    "        elif output_type == 'continuous':\n",
    "            pass\n",
    "        self.pos_arm = nn.Sequential(*pos_arm)\n",
    "        self.neg_arm = nn.Sequential(*neg_arm)\n",
    "    \n",
    "        self.net.apply(init_weights) \n",
    "        self.neg_arm.apply(init_weights) \n",
    "        self.pos_arm.apply(init_weights) \n",
    "\n",
    "\n",
    "    def forward(self, X, Z):\n",
    "        \n",
    "        out = self.net(torch.cat([X,Z],1))\n",
    "        out0 = self.neg_arm(out)\n",
    "        out1 = self.pos_arm(out)\n",
    "        cond = X.bool()\n",
    "        return torch.where(cond, out1, out0)\n",
    "\n",
    "    \n",
    "    \n",
    "class GNet(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layers_size, output_size, output_type, dropout):\n",
    "        super(GNet, self).__init__()      \n",
    "        self.output_type = output_type\n",
    "        layers = []\n",
    "        layers.extend([nn.Linear(input_size, layers_size), nn.ReLU()])\n",
    "        for i in range(num_layers-1):\n",
    "            layers.extend([nn.Linear(layers_size, layers_size), nn.ReLU(), nn.Dropout(p=dropout)])\n",
    "        layers.extend([nn.Linear(layers_size, output_size)])\n",
    "\n",
    "        if output_type == 'categorical':\n",
    "            layers.append(nn.Sigmoid())\n",
    "        elif output_type == 'continuous':\n",
    "            pass\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(init_weights) \n",
    "        \n",
    "    def forward(self, Z):\n",
    "        if self.output_type == 'categorical':\n",
    "            out = (0.01 + self.net(Z))/1.02\n",
    "#             out = self.net(Z)\n",
    "        elif self.output_type == 'continuous':\n",
    "            out = self.net(Z)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7283e3",
   "metadata": {},
   "source": [
    "## 3. Create a Neural Network training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bebbbe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, net, net_type='Q', outcome_type='categorical', iterations=None, batch_size=None, test_iter=None, lr=None):\n",
    "        self.net_type = net_type\n",
    "        self.net = net\n",
    "        self.iterations = iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.test_iter = test_iter\n",
    "        self.outcome_type = outcome_type\n",
    "        \n",
    "        if lr is not None:\n",
    "            self.optimizer = optim.SGD(self.net.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "        self.bce_loss = nn.BCELoss(reduction='none')\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "        \n",
    "    def train(self, x, y, z):\n",
    "        \n",
    "        # create a small validation set\n",
    "        indices = np.arange(len(x))\n",
    "        np.random.shuffle(indices)\n",
    "        val_inds = indices[:len(x)//8]\n",
    "        train_inds = indices[len(x)//8:]\n",
    "        x_val, y_val, z_val = x[val_inds], y[val_inds], z[val_inds]\n",
    "        x_train, y_train, z_train = x[train_inds], y[train_inds], z[train_inds]\n",
    "        \n",
    "        indices = np.arange(len(x_train))\n",
    "        \n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "\n",
    "        best_model = None\n",
    "        best_model_test_loss = 1e10\n",
    "        best_early_stop_test_loss = 1e10\n",
    "        test_loss_window = []\n",
    "        window_length = 50  # number of measures of loss over which to determine early stopping\n",
    "        stopping_iteration = self.iterations  # initialise early stopping iter as the total iters\n",
    "        \n",
    "        for it in range(self.iterations):\n",
    "            inds = np.random.choice(indices, self.batch_size)\n",
    "            x_batch, y_batch, z_batch = x_train[inds], y_train[inds], z_train[inds]\n",
    "\n",
    "            if self.net_type == 'Q':\n",
    "                pred = self.net(x_batch, z_batch)\n",
    "                \n",
    "                if self.outcome_type == 'categorical':\n",
    "                    loss = self.bce_loss(pred, y_batch).mean()\n",
    "                else:\n",
    "                    loss = self.mse_loss(pred, y_batch)\n",
    "                \n",
    "            elif self.net_type == 'G':\n",
    "                pred = self.net(z_batch)\n",
    "                if self.outcome_type == 'categorical':\n",
    "                    loss = self.bce_loss(pred, x_batch).mean()\n",
    "                else:\n",
    "                    loss = self.mse_loss(pred, x_batch)\n",
    "\n",
    "            loss.backward()\n",
    " \n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if (it % self.test_iter == 0) or (it == (self.iterations-1)):\n",
    "                self.net.eval()\n",
    "\n",
    "                if self.net_type == 'Q':\n",
    "                    pred = self.net(x_train[:800], z_train[:800])\n",
    "\n",
    "                    if self.outcome_type == 'categorical':\n",
    "                        loss = self.bce_loss(pred, y_train[:800]).mean()\n",
    "                    else:\n",
    "                        loss = self.mse_loss(pred, y_train[:800])\n",
    "\n",
    "                elif self.net_type == 'G':\n",
    "                    pred = self.net(z_train[:800])\n",
    "                    if self.outcome_type == 'categorical':\n",
    "                        loss = self.bce_loss(pred, x_train[:800]).mean()\n",
    "                    else:\n",
    "                        loss = self.mse_loss(pred, x_train[:800])\n",
    "\n",
    "\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "                loss_test, _ = self.test(self.net, x_val, y_val, z_val)\n",
    "                loss_test = loss_test.detach().numpy()\n",
    "                test_losses.append(loss_test.item())\n",
    "\n",
    "                self.net.train()\n",
    "   \n",
    "                # Early Stopping Code part 1\n",
    "                if len(test_loss_window) > window_length:  # reset window\n",
    "                    test_loss_window = [] \n",
    "                test_loss_window.append(loss_test)\n",
    "                # Early Stopping Code part 2\n",
    "                if len(test_loss_window) == window_length:  # if we have a complete window\n",
    "                    av_loss_window = np.mean(test_loss_window)  # take average\n",
    "                    if av_loss_window < best_early_stop_test_loss:\n",
    "                        best_early_stop_test_loss = av_loss_window\n",
    "                    else:\n",
    "                        print('Test loss window average ',av_loss_window, ' increasing, breaking loop at iter ', it)\n",
    "                        stopping_iteration = it\n",
    "                        break\n",
    "        \n",
    "                if (loss_test < best_model_test_loss):\n",
    "                    best_model_test_loss = loss_test\n",
    "                    best_model = self.net\n",
    "                \n",
    "        return train_losses, test_losses, stopping_iteration, best_model, best_model_test_loss\n",
    "    \n",
    "    \n",
    "    def test(self, model, x, y, z):\n",
    "        model.eval()\n",
    "        \n",
    "        if self.net_type == 'Q':\n",
    "            pred = model(x, z)\n",
    "\n",
    "            if self.outcome_type == 'categorical':\n",
    "                loss = self.bce_loss(pred, y).mean()\n",
    "            else:\n",
    "                loss = self.mse_loss(pred, y)\n",
    "\n",
    "        elif self.net_type == 'G':\n",
    "            pred = model(z)\n",
    "            if self.outcome_type == 'categorical':\n",
    "                loss = self.bce_loss(pred, x).mean()\n",
    "            else:\n",
    "                loss = self.mse_loss(pred, x)\n",
    "        \n",
    "        return loss, pred\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e57e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4e2e5e6",
   "metadata": {},
   "source": [
    "## 4. Create a hyperparameter tuning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0658fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tuner(object):\n",
    "    def __init__(self, x, y, z, trials, net_type='Q', best_params=None):\n",
    "        self.net_type = net_type\n",
    "        self.best_params = best_params\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "        self.trials = trials\n",
    "        self.test_iter = 5\n",
    "        self.best_params = best_params\n",
    "        self.net = None\n",
    "        self.best_model = None\n",
    "        \n",
    "    def tune(self):\n",
    "\n",
    "        output_type = 'categorical'\n",
    "        output_size = 1\n",
    "        \n",
    "        if self.net_type == 'Q':\n",
    "            input_size = z.shape[-1] + 1  # we will concatenate the treatment var inside the qnet class\n",
    "        elif self.net_type == 'G':\n",
    "            input_size = z.shape[-1] \n",
    "            \n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        bs_ = []\n",
    "        iters_ = []\n",
    "        lr_ = []\n",
    "        stop_it_ = []   # list for early stopping iteration\n",
    "        layers_ = []\n",
    "        dropout_ = []\n",
    "        layer_size_ = []\n",
    "        best_loss = 1e10\n",
    "        best_losses = []\n",
    "        for trial in range(self.trials):\n",
    "            # sample hyper params and store the history\n",
    "            bs = np.random.randint(10,64) if self.best_params == None else self.best_params['batch_size']\n",
    "            bs_.append(bs)\n",
    "            iters = np.random.randint(15000,100000) if self.best_params == None else self.best_params['iters']\n",
    "            iters_.append(iters)\n",
    "            lr = np.random.uniform(0.0001, 0.01) if self.best_params == None else self.best_params['lr']\n",
    "            lr_.append(lr)\n",
    "            layers = np.random.randint(2, 6) if self.best_params == None else self.best_params['layers']\n",
    "            layers_.append(layers)\n",
    "            dropout = np.random.uniform(0.1,0.5) if self.best_params == None else self.best_params['dropout']\n",
    "            dropout_.append(dropout)\n",
    "            layer_size = np.random.randint(4, 32) if self.best_params == None else self.best_params['layer_size']\n",
    "            layer_size_.append(layer_size)\n",
    "            print('======== Trial {} of {} ========='.format(trial, self.trials-1))\n",
    "            print('Batch size', bs, ' Iters', iters, ' Lr', lr, ' Layers', layers,\n",
    "                 ' Dropout', dropout, ' Layer Size', layer_size)\n",
    "\n",
    "            if self.net_type == 'Q':\n",
    "                self.net = QNet(input_size=input_size, num_layers=layers,\n",
    "                          layers_size=layer_size, output_size=output_size,\n",
    "                         output_type=output_type, dropout=dropout)\n",
    "            elif self.net_type == 'G': \n",
    "                self.net = GNet(input_size=input_size, num_layers=layers,\n",
    "                          layers_size=layer_size, output_size=output_size,\n",
    "                         output_type=output_type, dropout=dropout)\n",
    "\n",
    "            trainer = Trainer(net=self.net, net_type=self.net_type, outcome_type=output_type,\n",
    "                              iterations=iters, batch_size=bs, test_iter=self.test_iter, lr=lr)\n",
    "            train_loss_, val_loss_, stop_it, best_model, best_model_test_loss_ = trainer.train(self.x, self.y, self.z)\n",
    "            \n",
    "            \n",
    "            print('Best number of iterations: ', stop_it, 'compared with total:', iters)\n",
    "            stop_it_.append(stop_it)\n",
    "            train_loss.append(train_loss_[-1])\n",
    "            val_loss.append(val_loss_[-1])\n",
    "            best_losses.append(best_model_test_loss_)\n",
    "            \n",
    "            total_val_loss = val_loss_[-1]\n",
    "            \n",
    "            if best_model_test_loss_ < best_loss:\n",
    "                print('old loss:', best_loss)\n",
    "                print('new loss:', total_val_loss)\n",
    "                print('best model updated')\n",
    "                best_loss = best_model_test_loss_\n",
    "                self.best_model = best_model\n",
    "\n",
    "        tuning_dict = {'batch_size': bs_, 'layers':layers_, 'dropout':dropout_,\n",
    "                      'layer_size':layer_size_,'lr':lr_, 'iters':iters_, 'stop_it': stop_it_,\n",
    "                      'train_loss':train_loss, 'val_loss':val_loss, 'best_model_test_loss':best_losses}\n",
    "        \n",
    "        return tuning_dict, self.best_model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6fc57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "129eeea7",
   "metadata": {},
   "source": [
    "## 5. Run Hyperparameter Search\n",
    "\n",
    "Now we have everything we need, we can initialize the neural networks, run hyperparameter search to identify the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "475fa678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Trial 0 of 99 =========\n",
      "Batch size 44  Iters 53624  Lr 0.000652378214512967  Layers 4  Dropout 0.2867448829614486  Layer Size 18\n",
      "Test loss window average  0.5645517  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 53624\n",
      "old loss: 10000000000.0\n",
      "new loss: 0.5647324919700623\n",
      "best model updated\n",
      "======== Trial 1 of 99 =========\n",
      "Batch size 60  Iters 30352  Lr 0.004538234392412887  Layers 5  Dropout 0.1529889661860082  Layer Size 20\n",
      "Test loss window average  0.54603875  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 30352\n",
      "old loss: 0.5634665\n",
      "new loss: 0.5430728793144226\n",
      "best model updated\n",
      "======== Trial 2 of 99 =========\n",
      "Batch size 17  Iters 63422  Lr 0.002660742479973014  Layers 5  Dropout 0.1940530526557307  Layer Size 9\n",
      "Test loss window average  0.57051617  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 63422\n",
      "======== Trial 3 of 99 =========\n",
      "Batch size 15  Iters 31094  Lr 0.009106643815713421  Layers 3  Dropout 0.41682924868837956  Layer Size 27\n",
      "Test loss window average  0.55080664  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 31094\n",
      "======== Trial 4 of 99 =========\n",
      "Batch size 35  Iters 79333  Lr 0.0006584710509871873  Layers 3  Dropout 0.4344811239855406  Layer Size 24\n",
      "Test loss window average  0.56145173  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 79333\n",
      "======== Trial 5 of 99 =========\n",
      "Batch size 13  Iters 74697  Lr 0.006139502729173394  Layers 2  Dropout 0.25400179988250843  Layer Size 20\n",
      "Test loss window average  0.53599554  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 74697\n",
      "old loss: 0.5391099\n",
      "new loss: 0.5303732752799988\n",
      "best model updated\n",
      "======== Trial 6 of 99 =========\n",
      "Batch size 30  Iters 82016  Lr 0.006859683598895172  Layers 4  Dropout 0.21640127312909227  Layer Size 31\n",
      "Test loss window average  0.54025364  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 82016\n",
      "======== Trial 7 of 99 =========\n",
      "Batch size 11  Iters 40774  Lr 0.0039566048715902495  Layers 3  Dropout 0.32013073573234474  Layer Size 6\n",
      "Test loss window average  0.5677127  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 40774\n",
      "======== Trial 8 of 99 =========\n",
      "Batch size 35  Iters 51959  Lr 0.00022067388277503147  Layers 3  Dropout 0.2501271884944541  Layer Size 17\n",
      "Test loss window average  0.5549655  increasing, breaking loop at iter  5855\n",
      "Best number of iterations:  5855 compared with total: 51959\n",
      "======== Trial 9 of 99 =========\n",
      "Batch size 18  Iters 48916  Lr 0.0006237753461884758  Layers 3  Dropout 0.43599504956293555  Layer Size 16\n",
      "Test loss window average  0.55908644  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 48916\n",
      "======== Trial 10 of 99 =========\n",
      "Batch size 44  Iters 51124  Lr 0.0017820787329390008  Layers 2  Dropout 0.43147235195573386  Layer Size 25\n",
      "Test loss window average  0.5409744  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 51124\n",
      "======== Trial 11 of 99 =========\n",
      "Batch size 62  Iters 43118  Lr 0.008540508361598384  Layers 3  Dropout 0.26548425727499414  Layer Size 15\n",
      "Test loss window average  0.5276314  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 43118\n",
      "old loss: 0.5296315\n",
      "new loss: 0.5313006639480591\n",
      "best model updated\n",
      "======== Trial 12 of 99 =========\n",
      "Batch size 60  Iters 33729  Lr 0.0037484405901764563  Layers 3  Dropout 0.13732664685080503  Layer Size 8\n",
      "Test loss window average  0.5457575  increasing, breaking loop at iter  3815\n",
      "Best number of iterations:  3815 compared with total: 33729\n",
      "======== Trial 13 of 99 =========\n",
      "Batch size 46  Iters 62031  Lr 0.0049176442526369605  Layers 5  Dropout 0.48095441717675236  Layer Size 4\n",
      "Test loss window average  0.5612071  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 62031\n",
      "======== Trial 14 of 99 =========\n",
      "Batch size 52  Iters 58488  Lr 0.0038337194939848606  Layers 5  Dropout 0.33448668104391294  Layer Size 26\n",
      "Test loss window average  0.55230314  increasing, breaking loop at iter  2795\n",
      "Best number of iterations:  2795 compared with total: 58488\n",
      "======== Trial 15 of 99 =========\n",
      "Batch size 36  Iters 58053  Lr 0.00015674449287340155  Layers 3  Dropout 0.4657569611149347  Layer Size 10\n",
      "Test loss window average  0.5692502  increasing, breaking loop at iter  3050\n",
      "Best number of iterations:  3050 compared with total: 58053\n",
      "======== Trial 16 of 99 =========\n",
      "Batch size 14  Iters 43837  Lr 0.0068629848016154025  Layers 4  Dropout 0.2189328159072853  Layer Size 19\n",
      "Test loss window average  0.5267037  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 43837\n",
      "old loss: 0.52539146\n",
      "new loss: 0.5186231732368469\n",
      "best model updated\n",
      "======== Trial 17 of 99 =========\n",
      "Batch size 17  Iters 77402  Lr 0.0057614146520926415  Layers 5  Dropout 0.13498109487694163  Layer Size 30\n",
      "Test loss window average  0.53862804  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 77402\n",
      "======== Trial 18 of 99 =========\n",
      "Batch size 61  Iters 61592  Lr 0.0018059916585856147  Layers 2  Dropout 0.45154226683268317  Layer Size 28\n",
      "Test loss window average  0.52662665  increasing, breaking loop at iter  4070\n",
      "Best number of iterations:  4070 compared with total: 61592\n",
      "======== Trial 19 of 99 =========\n",
      "Batch size 33  Iters 35759  Lr 0.00532305547121735  Layers 2  Dropout 0.28243822278322456  Layer Size 19\n",
      "Test loss window average  0.5459832  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 35759\n",
      "======== Trial 20 of 99 =========\n",
      "Batch size 28  Iters 17914  Lr 0.005107891408068364  Layers 4  Dropout 0.20260373872315626  Layer Size 20\n",
      "Test loss window average  0.5283069  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 17914\n",
      "old loss: 0.51517063\n",
      "new loss: 0.514214038848877\n",
      "best model updated\n",
      "======== Trial 21 of 99 =========\n",
      "Batch size 17  Iters 37644  Lr 0.00826522282378668  Layers 3  Dropout 0.1307513443977848  Layer Size 20\n",
      "Test loss window average  0.5281796  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 37644\n",
      "======== Trial 22 of 99 =========\n",
      "Batch size 58  Iters 37287  Lr 0.0023517008347862553  Layers 4  Dropout 0.3573117935687097  Layer Size 5\n",
      "Test loss window average  0.57466865  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 37287\n",
      "======== Trial 23 of 99 =========\n",
      "Batch size 28  Iters 34960  Lr 0.00772777205769063  Layers 2  Dropout 0.1370120648194833  Layer Size 25\n",
      "Test loss window average  0.5235155  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 34960\n",
      "======== Trial 24 of 99 =========\n",
      "Batch size 52  Iters 43127  Lr 0.000270805694576426  Layers 3  Dropout 0.4442205386056348  Layer Size 7\n",
      "Test loss window average  0.5561099  increasing, breaking loop at iter  4325\n",
      "Best number of iterations:  4325 compared with total: 43127\n",
      "======== Trial 25 of 99 =========\n",
      "Batch size 52  Iters 52571  Lr 0.0018340430589413948  Layers 4  Dropout 0.31791414725342737  Layer Size 22\n",
      "Test loss window average  0.5605158  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 52571\n",
      "======== Trial 26 of 99 =========\n",
      "Batch size 36  Iters 89438  Lr 0.0006687395036588879  Layers 2  Dropout 0.39471171205296474  Layer Size 10\n",
      "Test loss window average  0.5562527  increasing, breaking loop at iter  5600\n",
      "Best number of iterations:  5600 compared with total: 89438\n",
      "======== Trial 27 of 99 =========\n",
      "Batch size 17  Iters 93589  Lr 0.0032207527500207676  Layers 5  Dropout 0.4290558319396385  Layer Size 11\n",
      "Test loss window average  0.5735087  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 93589\n",
      "======== Trial 28 of 99 =========\n",
      "Batch size 49  Iters 36954  Lr 0.0025760005347907665  Layers 2  Dropout 0.3445056290103109  Layer Size 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.52553034  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 36954\n",
      "======== Trial 29 of 99 =========\n",
      "Batch size 30  Iters 28466  Lr 0.0005331026181408581  Layers 5  Dropout 0.45072183774859975  Layer Size 14\n",
      "Test loss window average  0.56033033  increasing, breaking loop at iter  2795\n",
      "Best number of iterations:  2795 compared with total: 28466\n",
      "======== Trial 30 of 99 =========\n",
      "Batch size 22  Iters 16516  Lr 0.0016535880487102315  Layers 2  Dropout 0.4845874497151136  Layer Size 27\n",
      "Test loss window average  0.56248254  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 16516\n",
      "======== Trial 31 of 99 =========\n",
      "Batch size 33  Iters 44222  Lr 0.002870592548521308  Layers 5  Dropout 0.17594685868376625  Layer Size 27\n",
      "Test loss window average  0.53587896  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 44222\n",
      "======== Trial 32 of 99 =========\n",
      "Batch size 26  Iters 56309  Lr 0.005384744285485241  Layers 5  Dropout 0.12422635299393191  Layer Size 28\n",
      "Test loss window average  0.5367392  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 56309\n",
      "======== Trial 33 of 99 =========\n",
      "Batch size 35  Iters 62054  Lr 0.0019854768785432615  Layers 2  Dropout 0.43200148528907234  Layer Size 10\n",
      "Test loss window average  0.55348283  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 62054\n",
      "======== Trial 34 of 99 =========\n",
      "Batch size 13  Iters 64051  Lr 0.009853659352368283  Layers 2  Dropout 0.3398670129921024  Layer Size 30\n",
      "Test loss window average  0.53290606  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 64051\n",
      "======== Trial 35 of 99 =========\n",
      "Batch size 37  Iters 71957  Lr 0.004839685224381257  Layers 2  Dropout 0.3862134364037866  Layer Size 9\n",
      "Test loss window average  0.5471046  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 71957\n",
      "======== Trial 36 of 99 =========\n",
      "Batch size 57  Iters 42994  Lr 0.006259486982975904  Layers 4  Dropout 0.4387991824869658  Layer Size 14\n",
      "Test loss window average  0.5667676  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 42994\n",
      "======== Trial 37 of 99 =========\n",
      "Batch size 37  Iters 63246  Lr 0.007655630687089553  Layers 2  Dropout 0.11632235869710544  Layer Size 12\n",
      "Test loss window average  0.505479  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 63246\n",
      "old loss: 0.51455206\n",
      "new loss: 0.5038195252418518\n",
      "best model updated\n",
      "======== Trial 38 of 99 =========\n",
      "Batch size 63  Iters 84894  Lr 0.005139362561474301  Layers 3  Dropout 0.19475918131748926  Layer Size 16\n",
      "Test loss window average  0.5254765  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 84894\n",
      "======== Trial 39 of 99 =========\n",
      "Batch size 39  Iters 81334  Lr 0.00033719207424606177  Layers 2  Dropout 0.15540438301843926  Layer Size 29\n",
      "Test loss window average  0.54649293  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 81334\n",
      "======== Trial 40 of 99 =========\n",
      "Batch size 53  Iters 36244  Lr 0.0025751744557165747  Layers 3  Dropout 0.2111428721484986  Layer Size 4\n",
      "Test loss window average  0.56680673  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 36244\n",
      "======== Trial 41 of 99 =========\n",
      "Batch size 46  Iters 82901  Lr 0.006092532054375525  Layers 5  Dropout 0.3186018012586199  Layer Size 19\n",
      "Test loss window average  0.5383933  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 82901\n",
      "======== Trial 42 of 99 =========\n",
      "Batch size 38  Iters 45821  Lr 0.0011227586178987976  Layers 2  Dropout 0.3660604410956888  Layer Size 7\n",
      "Test loss window average  0.5455425  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 45821\n",
      "======== Trial 43 of 99 =========\n",
      "Batch size 31  Iters 29001  Lr 0.006844842225714968  Layers 4  Dropout 0.4561842106337778  Layer Size 6\n",
      "Test loss window average  0.55602485  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 29001\n",
      "======== Trial 44 of 99 =========\n",
      "Batch size 12  Iters 15699  Lr 0.0036587877237379634  Layers 2  Dropout 0.33025302978663984  Layer Size 27\n",
      "Test loss window average  0.5228002  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 15699\n",
      "======== Trial 45 of 99 =========\n",
      "Batch size 24  Iters 70339  Lr 0.00025484746273668653  Layers 3  Dropout 0.20336209556730764  Layer Size 14\n",
      "Test loss window average  0.5404837  increasing, breaking loop at iter  4325\n",
      "Best number of iterations:  4325 compared with total: 70339\n",
      "======== Trial 46 of 99 =========\n",
      "Batch size 47  Iters 21284  Lr 0.005454707964217441  Layers 5  Dropout 0.2839033013215682  Layer Size 13\n",
      "Test loss window average  0.57007414  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 21284\n",
      "======== Trial 47 of 99 =========\n",
      "Batch size 40  Iters 93983  Lr 0.004306200539148876  Layers 2  Dropout 0.4991269147389855  Layer Size 16\n",
      "Test loss window average  0.57397944  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 93983\n",
      "======== Trial 48 of 99 =========\n",
      "Batch size 31  Iters 92900  Lr 0.0010086174048889057  Layers 4  Dropout 0.4602910941980136  Layer Size 22\n",
      "Test loss window average  0.57721514  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 92900\n",
      "======== Trial 49 of 99 =========\n",
      "Batch size 60  Iters 22384  Lr 0.008899309631772906  Layers 3  Dropout 0.4269364811991627  Layer Size 20\n",
      "Test loss window average  0.52591467  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 22384\n",
      "======== Trial 50 of 99 =========\n",
      "Batch size 14  Iters 71066  Lr 0.0006650427187104164  Layers 4  Dropout 0.25754446711387924  Layer Size 8\n",
      "Test loss window average  0.5536006  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 71066\n",
      "======== Trial 51 of 99 =========\n",
      "Batch size 55  Iters 27810  Lr 0.002000564061853039  Layers 3  Dropout 0.261107171781799  Layer Size 14\n",
      "Test loss window average  0.53399223  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 27810\n",
      "======== Trial 52 of 99 =========\n",
      "Batch size 24  Iters 95322  Lr 0.007246377455787181  Layers 2  Dropout 0.11077668519600037  Layer Size 10\n",
      "Test loss window average  0.54481506  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 95322\n",
      "======== Trial 53 of 99 =========\n",
      "Batch size 52  Iters 93158  Lr 0.007339686470642841  Layers 2  Dropout 0.16551652164761035  Layer Size 28\n",
      "Test loss window average  0.54470897  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 93158\n",
      "======== Trial 54 of 99 =========\n",
      "Batch size 46  Iters 27577  Lr 0.0038525815564876826  Layers 4  Dropout 0.241676944348742  Layer Size 19\n",
      "Test loss window average  0.5192336  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 27577\n",
      "======== Trial 55 of 99 =========\n",
      "Batch size 30  Iters 20578  Lr 0.0063360745578400065  Layers 4  Dropout 0.1773495595387519  Layer Size 9\n",
      "Test loss window average  0.5348046  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 20578\n",
      "======== Trial 56 of 99 =========\n",
      "Batch size 42  Iters 42022  Lr 0.006100310166053541  Layers 5  Dropout 0.42349607414792434  Layer Size 17\n",
      "Test loss window average  0.56843644  increasing, breaking loop at iter  3560\n",
      "Best number of iterations:  3560 compared with total: 42022\n",
      "======== Trial 57 of 99 =========\n",
      "Batch size 25  Iters 50869  Lr 0.0006857201364841507  Layers 4  Dropout 0.1709392293256773  Layer Size 24\n",
      "Test loss window average  0.5301884  increasing, breaking loop at iter  5090\n",
      "Best number of iterations:  5090 compared with total: 50869\n",
      "======== Trial 58 of 99 =========\n",
      "Batch size 59  Iters 70957  Lr 0.002234094548160649  Layers 5  Dropout 0.1911187979647722  Layer Size 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.5340366  increasing, breaking loop at iter  3815\n",
      "Best number of iterations:  3815 compared with total: 70957\n",
      "======== Trial 59 of 99 =========\n",
      "Batch size 36  Iters 90131  Lr 0.0012626264690249866  Layers 2  Dropout 0.4304862774769017  Layer Size 29\n",
      "Test loss window average  0.53872836  increasing, breaking loop at iter  4325\n",
      "Best number of iterations:  4325 compared with total: 90131\n",
      "======== Trial 60 of 99 =========\n",
      "Batch size 28  Iters 42219  Lr 0.0009667178418564544  Layers 3  Dropout 0.26048760338309346  Layer Size 12\n",
      "Test loss window average  0.5612828  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 42219\n",
      "======== Trial 61 of 99 =========\n",
      "Batch size 43  Iters 54881  Lr 0.006209376131094305  Layers 3  Dropout 0.13428828304912446  Layer Size 11\n",
      "Test loss window average  0.5247466  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 54881\n",
      "======== Trial 62 of 99 =========\n",
      "Batch size 28  Iters 36920  Lr 0.007626246630512273  Layers 3  Dropout 0.33041813988072966  Layer Size 13\n",
      "Test loss window average  0.5452267  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 36920\n",
      "======== Trial 63 of 99 =========\n",
      "Batch size 12  Iters 97329  Lr 0.00025623235945557735  Layers 2  Dropout 0.305939669321295  Layer Size 9\n",
      "Test loss window average  0.5381529  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 97329\n",
      "======== Trial 64 of 99 =========\n",
      "Batch size 21  Iters 67882  Lr 0.008367712187228234  Layers 4  Dropout 0.26807528361645727  Layer Size 23\n",
      "Test loss window average  0.5587818  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 67882\n",
      "======== Trial 65 of 99 =========\n",
      "Batch size 37  Iters 77272  Lr 0.002263949248491562  Layers 4  Dropout 0.12691789419413296  Layer Size 17\n",
      "Test loss window average  0.5358484  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 77272\n",
      "======== Trial 66 of 99 =========\n",
      "Batch size 58  Iters 97335  Lr 0.004831360114168112  Layers 4  Dropout 0.2628973606405395  Layer Size 20\n",
      "Test loss window average  0.54287416  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 97335\n",
      "======== Trial 67 of 99 =========\n",
      "Batch size 38  Iters 76232  Lr 0.006160456745850626  Layers 3  Dropout 0.3324205953119462  Layer Size 12\n",
      "Test loss window average  0.55599004  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 76232\n",
      "======== Trial 68 of 99 =========\n",
      "Batch size 14  Iters 52754  Lr 0.00782301537662952  Layers 3  Dropout 0.24261488763649106  Layer Size 23\n",
      "Test loss window average  0.53056175  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 52754\n",
      "======== Trial 69 of 99 =========\n",
      "Batch size 24  Iters 28150  Lr 0.004973115781818063  Layers 2  Dropout 0.3562323455999118  Layer Size 24\n",
      "Test loss window average  0.5442949  increasing, breaking loop at iter  2795\n",
      "Best number of iterations:  2795 compared with total: 28150\n",
      "======== Trial 70 of 99 =========\n",
      "Batch size 35  Iters 65936  Lr 0.004319369270168055  Layers 5  Dropout 0.2857316885282982  Layer Size 15\n",
      "Test loss window average  0.5563575  increasing, breaking loop at iter  2795\n",
      "Best number of iterations:  2795 compared with total: 65936\n",
      "======== Trial 71 of 99 =========\n",
      "Batch size 38  Iters 33653  Lr 0.0033256317966113337  Layers 4  Dropout 0.14340391502114427  Layer Size 26\n",
      "Test loss window average  0.5362259  increasing, breaking loop at iter  3305\n",
      "Best number of iterations:  3305 compared with total: 33653\n",
      "======== Trial 72 of 99 =========\n",
      "Batch size 27  Iters 32094  Lr 0.004299465520688275  Layers 3  Dropout 0.17681184457583998  Layer Size 28\n",
      "Test loss window average  0.53371614  increasing, breaking loop at iter  2795\n",
      "Best number of iterations:  2795 compared with total: 32094\n",
      "======== Trial 73 of 99 =========\n",
      "Batch size 16  Iters 79068  Lr 0.008070108594798198  Layers 3  Dropout 0.47348717266004925  Layer Size 4\n",
      "Test loss window average  0.56943244  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 79068\n",
      "======== Trial 74 of 99 =========\n",
      "Batch size 52  Iters 41570  Lr 0.0024427538352555936  Layers 4  Dropout 0.22214472427820262  Layer Size 20\n",
      "Test loss window average  0.5539457  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 41570\n",
      "======== Trial 75 of 99 =========\n",
      "Batch size 17  Iters 33399  Lr 0.004927099349710861  Layers 3  Dropout 0.2071531088688411  Layer Size 13\n",
      "Test loss window average  0.53066576  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 33399\n",
      "======== Trial 76 of 99 =========\n",
      "Batch size 30  Iters 97775  Lr 0.004031116217110889  Layers 3  Dropout 0.47385872678897434  Layer Size 17\n",
      "Test loss window average  0.5758357  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 97775\n",
      "======== Trial 77 of 99 =========\n",
      "Batch size 14  Iters 52710  Lr 0.008456852172313458  Layers 5  Dropout 0.3977234246833641  Layer Size 21\n",
      "Test loss window average  0.56028503  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 52710\n",
      "======== Trial 78 of 99 =========\n",
      "Batch size 31  Iters 52699  Lr 0.009022737803320056  Layers 4  Dropout 0.26575789083627793  Layer Size 27\n",
      "Test loss window average  0.5316399  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 52699\n",
      "======== Trial 79 of 99 =========\n",
      "Batch size 38  Iters 73702  Lr 0.001991765004953256  Layers 3  Dropout 0.40118773628501436  Layer Size 18\n",
      "Test loss window average  0.5555947  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 73702\n",
      "======== Trial 80 of 99 =========\n",
      "Batch size 44  Iters 27525  Lr 0.007085306878949308  Layers 4  Dropout 0.4777155486029513  Layer Size 28\n",
      "Test loss window average  0.5413662  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 27525\n",
      "======== Trial 81 of 99 =========\n",
      "Batch size 51  Iters 77049  Lr 0.007969326673840796  Layers 2  Dropout 0.3768865968866596  Layer Size 22\n",
      "Test loss window average  0.53834844  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 77049\n",
      "======== Trial 82 of 99 =========\n",
      "Batch size 17  Iters 60885  Lr 0.00427560326913978  Layers 4  Dropout 0.3041182790141791  Layer Size 30\n",
      "Test loss window average  0.5605375  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 60885\n",
      "======== Trial 83 of 99 =========\n",
      "Batch size 46  Iters 88097  Lr 0.009306899932012613  Layers 2  Dropout 0.3364128937929405  Layer Size 25\n",
      "Test loss window average  0.534977  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 88097\n",
      "======== Trial 84 of 99 =========\n",
      "Batch size 13  Iters 44990  Lr 0.005924988454926377  Layers 2  Dropout 0.3770940745799355  Layer Size 9\n",
      "Test loss window average  0.53612757  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 44990\n",
      "======== Trial 85 of 99 =========\n",
      "Batch size 42  Iters 67013  Lr 0.009193565577188752  Layers 4  Dropout 0.21992348686513458  Layer Size 5\n",
      "Test loss window average  0.55461615  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 67013\n",
      "======== Trial 86 of 99 =========\n",
      "Batch size 63  Iters 26688  Lr 0.005501978006037982  Layers 4  Dropout 0.4001486961080226  Layer Size 24\n",
      "Test loss window average  0.5572878  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 26688\n",
      "======== Trial 87 of 99 =========\n",
      "Batch size 36  Iters 70515  Lr 0.005540058063058432  Layers 5  Dropout 0.22452208661261427  Layer Size 26\n",
      "Test loss window average  0.5356515  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 70515\n",
      "======== Trial 88 of 99 =========\n",
      "Batch size 33  Iters 94633  Lr 0.005532037698461538  Layers 3  Dropout 0.48886121979455044  Layer Size 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.55881363  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 94633\n",
      "======== Trial 89 of 99 =========\n",
      "Batch size 59  Iters 30867  Lr 0.007714000480926086  Layers 3  Dropout 0.4919465483714843  Layer Size 17\n",
      "Test loss window average  0.5472176  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 30867\n",
      "======== Trial 90 of 99 =========\n",
      "Batch size 15  Iters 35345  Lr 0.00501280616127576  Layers 3  Dropout 0.3184957154703002  Layer Size 21\n",
      "Test loss window average  0.5499251  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 35345\n",
      "======== Trial 91 of 99 =========\n",
      "Batch size 55  Iters 33315  Lr 0.0009376272109717436  Layers 4  Dropout 0.3738426711051771  Layer Size 8\n",
      "Test loss window average  0.54993844  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 33315\n",
      "======== Trial 92 of 99 =========\n",
      "Batch size 36  Iters 57445  Lr 0.004400215599000271  Layers 2  Dropout 0.2528897827614014  Layer Size 4\n",
      "Test loss window average  0.5620941  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 57445\n",
      "======== Trial 93 of 99 =========\n",
      "Batch size 16  Iters 41679  Lr 0.0021079827776781055  Layers 5  Dropout 0.2794320557285513  Layer Size 14\n",
      "Test loss window average  0.5530688  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 41679\n",
      "======== Trial 94 of 99 =========\n",
      "Batch size 61  Iters 42499  Lr 0.005536124750072699  Layers 4  Dropout 0.36173235661439673  Layer Size 21\n",
      "Test loss window average  0.55686903  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 42499\n",
      "======== Trial 95 of 99 =========\n",
      "Batch size 17  Iters 69794  Lr 0.008515489446527897  Layers 4  Dropout 0.1940342633248406  Layer Size 27\n",
      "Test loss window average  0.5150026  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 69794\n",
      "======== Trial 96 of 99 =========\n",
      "Batch size 61  Iters 51101  Lr 0.008695126182453751  Layers 4  Dropout 0.34409975416957883  Layer Size 24\n",
      "Test loss window average  0.5508827  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 51101\n",
      "======== Trial 97 of 99 =========\n",
      "Batch size 33  Iters 53193  Lr 0.006968487506758217  Layers 2  Dropout 0.29160850087308954  Layer Size 26\n",
      "Test loss window average  0.55322367  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 53193\n",
      "======== Trial 98 of 99 =========\n",
      "Batch size 10  Iters 57496  Lr 0.008010568456762639  Layers 5  Dropout 0.40629482876852874  Layer Size 18\n",
      "Test loss window average  0.57714474  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 57496\n",
      "======== Trial 99 of 99 =========\n",
      "Batch size 49  Iters 84894  Lr 0.0015263686627438964  Layers 3  Dropout 0.1702791909837172  Layer Size 7\n",
      "Test loss window average  0.5561132  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 84894\n",
      "======== Trial 0 of 99 =========\n",
      "Batch size 48  Iters 79881  Lr 0.007242436744130267  Layers 2  Dropout 0.25130791603687797  Layer Size 4\n",
      "Test loss window average  0.62003195  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 79881\n",
      "old loss: 10000000000.0\n",
      "new loss: 0.6198300719261169\n",
      "best model updated\n",
      "======== Trial 1 of 99 =========\n",
      "Batch size 56  Iters 15791  Lr 0.004229593935100168  Layers 3  Dropout 0.3108971096095936  Layer Size 16\n",
      "Test loss window average  0.6273465  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 15791\n",
      "======== Trial 2 of 99 =========\n",
      "Batch size 10  Iters 32965  Lr 0.007675280916805273  Layers 3  Dropout 0.4629266992556412  Layer Size 25\n",
      "Test loss window average  0.617078  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 32965\n",
      "old loss: 0.61862063\n",
      "new loss: 0.6188605427742004\n",
      "best model updated\n",
      "======== Trial 3 of 99 =========\n",
      "Batch size 21  Iters 22137  Lr 0.008419725726728446  Layers 3  Dropout 0.46684811416026484  Layer Size 13\n",
      "Test loss window average  0.6112074  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 22137\n",
      "old loss: 0.60973054\n",
      "new loss: 0.6152473092079163\n",
      "best model updated\n",
      "======== Trial 4 of 99 =========\n",
      "Batch size 22  Iters 55334  Lr 0.005615063965990585  Layers 5  Dropout 0.3300429739136266  Layer Size 13\n",
      "Test loss window average  0.6240156  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 55334\n",
      "======== Trial 5 of 99 =========\n",
      "Batch size 52  Iters 48845  Lr 0.0026118976865651252  Layers 2  Dropout 0.33466152990633924  Layer Size 23\n",
      "Test loss window average  0.5994554  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 48845\n",
      "old loss: 0.6056331\n",
      "new loss: 0.6000021696090698\n",
      "best model updated\n",
      "======== Trial 6 of 99 =========\n",
      "Batch size 14  Iters 90638  Lr 0.005803645460631164  Layers 4  Dropout 0.1318462365512474  Layer Size 16\n",
      "Test loss window average  0.6179307  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 90638\n",
      "======== Trial 7 of 99 =========\n",
      "Batch size 59  Iters 25435  Lr 0.000900202479172398  Layers 5  Dropout 0.44397432137562476  Layer Size 6\n",
      "Test loss window average  0.6313396  increasing, breaking loop at iter  3050\n",
      "Best number of iterations:  3050 compared with total: 25435\n",
      "======== Trial 8 of 99 =========\n",
      "Batch size 21  Iters 82212  Lr 0.007934016447251887  Layers 4  Dropout 0.448981812105692  Layer Size 29\n",
      "Test loss window average  0.6235971  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 82212\n",
      "======== Trial 9 of 99 =========\n",
      "Batch size 25  Iters 56528  Lr 0.007910085961053604  Layers 3  Dropout 0.17216701609799304  Layer Size 6\n",
      "Test loss window average  0.6142454  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 56528\n",
      "======== Trial 10 of 99 =========\n",
      "Batch size 55  Iters 24678  Lr 0.00900822509789083  Layers 3  Dropout 0.470744908736632  Layer Size 16\n",
      "Test loss window average  0.6148567  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 24678\n",
      "======== Trial 11 of 99 =========\n",
      "Batch size 61  Iters 75344  Lr 0.006696342486893651  Layers 5  Dropout 0.15294980618660117  Layer Size 30\n",
      "Test loss window average  0.61354804  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 75344\n",
      "======== Trial 12 of 99 =========\n",
      "Batch size 35  Iters 99479  Lr 0.009565694622551666  Layers 3  Dropout 0.12944855271869493  Layer Size 25\n",
      "Test loss window average  0.59130794  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 99479\n",
      "old loss: 0.5978387\n",
      "new loss: 0.6029187440872192\n",
      "best model updated\n",
      "======== Trial 13 of 99 =========\n",
      "Batch size 40  Iters 69054  Lr 0.0065917062139133524  Layers 4  Dropout 0.4510555694742612  Layer Size 22\n",
      "Test loss window average  0.6067251  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 69054\n",
      "======== Trial 14 of 99 =========\n",
      "Batch size 48  Iters 44933  Lr 0.0077219986924473615  Layers 3  Dropout 0.3004287598106137  Layer Size 16\n",
      "Test loss window average  0.6107422  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 44933\n",
      "======== Trial 15 of 99 =========\n",
      "Batch size 40  Iters 64132  Lr 0.00019726426721889014  Layers 2  Dropout 0.24074924068467207  Layer Size 21\n",
      "Test loss window average  0.6126162  increasing, breaking loop at iter  9680\n",
      "Best number of iterations:  9680 compared with total: 64132\n",
      "======== Trial 16 of 99 =========\n",
      "Batch size 29  Iters 78393  Lr 0.0027514859120986103  Layers 2  Dropout 0.36658447004200945  Layer Size 4\n",
      "Test loss window average  0.6126605  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 78393\n",
      "======== Trial 17 of 99 =========\n",
      "Batch size 35  Iters 72708  Lr 0.005489692961254282  Layers 3  Dropout 0.34579329940679027  Layer Size 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.6046869  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 72708\n",
      "======== Trial 18 of 99 =========\n",
      "Batch size 52  Iters 70184  Lr 0.0007936796816116671  Layers 3  Dropout 0.2515096497113257  Layer Size 13\n",
      "Test loss window average  0.6127968  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 70184\n",
      "======== Trial 19 of 99 =========\n",
      "Batch size 11  Iters 49426  Lr 0.003787932034706455  Layers 5  Dropout 0.10825498645642857  Layer Size 29\n",
      "Test loss window average  0.60222495  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 49426\n",
      "======== Trial 20 of 99 =========\n",
      "Batch size 49  Iters 30986  Lr 0.0007421334575037287  Layers 4  Dropout 0.2086778838741799  Layer Size 15\n",
      "Test loss window average  0.61609876  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 30986\n",
      "======== Trial 21 of 99 =========\n",
      "Batch size 54  Iters 71722  Lr 0.009873934660022557  Layers 5  Dropout 0.18080371020626937  Layer Size 16\n",
      "Test loss window average  0.59795487  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 71722\n",
      "======== Trial 22 of 99 =========\n",
      "Batch size 22  Iters 25395  Lr 0.003380894500882286  Layers 3  Dropout 0.4331498210247441  Layer Size 10\n",
      "Test loss window average  0.62192893  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 25395\n",
      "======== Trial 23 of 99 =========\n",
      "Batch size 12  Iters 15511  Lr 0.00884638857611414  Layers 3  Dropout 0.18038516846421412  Layer Size 4\n",
      "Test loss window average  0.6374187  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 15511\n",
      "======== Trial 24 of 99 =========\n",
      "Batch size 47  Iters 88307  Lr 0.007449512879327376  Layers 4  Dropout 0.3036373300630234  Layer Size 16\n",
      "Test loss window average  0.60964113  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 88307\n",
      "======== Trial 25 of 99 =========\n",
      "Batch size 54  Iters 92408  Lr 0.0091278513396772  Layers 2  Dropout 0.16448623790166378  Layer Size 31\n",
      "Test loss window average  0.600204  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 92408\n",
      "======== Trial 26 of 99 =========\n",
      "Batch size 10  Iters 95991  Lr 0.0055909338402250494  Layers 3  Dropout 0.3132351969342442  Layer Size 5\n",
      "Test loss window average  0.61148554  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 95991\n",
      "======== Trial 27 of 99 =========\n",
      "Batch size 17  Iters 56528  Lr 0.00039792605437572007  Layers 3  Dropout 0.44291527856909485  Layer Size 13\n",
      "Test loss window average  0.6291685  increasing, breaking loop at iter  4580\n",
      "Best number of iterations:  4580 compared with total: 56528\n",
      "======== Trial 28 of 99 =========\n",
      "Batch size 25  Iters 84892  Lr 0.0011470167743406886  Layers 3  Dropout 0.2923682863137645  Layer Size 15\n",
      "Test loss window average  0.6210866  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 84892\n",
      "======== Trial 29 of 99 =========\n",
      "Batch size 57  Iters 73775  Lr 0.003620483852033307  Layers 2  Dropout 0.33549145700097194  Layer Size 31\n",
      "Test loss window average  0.5966933  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 73775\n",
      "======== Trial 30 of 99 =========\n",
      "Batch size 45  Iters 93016  Lr 0.0088253559657094  Layers 5  Dropout 0.26239787911614165  Layer Size 23\n",
      "Test loss window average  0.61918515  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 93016\n",
      "======== Trial 31 of 99 =========\n",
      "Batch size 55  Iters 90230  Lr 0.001159795884156374  Layers 2  Dropout 0.17649875815887126  Layer Size 24\n",
      "Test loss window average  0.5941803  increasing, breaking loop at iter  3305\n",
      "Best number of iterations:  3305 compared with total: 90230\n",
      "======== Trial 32 of 99 =========\n",
      "Batch size 54  Iters 93008  Lr 0.005668116810980131  Layers 3  Dropout 0.1913577375223103  Layer Size 14\n",
      "Test loss window average  0.60284287  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 93008\n",
      "======== Trial 33 of 99 =========\n",
      "Batch size 35  Iters 23061  Lr 0.009845172732625565  Layers 2  Dropout 0.14898530971228513  Layer Size 6\n",
      "Test loss window average  0.60170233  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 23061\n",
      "======== Trial 34 of 99 =========\n",
      "Batch size 50  Iters 95740  Lr 0.00890596818258697  Layers 5  Dropout 0.24117199441640969  Layer Size 14\n",
      "Test loss window average  0.61731577  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 95740\n",
      "======== Trial 35 of 99 =========\n",
      "Batch size 20  Iters 67889  Lr 0.0020494941825275077  Layers 3  Dropout 0.4625063350800884  Layer Size 18\n",
      "Test loss window average  0.6181569  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 67889\n",
      "======== Trial 36 of 99 =========\n",
      "Batch size 51  Iters 63515  Lr 0.006241560140599062  Layers 4  Dropout 0.4415420205970362  Layer Size 17\n",
      "Test loss window average  0.60431784  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 63515\n",
      "======== Trial 37 of 99 =========\n",
      "Batch size 58  Iters 46397  Lr 0.005759123125002743  Layers 3  Dropout 0.48133824235694456  Layer Size 9\n",
      "Test loss window average  0.6338129  increasing, breaking loop at iter  3305\n",
      "Best number of iterations:  3305 compared with total: 46397\n",
      "======== Trial 38 of 99 =========\n",
      "Batch size 22  Iters 40704  Lr 0.004229573194970851  Layers 4  Dropout 0.15557113578533005  Layer Size 10\n",
      "Test loss window average  0.5970828  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 40704\n",
      "======== Trial 39 of 99 =========\n",
      "Batch size 45  Iters 68530  Lr 0.003653189046251541  Layers 4  Dropout 0.32146502552415057  Layer Size 8\n",
      "Test loss window average  0.6110752  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 68530\n",
      "======== Trial 40 of 99 =========\n",
      "Batch size 48  Iters 61684  Lr 0.0006296651399131848  Layers 2  Dropout 0.3844578569802527  Layer Size 11\n",
      "Test loss window average  0.6069958  increasing, breaking loop at iter  3305\n",
      "Best number of iterations:  3305 compared with total: 61684\n",
      "======== Trial 41 of 99 =========\n",
      "Batch size 48  Iters 52975  Lr 0.004469164131330033  Layers 2  Dropout 0.14055179482221655  Layer Size 29\n",
      "Test loss window average  0.61034834  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 52975\n",
      "======== Trial 42 of 99 =========\n",
      "Batch size 63  Iters 36144  Lr 0.006052124439145795  Layers 2  Dropout 0.22097883145326558  Layer Size 29\n",
      "Test loss window average  0.60391665  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 36144\n",
      "======== Trial 43 of 99 =========\n",
      "Batch size 61  Iters 66619  Lr 0.005520575117255973  Layers 5  Dropout 0.3586021621492098  Layer Size 6\n",
      "Test loss window average  0.62068933  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 66619\n",
      "======== Trial 44 of 99 =========\n",
      "Batch size 12  Iters 98096  Lr 0.003740569940535782  Layers 3  Dropout 0.33298362050002794  Layer Size 4\n",
      "Test loss window average  0.612858  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 98096\n",
      "======== Trial 45 of 99 =========\n",
      "Batch size 10  Iters 22432  Lr 0.009242627662337577  Layers 2  Dropout 0.3401374324087564  Layer Size 20\n",
      "Test loss window average  0.6184891  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 22432\n",
      "======== Trial 46 of 99 =========\n",
      "Batch size 54  Iters 66995  Lr 0.0010883886348437752  Layers 2  Dropout 0.2217167111461345  Layer Size 20\n",
      "Test loss window average  0.60411286  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 66995\n",
      "======== Trial 47 of 99 =========\n",
      "Batch size 23  Iters 64261  Lr 0.0029452579408672914  Layers 2  Dropout 0.24085221688287972  Layer Size 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.6138474  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 64261\n",
      "======== Trial 48 of 99 =========\n",
      "Batch size 55  Iters 23506  Lr 0.001366278821972811  Layers 2  Dropout 0.37283931899352885  Layer Size 15\n",
      "Test loss window average  0.5950601  increasing, breaking loop at iter  3305\n",
      "Best number of iterations:  3305 compared with total: 23506\n",
      "======== Trial 49 of 99 =========\n",
      "Batch size 33  Iters 60716  Lr 0.003767272274498701  Layers 3  Dropout 0.4716626044475174  Layer Size 8\n",
      "Test loss window average  0.6440361  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 60716\n",
      "======== Trial 50 of 99 =========\n",
      "Batch size 36  Iters 48160  Lr 0.0007563700094954907  Layers 4  Dropout 0.45038919287126256  Layer Size 22\n",
      "Test loss window average  0.6223841  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 48160\n",
      "======== Trial 51 of 99 =========\n",
      "Batch size 24  Iters 68377  Lr 0.004786758557390873  Layers 5  Dropout 0.4758684771432765  Layer Size 11\n",
      "Test loss window average  0.63296825  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 68377\n",
      "======== Trial 52 of 99 =========\n",
      "Batch size 15  Iters 32148  Lr 0.0011029937420896556  Layers 3  Dropout 0.23092045328737074  Layer Size 24\n",
      "Test loss window average  0.6078404  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 32148\n",
      "======== Trial 53 of 99 =========\n",
      "Batch size 36  Iters 41406  Lr 0.008568184763373584  Layers 4  Dropout 0.25287854421683714  Layer Size 14\n",
      "Test loss window average  0.6235802  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 41406\n",
      "======== Trial 54 of 99 =========\n",
      "Batch size 45  Iters 19395  Lr 0.009634076768131002  Layers 4  Dropout 0.28247185048384976  Layer Size 6\n",
      "Test loss window average  0.60661846  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 19395\n",
      "======== Trial 55 of 99 =========\n",
      "Batch size 41  Iters 41243  Lr 0.006863452238841166  Layers 2  Dropout 0.44652740023759285  Layer Size 4\n",
      "Test loss window average  0.6390726  increasing, breaking loop at iter  3050\n",
      "Best number of iterations:  3050 compared with total: 41243\n",
      "======== Trial 56 of 99 =========\n",
      "Batch size 48  Iters 32981  Lr 0.0011038281782623995  Layers 4  Dropout 0.41633356361641993  Layer Size 30\n",
      "Test loss window average  0.61477286  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 32981\n",
      "======== Trial 57 of 99 =========\n",
      "Batch size 36  Iters 50027  Lr 0.002286964428314527  Layers 4  Dropout 0.30628552610000015  Layer Size 9\n",
      "Test loss window average  0.61400914  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 50027\n",
      "======== Trial 58 of 99 =========\n",
      "Batch size 21  Iters 52731  Lr 0.00066759799159504  Layers 2  Dropout 0.4940732349862833  Layer Size 31\n",
      "Test loss window average  0.6132651  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 52731\n",
      "======== Trial 59 of 99 =========\n",
      "Batch size 16  Iters 78963  Lr 0.00378326499754749  Layers 2  Dropout 0.3493518271780629  Layer Size 15\n",
      "Test loss window average  0.59511846  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 78963\n",
      "======== Trial 60 of 99 =========\n",
      "Batch size 33  Iters 96867  Lr 0.00035555383019011846  Layers 2  Dropout 0.4734981754752561  Layer Size 24\n",
      "Test loss window average  0.6048969  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 96867\n",
      "======== Trial 61 of 99 =========\n",
      "Batch size 28  Iters 41547  Lr 0.003513982490625871  Layers 3  Dropout 0.13703843132595173  Layer Size 22\n",
      "Test loss window average  0.59772754  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 41547\n",
      "======== Trial 62 of 99 =========\n",
      "Batch size 31  Iters 87830  Lr 0.007063843108971377  Layers 2  Dropout 0.1356890109668324  Layer Size 11\n",
      "Test loss window average  0.6017367  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 87830\n",
      "======== Trial 63 of 99 =========\n",
      "Batch size 23  Iters 68580  Lr 0.005004068110910728  Layers 4  Dropout 0.2832907371084872  Layer Size 13\n",
      "Test loss window average  0.6170801  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 68580\n",
      "======== Trial 64 of 99 =========\n",
      "Batch size 26  Iters 65607  Lr 0.003999987680488652  Layers 5  Dropout 0.3789808905153903  Layer Size 5\n",
      "Test loss window average  0.63689464  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 65607\n",
      "======== Trial 65 of 99 =========\n",
      "Batch size 31  Iters 65748  Lr 0.0014646327622870837  Layers 2  Dropout 0.4642973573121145  Layer Size 20\n",
      "Test loss window average  0.6025231  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 65748\n",
      "======== Trial 66 of 99 =========\n",
      "Batch size 17  Iters 88766  Lr 0.003794574998200423  Layers 5  Dropout 0.3325061311236236  Layer Size 23\n",
      "Test loss window average  0.6148173  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 88766\n",
      "======== Trial 67 of 99 =========\n",
      "Batch size 23  Iters 56113  Lr 0.001993098205419765  Layers 3  Dropout 0.3720729674928097  Layer Size 18\n",
      "Test loss window average  0.58386415  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 56113\n",
      "old loss: 0.5852729\n",
      "new loss: 0.5820919275283813\n",
      "best model updated\n",
      "======== Trial 68 of 99 =========\n",
      "Batch size 26  Iters 43439  Lr 0.0025136820709427657  Layers 4  Dropout 0.2540251286707461  Layer Size 26\n",
      "Test loss window average  0.6007755  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 43439\n",
      "======== Trial 69 of 99 =========\n",
      "Batch size 20  Iters 44549  Lr 0.005216279173570506  Layers 3  Dropout 0.4275832834034037  Layer Size 24\n",
      "Test loss window average  0.6107825  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 44549\n",
      "======== Trial 70 of 99 =========\n",
      "Batch size 45  Iters 18322  Lr 0.006416193141177863  Layers 4  Dropout 0.16951042324246104  Layer Size 9\n",
      "Test loss window average  0.6153549  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 18322\n",
      "======== Trial 71 of 99 =========\n",
      "Batch size 58  Iters 44463  Lr 0.00743991387007288  Layers 2  Dropout 0.288432121900706  Layer Size 8\n",
      "Test loss window average  0.6022112  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 44463\n",
      "======== Trial 72 of 99 =========\n",
      "Batch size 32  Iters 43910  Lr 0.0034917865154961416  Layers 3  Dropout 0.3130309884338066  Layer Size 31\n",
      "Test loss window average  0.5880973  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 43910\n",
      "======== Trial 73 of 99 =========\n",
      "Batch size 20  Iters 56397  Lr 0.0020907370982721956  Layers 3  Dropout 0.14514752898093192  Layer Size 13\n",
      "Test loss window average  0.6038184  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 56397\n",
      "======== Trial 74 of 99 =========\n",
      "Batch size 33  Iters 74458  Lr 0.0049268311061225955  Layers 2  Dropout 0.348690085874311  Layer Size 17\n",
      "Test loss window average  0.602922  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 74458\n",
      "======== Trial 75 of 99 =========\n",
      "Batch size 37  Iters 72061  Lr 0.0050868622168410435  Layers 4  Dropout 0.3655498933066966  Layer Size 24\n",
      "Test loss window average  0.6209391  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 72061\n",
      "======== Trial 76 of 99 =========\n",
      "Batch size 51  Iters 43515  Lr 0.009210577313007699  Layers 4  Dropout 0.26183508795163957  Layer Size 28\n",
      "Test loss window average  0.59788424  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 43515\n",
      "======== Trial 77 of 99 =========\n",
      "Batch size 57  Iters 73400  Lr 0.009923488241185961  Layers 3  Dropout 0.3439254220846273  Layer Size 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.6104931  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 73400\n",
      "======== Trial 78 of 99 =========\n",
      "Batch size 10  Iters 96021  Lr 0.006965395008320336  Layers 2  Dropout 0.4802948014293721  Layer Size 26\n",
      "Test loss window average  0.6211338  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 96021\n",
      "======== Trial 79 of 99 =========\n",
      "Batch size 54  Iters 77220  Lr 0.003661762497925869  Layers 4  Dropout 0.41663872001160296  Layer Size 25\n",
      "Test loss window average  0.6279073  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 77220\n",
      "======== Trial 80 of 99 =========\n",
      "Batch size 16  Iters 50375  Lr 0.001967071879739477  Layers 2  Dropout 0.31270344533869865  Layer Size 11\n",
      "Test loss window average  0.6075075  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 50375\n",
      "======== Trial 81 of 99 =========\n",
      "Batch size 12  Iters 34376  Lr 0.009133014767361123  Layers 4  Dropout 0.2637532561711601  Layer Size 7\n",
      "Test loss window average  0.64374673  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 34376\n",
      "======== Trial 82 of 99 =========\n",
      "Batch size 22  Iters 58775  Lr 0.0061455407709621115  Layers 4  Dropout 0.3656763564688358  Layer Size 8\n",
      "Test loss window average  0.62003577  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 58775\n",
      "======== Trial 83 of 99 =========\n",
      "Batch size 31  Iters 97456  Lr 0.009239142241201443  Layers 4  Dropout 0.41493548492700016  Layer Size 6\n",
      "Test loss window average  0.6283921  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 97456\n",
      "======== Trial 84 of 99 =========\n",
      "Batch size 26  Iters 63395  Lr 0.0037827114648124787  Layers 2  Dropout 0.3516970364232126  Layer Size 14\n",
      "Test loss window average  0.6051163  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 63395\n",
      "======== Trial 85 of 99 =========\n",
      "Batch size 58  Iters 28697  Lr 0.006692248134244196  Layers 4  Dropout 0.2493858055961279  Layer Size 16\n",
      "Test loss window average  0.610673  increasing, breaking loop at iter  3050\n",
      "Best number of iterations:  3050 compared with total: 28697\n",
      "======== Trial 86 of 99 =========\n",
      "Batch size 32  Iters 96963  Lr 0.004887484615986428  Layers 4  Dropout 0.12005057720280572  Layer Size 11\n",
      "Test loss window average  0.5982671  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 96963\n",
      "======== Trial 87 of 99 =========\n",
      "Batch size 28  Iters 16319  Lr 0.008819504883741447  Layers 4  Dropout 0.3831860119057561  Layer Size 10\n",
      "Test loss window average  0.6200256  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 16319\n",
      "======== Trial 88 of 99 =========\n",
      "Batch size 33  Iters 17046  Lr 0.0012416722919106698  Layers 4  Dropout 0.26375415560245696  Layer Size 9\n",
      "Test loss window average  0.61513793  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 17046\n",
      "======== Trial 89 of 99 =========\n",
      "Batch size 51  Iters 44271  Lr 0.007935472672867877  Layers 4  Dropout 0.17399336255690012  Layer Size 27\n",
      "Test loss window average  0.5959844  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 44271\n",
      "======== Trial 90 of 99 =========\n",
      "Batch size 38  Iters 16481  Lr 0.005075821981222215  Layers 5  Dropout 0.4091561125178954  Layer Size 19\n",
      "Test loss window average  0.623997  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 16481\n",
      "======== Trial 91 of 99 =========\n",
      "Batch size 54  Iters 71819  Lr 0.0016518331292383098  Layers 5  Dropout 0.23784985222347188  Layer Size 21\n",
      "Test loss window average  0.6229773  increasing, breaking loop at iter  2795\n",
      "Best number of iterations:  2795 compared with total: 71819\n",
      "======== Trial 92 of 99 =========\n",
      "Batch size 23  Iters 49509  Lr 0.0013764280988330163  Layers 5  Dropout 0.2469540650251898  Layer Size 17\n",
      "Test loss window average  0.6108433  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 49509\n",
      "======== Trial 93 of 99 =========\n",
      "Batch size 32  Iters 53858  Lr 0.007313293688586372  Layers 5  Dropout 0.2605176637923725  Layer Size 19\n",
      "Test loss window average  0.62425536  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 53858\n",
      "======== Trial 94 of 99 =========\n",
      "Batch size 45  Iters 18652  Lr 0.0013113009783230927  Layers 2  Dropout 0.3519618310045668  Layer Size 23\n",
      "Test loss window average  0.62463915  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 18652\n",
      "======== Trial 95 of 99 =========\n",
      "Batch size 30  Iters 83802  Lr 0.000872544864607637  Layers 3  Dropout 0.24160158230109285  Layer Size 30\n",
      "Test loss window average  0.61850375  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 83802\n",
      "======== Trial 96 of 99 =========\n",
      "Batch size 53  Iters 15834  Lr 0.002836101539948313  Layers 3  Dropout 0.34680365818493775  Layer Size 24\n",
      "Test loss window average  0.6028886  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 15834\n",
      "======== Trial 97 of 99 =========\n",
      "Batch size 44  Iters 71925  Lr 0.009631012297743533  Layers 2  Dropout 0.10593859219923614  Layer Size 24\n",
      "Test loss window average  0.5905506  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 71925\n",
      "======== Trial 98 of 99 =========\n",
      "Batch size 54  Iters 17310  Lr 0.0027144132926108016  Layers 3  Dropout 0.14126867935769974  Layer Size 10\n",
      "Test loss window average  0.62394863  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 17310\n",
      "======== Trial 99 of 99 =========\n",
      "Batch size 47  Iters 20905  Lr 0.005691909227111308  Layers 2  Dropout 0.1149791090766303  Layer Size 10\n",
      "Test loss window average  0.5990633  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 20905\n"
     ]
    }
   ],
   "source": [
    "# First establish ground truth treatment effect:\n",
    "N = 5000000\n",
    "Z, x, y, Y1, Y0 = generate_data(N, seed=0)\n",
    "true_psi = (Y1-Y0).mean()\n",
    "\n",
    "\n",
    "# Set some params\n",
    "N = 10000\n",
    "seed = 0\n",
    "num_tuning_trials = 100\n",
    "\n",
    "# data generation:\n",
    "z, x, y, _, _ = generate_data(N, 0)\n",
    "x = torch.tensor(x).type(torch.float32)\n",
    "z = torch.tensor(z).type(torch.float32)\n",
    "y = torch.tensor(y).type(torch.float32)\n",
    "    \n",
    "qtuner = Tuner(x=x,y=y,z=z, net_type='Q', trials=num_tuning_trials)\n",
    "qtuning_history, best_q = qtuner.tune()\n",
    "\n",
    "qtotal_losses = np.asarray(qtuning_history['best_model_test_loss'])\n",
    "qbest_index = np.argmin(qtotal_losses)\n",
    "\n",
    "qbest_params = {}\n",
    "for key in qtuning_history.keys():\n",
    "    qbest_params[key] = qtuning_history[key][qbest_index]\n",
    "    \n",
    "    \n",
    "gtuner = Tuner(x=x,y=y,z=z, net_type='G', trials=num_tuning_trials)\n",
    "gtuning_history, best_g = gtuner.tune()\n",
    "\n",
    "gtotal_losses = np.asarray(gtuning_history['best_model_test_loss'])\n",
    "gbest_index = np.argmin(gtotal_losses)\n",
    "\n",
    "gbest_params = {}\n",
    "for key in gtuning_history.keys():\n",
    "    gbest_params[key] = gtuning_history[key][gbest_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4090404",
   "metadata": {},
   "source": [
    "## 6. Run Simulation\n",
    "\n",
    "Now we have the best hyperparameters, we will run the simulations accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9709b014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Q params: {'batch_size': 37, 'layers': 2, 'dropout': 0.11632235869710544, 'layer_size': 12, 'lr': 0.007655630687089553, 'iters': 63246, 'stop_it': 1775, 'train_loss': 0.5133829712867737, 'val_loss': 0.5038195252418518, 'best_model_test_loss': array(0.49726328, dtype=float32)}\n",
      "Best G params: {'batch_size': 23, 'layers': 3, 'dropout': 0.3720729674928097, 'layer_size': 18, 'lr': 0.001993098205419765, 'iters': 56113, 'stop_it': 1010, 'train_loss': 0.5900227427482605, 'val_loss': 0.5820919275283813, 'best_model_test_loss': array(0.5760905, dtype=float32)}\n",
      "=====================RUN 0===================\n",
      "Test loss window average  0.5315169  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6270577  increasing, breaking loop at iter  1520\n",
      "=====================RUN 1===================\n",
      "Test loss window average  0.5466991  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.61129314  increasing, breaking loop at iter  1010\n",
      "=====================RUN 2===================\n",
      "Test loss window average  0.5363453  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6085139  increasing, breaking loop at iter  3815\n",
      "=====================RUN 3===================\n",
      "Test loss window average  0.51961035  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.62434405  increasing, breaking loop at iter  500\n",
      "=====================RUN 4===================\n",
      "Test loss window average  0.54241854  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.61202013  increasing, breaking loop at iter  755\n",
      "=====================RUN 5===================\n",
      "Test loss window average  0.51023555  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60559934  increasing, breaking loop at iter  2285\n",
      "=====================RUN 6===================\n",
      "Test loss window average  0.554957  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.62145936  increasing, breaking loop at iter  1775\n",
      "=====================RUN 7===================\n",
      "Test loss window average  0.53686476  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6149988  increasing, breaking loop at iter  1775\n",
      "=====================RUN 8===================\n",
      "Test loss window average  0.54141927  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6104983  increasing, breaking loop at iter  2285\n",
      "=====================RUN 9===================\n",
      "Test loss window average  0.54517627  increasing, breaking loop at iter  2795\n",
      "Test loss window average  0.61981106  increasing, breaking loop at iter  755\n",
      "=====================RUN 10===================\n",
      "Test loss window average  0.5527399  increasing, breaking loop at iter  2540\n",
      "Test loss window average  0.6205765  increasing, breaking loop at iter  500\n",
      "=====================RUN 11===================\n",
      "Test loss window average  0.5334408  increasing, breaking loop at iter  2285\n",
      "Test loss window average  0.600611  increasing, breaking loop at iter  2795\n",
      "=====================RUN 12===================\n",
      "Test loss window average  0.5279976  increasing, breaking loop at iter  2540\n",
      "Test loss window average  0.61795306  increasing, breaking loop at iter  3560\n",
      "=====================RUN 13===================\n",
      "Test loss window average  0.5207048  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.63101923  increasing, breaking loop at iter  1520\n",
      "=====================RUN 14===================\n",
      "Test loss window average  0.53477293  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.62334794  increasing, breaking loop at iter  1010\n",
      "=====================RUN 15===================\n",
      "Test loss window average  0.54136556  increasing, breaking loop at iter  2285\n",
      "Test loss window average  0.6084892  increasing, breaking loop at iter  2030\n",
      "=====================RUN 16===================\n",
      "Test loss window average  0.52369803  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6338879  increasing, breaking loop at iter  755\n",
      "=====================RUN 17===================\n",
      "Test loss window average  0.53133845  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6146637  increasing, breaking loop at iter  500\n",
      "=====================RUN 18===================\n",
      "Test loss window average  0.5427488  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6286982  increasing, breaking loop at iter  500\n",
      "=====================RUN 19===================\n",
      "Test loss window average  0.55068475  increasing, breaking loop at iter  2540\n",
      "Test loss window average  0.62673175  increasing, breaking loop at iter  2030\n",
      "=====================RUN 20===================\n",
      "Test loss window average  0.53975725  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6325529  increasing, breaking loop at iter  1265\n",
      "=====================RUN 21===================\n",
      "Test loss window average  0.54804564  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6286363  increasing, breaking loop at iter  1265\n",
      "=====================RUN 22===================\n",
      "Test loss window average  0.53600824  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.61416674  increasing, breaking loop at iter  755\n",
      "=====================RUN 23===================\n",
      "Test loss window average  0.519638  increasing, breaking loop at iter  2795\n",
      "Test loss window average  0.62359273  increasing, breaking loop at iter  3050\n",
      "=====================RUN 24===================\n",
      "Test loss window average  0.5302737  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.62756705  increasing, breaking loop at iter  755\n",
      "=====================RUN 25===================\n",
      "Test loss window average  0.5281153  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6253521  increasing, breaking loop at iter  1265\n",
      "=====================RUN 26===================\n",
      "Test loss window average  0.5273249  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6188163  increasing, breaking loop at iter  2795\n",
      "=====================RUN 27===================\n",
      "Test loss window average  0.52905566  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.62477255  increasing, breaking loop at iter  1010\n",
      "=====================RUN 28===================\n",
      "Test loss window average  0.5542448  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.62320817  increasing, breaking loop at iter  1265\n",
      "=====================RUN 29===================\n",
      "Test loss window average  0.50866085  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6338426  increasing, breaking loop at iter  755\n",
      "=====================RUN 30===================\n",
      "Test loss window average  0.5523733  increasing, breaking loop at iter  2540\n",
      "Test loss window average  0.6128301  increasing, breaking loop at iter  3050\n",
      "=====================RUN 31===================\n",
      "Test loss window average  0.53590935  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.61722773  increasing, breaking loop at iter  3305\n",
      "=====================RUN 32===================\n",
      "Test loss window average  0.53141105  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6082666  increasing, breaking loop at iter  1265\n",
      "=====================RUN 33===================\n",
      "Test loss window average  0.53415054  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6315448  increasing, breaking loop at iter  1265\n",
      "=====================RUN 34===================\n",
      "Test loss window average  0.5212118  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6191039  increasing, breaking loop at iter  1265\n",
      "=====================RUN 35===================\n",
      "Test loss window average  0.547779  increasing, breaking loop at iter  2540\n",
      "Test loss window average  0.6189275  increasing, breaking loop at iter  500\n",
      "=====================RUN 36===================\n",
      "Test loss window average  0.5431728  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60944164  increasing, breaking loop at iter  1265\n",
      "=====================RUN 37===================\n",
      "Test loss window average  0.5517097  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6057491  increasing, breaking loop at iter  2795\n",
      "=====================RUN 38===================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.54827636  increasing, breaking loop at iter  2285\n",
      "Test loss window average  0.608857  increasing, breaking loop at iter  1010\n",
      "=====================RUN 39===================\n",
      "Test loss window average  0.5148266  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.62741715  increasing, breaking loop at iter  1520\n",
      "=====================RUN 40===================\n",
      "Test loss window average  0.5245271  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.61601615  increasing, breaking loop at iter  755\n",
      "=====================RUN 41===================\n",
      "Test loss window average  0.54951245  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6181898  increasing, breaking loop at iter  1520\n",
      "=====================RUN 42===================\n",
      "Test loss window average  0.5288228  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.61504835  increasing, breaking loop at iter  1010\n",
      "=====================RUN 43===================\n",
      "Test loss window average  0.50697684  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.61652935  increasing, breaking loop at iter  3815\n",
      "=====================RUN 44===================\n",
      "Test loss window average  0.51768214  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6094358  increasing, breaking loop at iter  1520\n",
      "=====================RUN 45===================\n",
      "Test loss window average  0.5175972  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6215522  increasing, breaking loop at iter  1010\n",
      "=====================RUN 46===================\n",
      "Test loss window average  0.54273844  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.622509  increasing, breaking loop at iter  755\n",
      "=====================RUN 47===================\n",
      "Test loss window average  0.531782  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6179527  increasing, breaking loop at iter  1265\n",
      "=====================RUN 48===================\n",
      "Test loss window average  0.52629197  increasing, breaking loop at iter  3305\n",
      "Test loss window average  0.63124436  increasing, breaking loop at iter  1265\n",
      "=====================RUN 49===================\n",
      "Test loss window average  0.5320732  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6125207  increasing, breaking loop at iter  755\n",
      "=====================RUN 50===================\n",
      "Test loss window average  0.5145576  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.62803507  increasing, breaking loop at iter  1775\n",
      "=====================RUN 51===================\n",
      "Test loss window average  0.53001  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.59783953  increasing, breaking loop at iter  755\n",
      "=====================RUN 52===================\n",
      "Test loss window average  0.5662298  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6148378  increasing, breaking loop at iter  755\n",
      "=====================RUN 53===================\n",
      "Test loss window average  0.54662985  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.5950349  increasing, breaking loop at iter  1265\n",
      "=====================RUN 54===================\n",
      "Test loss window average  0.5449939  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.62198704  increasing, breaking loop at iter  755\n",
      "=====================RUN 55===================\n",
      "Test loss window average  0.53887594  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60374594  increasing, breaking loop at iter  1010\n",
      "=====================RUN 56===================\n",
      "Test loss window average  0.5427182  increasing, breaking loop at iter  2285\n",
      "Test loss window average  0.61029494  increasing, breaking loop at iter  1010\n",
      "=====================RUN 57===================\n",
      "Test loss window average  0.5455647  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6116372  increasing, breaking loop at iter  755\n",
      "=====================RUN 58===================\n",
      "Test loss window average  0.54622674  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6153852  increasing, breaking loop at iter  500\n",
      "=====================RUN 59===================\n",
      "Test loss window average  0.5277819  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60968065  increasing, breaking loop at iter  3050\n"
     ]
    }
   ],
   "source": [
    "print('Best Q params:', qbest_params)\n",
    "print('Best G params:', gbest_params)\n",
    "N = 10000\n",
    "seed = 0\n",
    "num_runs = 60\n",
    "\n",
    "output_type_Q = 'categorical'\n",
    "output_size_Q = 1\n",
    "output_type_G = 'categorical'\n",
    "output_size_G = 1\n",
    "input_size_Q = z.shape[-1] + 1  # we will concatenate the treatment var inside the qnet class\n",
    "input_size_G = z.shape[-1]\n",
    "qlayers = qbest_params['layers']\n",
    "qdropout = qbest_params['dropout']\n",
    "qlayer_size = qbest_params['layer_size']\n",
    "qiters = 100000  # use the early stopping iter\n",
    "qlr = qbest_params['lr']\n",
    "qbatch_size = qbest_params['batch_size']\n",
    "\n",
    "glayers = gbest_params['layers']\n",
    "gdropout = gbest_params['dropout']\n",
    "glayer_size = gbest_params['layer_size']\n",
    "giters = 100000  # use the early stopping iter\n",
    "glr = gbest_params['lr']\n",
    "gbatch_size = gbest_params['batch_size']\n",
    "\n",
    "estimates_naive = []\n",
    "estimates_upd = []\n",
    "for i in range(num_runs):\n",
    "    print('=====================RUN {}==================='.format(i))\n",
    "    seed += 1\n",
    "    # data generation:\n",
    "    z, x, y, _, _ = generate_data(N, seed=seed)\n",
    "    x = torch.tensor(x).type(torch.float32)\n",
    "    z = torch.tensor(z).type(torch.float32)\n",
    "    y = torch.tensor(y).type(torch.float32)\n",
    "    x_int1 = torch.ones_like(x)  # this is the 'intervention data'\n",
    "    x_int0 = torch.zeros_like(x)    \n",
    "    \n",
    "\n",
    "    qnet = QNet(input_size=input_size_Q, num_layers=qlayers,\n",
    "                          layers_size=qlayer_size, output_size=output_size_Q,\n",
    "                         output_type=output_type_Q, dropout=qdropout)\n",
    "\n",
    "    gnet = GNet(input_size=input_size_G, num_layers=glayers,\n",
    "                          layers_size=glayer_size, output_size=output_size_G,\n",
    "                         output_type=output_type_G, dropout=gdropout)\n",
    "\n",
    "\n",
    "    qtrainer = Trainer(net=qnet, net_type='Q', iterations=qiters, outcome_type=output_type_Q,\n",
    "                  batch_size=qbatch_size, test_iter=5, lr=qlr)\n",
    "    \n",
    "    gtrainer = Trainer(net=gnet, net_type='G', iterations=giters, outcome_type=output_type_G,\n",
    "                  batch_size=gbatch_size, test_iter=5, lr=glr)\n",
    "\n",
    "    train_loss_q_,  val_loss_q_, stop_it_q, best_model_q, best_model_test_loss_q = qtrainer.train(x, y, z)\n",
    "    train_loss_g_, val_loss_g_, stop_it_g, best_model_g, best_model_test_loss_g = gtrainer.train(x, y, z)\n",
    "    \n",
    "    _, y_pred = qtrainer.test(best_model_q, x, y, z)\n",
    "    _, x_pred = gtrainer.test(best_model_g, x, y, z)\n",
    "    \n",
    "    x_pred, y_pred = x_pred.detach().numpy(), y_pred.detach().numpy()\n",
    "\n",
    "    _,  Q1 = qtrainer.test(best_model_q, x_int1, y, z)\n",
    "    _, Q0 = qtrainer.test(best_model_q, x_int0, y, z)\n",
    "\n",
    "    _, G10 = gtrainer.test(best_model_g, x, y, z)\n",
    "\n",
    "\n",
    "    Q1 = Q1.detach().numpy()\n",
    "    Q0 = Q0.detach().numpy()\n",
    "    biased_psi = (Q1 - Q0).mean()\n",
    "\n",
    "    G10 = np.clip(G10.detach().numpy(), a_min=0.001, a_max=0.999)\n",
    "\n",
    "    H1 = 1/(G10)\n",
    "    H0 = 1 / (1 - G10)\n",
    "\n",
    "    x_ = x.detach().numpy()\n",
    "    y_ = y.detach().numpy()\n",
    "    D1 = x_ * H1 * (y_ - Q1) + Q1 - Q1.mean()\n",
    "    D0 = (1 - x_) * H0 * (y_ - Q0) + Q0 - Q0.mean()\n",
    "\n",
    "    Q1_star = Q1 + D1\n",
    "    Q0_star = Q0 + D0\n",
    "\n",
    "    upd_psi = (Q1_star - Q0_star).mean()\n",
    "\n",
    "    estimates_naive.append(biased_psi)\n",
    "    estimates_upd.append(upd_psi)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e37c9e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True psi:  0.1956508\n",
      "naive psi:  0.20881072  relative bias: 6.726226965311098 %\n",
      "updated TMLE psi:  0.20692608  relative bias: 5.762960133203548 %\n",
      "Reduction in bias: 0.9632668321075499 %\n"
     ]
    }
   ],
   "source": [
    "estimates_upd = np.asarray(estimates_upd)\n",
    "estimates_naive = np.asarray(estimates_naive)\n",
    "\n",
    "print('True psi: ', true_psi)\n",
    "print('naive psi: ', estimates_naive.mean(), ' relative bias:',\n",
    "      (estimates_naive.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('updated TMLE psi: ', estimates_upd.mean(), ' relative bias:',\n",
    "      (estimates_upd.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('Reduction in bias:', np.abs(estimates_naive.mean() - true_psi)/true_psi * 100 - \n",
    "     np.abs(estimates_upd.mean() - true_psi)/true_psi * 100, '%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4a92660",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive psi var: 0.00078004354\n",
      "updated psi var: 0.00013753674\n",
      "Average of reductions: 6.479485 %\n"
     ]
    }
   ],
   "source": [
    "# This takes the reduction in relative bias for each simulation first, then takes an average\n",
    "# (Owing to the nonlinearity of the ||x|| function, this gives different results which are\n",
    "# worth considering.)\n",
    "print('naive psi var:', estimates_naive.var())\n",
    "print('updated psi var:', estimates_upd.var())\n",
    "errors_naive = (estimates_naive - true_psi)/true_psi *100\n",
    "errors_updated = (estimates_upd - true_psi)/true_psi *100\n",
    "diff_errors = np.abs(errors_naive) - np.abs(errors_updated)\n",
    "print('Average of reductions:', diff_errors.mean(), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3d5d74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.60787255, dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(np.asarray(val_loss_g_))\n",
    "best_model_test_loss_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fef13a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.5256551, dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(np.asarray(val_loss_q_))\n",
    "best_model_test_loss_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0eca62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b57e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a61761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b0fca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
