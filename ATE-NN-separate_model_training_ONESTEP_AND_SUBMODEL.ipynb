{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ec6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8ec6e",
   "metadata": {},
   "source": [
    "## Summary of Results:\n",
    "\n",
    "$\\hat Q$ is the outcome estimator, $\\hat G$ is the propensity score estimator. Their respective columns tell us which estimators are use e.g. NN means a neural network was used.\n",
    "\n",
    "'Reduction' is the relative percent error reduction when compared against the plug-in estimator using the outcome model alone. The results are averages over 60 simulations.\n",
    "\n",
    "\n",
    "| Method | $\\hat Q$ | $\\hat G$ | Reduction $\\%$ | Rel. Error $\\%$ |\n",
    "| --- | --- | --- | --- |--- |\n",
    "| Naive | $NN$ | - |- |  4.059|\n",
    "| TMLE | $NN$ | $NN$ | 1.450 | 2.608 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1217b",
   "metadata": {},
   "source": [
    "## Problem Setup:\n",
    "\n",
    "This example is taken from https://arxiv.org/abs/2107.00681 by Hines, Dukes, Diaz-Ordaz, and Vansteelandt (2021) and the empirical evaluation follows https://onlinelibrary.wiley.com/doi/full/10.1002/sim.7628 by Miguel Angel Luque-Fernandez, Michael Schomaker, Bernard Rachet, Mireille E. Schnitzer (2018).\n",
    "\n",
    "\n",
    "The following experiments are very similar to the ones in ATE.ipynb, but this time we will fit the estimators using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049bda1",
   "metadata": {},
   "source": [
    "## 1. Define the DGP and some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f134282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigm(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def inv_sigm(x):\n",
    "    return np.log(x/(1-x))\n",
    "\n",
    "def generate_data(N, seed):\n",
    "    np.random.seed(seed=seed)\n",
    "    z1 = np.random.binomial(1, 0.5, (N,1))\n",
    "    z2 = np.random.binomial(1, 0.65, (N,1))\n",
    "    z3 = np.round(np.random.uniform(0, 4, (N,1)),3)\n",
    "    z4 = np.round(np.random.uniform(0, 5, (N,1)),3)\n",
    "    X = np.random.binomial(1, sigm(-0.4 + 0.2*z2 + 0.15*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y1 = np.random.binomial(1, sigm(-1 + 1 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y0 = np.random.binomial(1, sigm(-1 + 0 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y = Y1 * X + Y0 * (1-X)\n",
    "    Z = np.concatenate([z1,z2,z3,z4],1)\n",
    "    return Z, X, Y, Y1, Y0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed127b9",
   "metadata": {},
   "source": [
    "## 2. Define the Neural Network Objects/Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be134faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)     \n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layers_size, output_size, output_type, dropout):\n",
    "        super(QNet, self).__init__()      \n",
    "        \n",
    "        layers = []\n",
    "        layers.extend([nn.Linear(input_size, layers_size), nn.ReLU()])\n",
    "        for i in range(num_layers-1):\n",
    "            layers.extend([nn.Linear(layers_size, layers_size), nn.ReLU(), nn.Dropout(p=dropout)])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        pos_arm = []\n",
    "        pos_arm.extend([nn.Linear(layers_size, layers_size), nn.ReLU()])\n",
    "        pos_arm.extend([nn.Linear(layers_size, output_size)])     \n",
    "        \n",
    "        neg_arm = []\n",
    "        neg_arm.extend([nn.Linear(layers_size, layers_size), nn.ReLU()])\n",
    "        neg_arm.extend([nn.Linear(layers_size, output_size)])    \n",
    "        \n",
    "        if output_type == 'categorical':\n",
    "            pos_arm.append(nn.Sigmoid())\n",
    "            neg_arm.append(nn.Sigmoid())\n",
    "        elif output_type == 'continuous':\n",
    "            pass\n",
    "        self.pos_arm = nn.Sequential(*pos_arm)\n",
    "        self.neg_arm = nn.Sequential(*neg_arm)\n",
    "    \n",
    "        self.net.apply(init_weights) \n",
    "        self.neg_arm.apply(init_weights) \n",
    "        self.pos_arm.apply(init_weights) \n",
    "\n",
    "\n",
    "    def forward(self, X, Z):\n",
    "        \n",
    "        out = self.net(torch.cat([X,Z],1))\n",
    "        out0 = self.neg_arm(out)\n",
    "        out1 = self.pos_arm(out)\n",
    "        cond = X.bool()\n",
    "        return torch.where(cond, out1, out0)\n",
    "\n",
    "    \n",
    "    \n",
    "class GNet(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layers_size, output_size, output_type, dropout):\n",
    "        super(GNet, self).__init__()      \n",
    "        self.output_type = output_type\n",
    "        layers = []\n",
    "        layers.extend([nn.Linear(input_size, layers_size), nn.ReLU()])\n",
    "        for i in range(num_layers-1):\n",
    "            layers.extend([nn.Linear(layers_size, layers_size), nn.ReLU(), nn.Dropout(p=dropout)])\n",
    "        layers.extend([nn.Linear(layers_size, output_size)])\n",
    "\n",
    "        if output_type == 'categorical':\n",
    "            layers.append(nn.Sigmoid())\n",
    "        elif output_type == 'continuous':\n",
    "            pass\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(init_weights) \n",
    "        \n",
    "    def forward(self, Z):\n",
    "        if self.output_type == 'categorical':\n",
    "            out = (0.01 + self.net(Z))/1.02\n",
    "#             out = self.net(Z)\n",
    "        elif self.output_type == 'continuous':\n",
    "            out = self.net(Z)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7283e3",
   "metadata": {},
   "source": [
    "## 3. Create a Neural Network training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696557f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, net, net_type='Q', outcome_type='categorical', iterations=None, batch_size=None, test_iter=None, lr=None):\n",
    "        self.net_type = net_type\n",
    "        self.net = net\n",
    "        self.iterations = iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.test_iter = test_iter\n",
    "        self.outcome_type = outcome_type\n",
    "        \n",
    "        if lr is not None:\n",
    "            self.optimizer = optim.SGD(self.net.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "        self.bce_loss = nn.BCELoss(reduction='none')\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "        \n",
    "    def train(self, x, y, z):\n",
    "        \n",
    "        # create a small validation set\n",
    "        indices = np.arange(len(x))\n",
    "        np.random.shuffle(indices)\n",
    "        val_inds = indices[:len(x)//8]\n",
    "        train_inds = indices[len(x)//8:]\n",
    "        x_val, y_val, z_val = x[val_inds], y[val_inds], z[val_inds]\n",
    "        x_train, y_train, z_train = x[train_inds], y[train_inds], z[train_inds]\n",
    "        \n",
    "        indices = np.arange(len(x_train))\n",
    "        \n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "\n",
    "        best_model = None\n",
    "        best_model_test_loss = 1e10\n",
    "        best_early_stop_test_loss = 1e10\n",
    "        test_loss_window = []\n",
    "        window_length = 50  # number of measures of loss over which to determine early stopping\n",
    "        stopping_iteration = self.iterations  # initialise early stopping iter as the total iters\n",
    "        \n",
    "        for it in range(self.iterations):\n",
    "            inds = np.random.choice(indices, self.batch_size)\n",
    "            x_batch, y_batch, z_batch = x_train[inds], y_train[inds], z_train[inds]\n",
    "\n",
    "            if self.net_type == 'Q':\n",
    "                pred = self.net(x_batch, z_batch)\n",
    "                \n",
    "                if self.outcome_type == 'categorical':\n",
    "                    loss = self.bce_loss(pred, y_batch).mean()\n",
    "                else:\n",
    "                    loss = self.mse_loss(pred, y_batch)\n",
    "                \n",
    "            elif self.net_type == 'G':\n",
    "                pred = self.net(z_batch)\n",
    "                if self.outcome_type == 'categorical':\n",
    "                    loss = self.bce_loss(pred, x_batch).mean()\n",
    "                else:\n",
    "                    loss = self.mse_loss(pred, x_batch)\n",
    "\n",
    "            loss.backward()\n",
    " \n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if (it % self.test_iter == 0) or (it == (self.iterations-1)):\n",
    "                self.net.eval()\n",
    "\n",
    "                if self.net_type == 'Q':\n",
    "                    pred = self.net(x_train[:800], z_train[:800])\n",
    "\n",
    "                    if self.outcome_type == 'categorical':\n",
    "                        loss = self.bce_loss(pred, y_train[:800]).mean()\n",
    "                    else:\n",
    "                        loss = self.mse_loss(pred, y_train[:800])\n",
    "\n",
    "                elif self.net_type == 'G':\n",
    "                    pred = self.net(z_train[:800])\n",
    "                    if self.outcome_type == 'categorical':\n",
    "                        loss = self.bce_loss(pred, x_train[:800]).mean()\n",
    "                    else:\n",
    "                        loss = self.mse_loss(pred, x_train[:800])\n",
    "\n",
    "\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "                loss_test, _ = self.test(self.net, x_val, y_val, z_val)\n",
    "                loss_test = loss_test.detach().numpy()\n",
    "                test_losses.append(loss_test.item())\n",
    "\n",
    "                self.net.train()\n",
    "   \n",
    "                # Early Stopping Code part 1\n",
    "                if len(test_loss_window) > window_length:  # reset window\n",
    "                    test_loss_window = [] \n",
    "                test_loss_window.append(loss_test)\n",
    "                # Early Stopping Code part 2\n",
    "                if len(test_loss_window) == window_length:  # if we have a complete window\n",
    "                    av_loss_window = np.mean(test_loss_window)  # take average\n",
    "                    if av_loss_window < best_early_stop_test_loss:\n",
    "                        best_early_stop_test_loss = av_loss_window\n",
    "                    else:\n",
    "                        print('Test loss window average ',av_loss_window, ' increasing, breaking loop at iter ', it)\n",
    "                        stopping_iteration = it\n",
    "                        break\n",
    "        \n",
    "                if (loss_test < best_model_test_loss):\n",
    "                    best_model_test_loss = loss_test\n",
    "                    best_model = self.net\n",
    "                \n",
    "        return train_losses, test_losses, stopping_iteration, best_model, best_model_test_loss\n",
    "    \n",
    "    \n",
    "    def test(self, model, x, y, z):\n",
    "        model.eval()\n",
    "        \n",
    "        if self.net_type == 'Q':\n",
    "            pred = model(x, z)\n",
    "\n",
    "            if self.outcome_type == 'categorical':\n",
    "                loss = self.bce_loss(pred, y).mean()\n",
    "            else:\n",
    "                loss = self.mse_loss(pred, y)\n",
    "\n",
    "        elif self.net_type == 'G':\n",
    "            pred = model(z)\n",
    "            if self.outcome_type == 'categorical':\n",
    "                loss = self.bce_loss(pred, x).mean()\n",
    "            else:\n",
    "                loss = self.mse_loss(pred, x)\n",
    "        \n",
    "        return loss, pred\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e57e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4e2e5e6",
   "metadata": {},
   "source": [
    "## 4. Create a hyperparameter tuning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0658fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tuner(object):\n",
    "    def __init__(self, x, y, z, trials, net_type='Q', best_params=None):\n",
    "        self.net_type = net_type\n",
    "        self.best_params = best_params\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "        self.trials = trials\n",
    "        self.test_iter = 5\n",
    "        self.best_params = best_params\n",
    "        self.net = None\n",
    "        self.best_model = None\n",
    "        \n",
    "    def tune(self):\n",
    "\n",
    "        output_type = 'categorical'\n",
    "        output_size = 1\n",
    "        \n",
    "        if self.net_type == 'Q':\n",
    "            input_size = z.shape[-1] + 1  # we will concatenate the treatment var inside the qnet class\n",
    "        elif self.net_type == 'G':\n",
    "            input_size = z.shape[-1] \n",
    "            \n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        bs_ = []\n",
    "        iters_ = []\n",
    "        lr_ = []\n",
    "        stop_it_ = []   # list for early stopping iteration\n",
    "        layers_ = []\n",
    "        dropout_ = []\n",
    "        layer_size_ = []\n",
    "        best_loss = 1e10\n",
    "        best_losses = []\n",
    "        for trial in range(self.trials):\n",
    "            # sample hyper params and store the history\n",
    "            bs = np.random.randint(10,64) if self.best_params == None else self.best_params['batch_size']\n",
    "            bs_.append(bs)\n",
    "            iters = np.random.randint(15000,100000) if self.best_params == None else self.best_params['iters']\n",
    "            iters_.append(iters)\n",
    "            lr = np.random.uniform(0.0001, 0.01) if self.best_params == None else self.best_params['lr']\n",
    "            lr_.append(lr)\n",
    "            layers = np.random.randint(2, 6) if self.best_params == None else self.best_params['layers']\n",
    "            layers_.append(layers)\n",
    "            dropout = np.random.uniform(0.1,0.5) if self.best_params == None else self.best_params['dropout']\n",
    "            dropout_.append(dropout)\n",
    "            layer_size = np.random.randint(4, 32) if self.best_params == None else self.best_params['layer_size']\n",
    "            layer_size_.append(layer_size)\n",
    "            print('======== Trial {} of {} ========='.format(trial, self.trials-1))\n",
    "            print('Batch size', bs, ' Iters', iters, ' Lr', lr, ' Layers', layers,\n",
    "                 ' Dropout', dropout, ' Layer Size', layer_size)\n",
    "\n",
    "            if self.net_type == 'Q':\n",
    "                self.net = QNet(input_size=input_size, num_layers=layers,\n",
    "                          layers_size=layer_size, output_size=output_size,\n",
    "                         output_type=output_type, dropout=dropout)\n",
    "            elif self.net_type == 'G': \n",
    "                self.net = GNet(input_size=input_size, num_layers=layers,\n",
    "                          layers_size=layer_size, output_size=output_size,\n",
    "                         output_type=output_type, dropout=dropout)\n",
    "\n",
    "            trainer = Trainer(net=self.net, net_type=self.net_type, outcome_type=output_type,\n",
    "                              iterations=iters, batch_size=bs, test_iter=self.test_iter, lr=lr)\n",
    "            train_loss_, val_loss_, stop_it, best_model, best_model_test_loss_ = trainer.train(self.x, self.y, self.z)\n",
    "            \n",
    "            \n",
    "            print('Best number of iterations: ', stop_it, 'compared with total:', iters)\n",
    "            stop_it_.append(stop_it)\n",
    "            train_loss.append(train_loss_[-1])\n",
    "            val_loss.append(val_loss_[-1])\n",
    "            best_losses.append(best_model_test_loss_)\n",
    "            \n",
    "            total_val_loss = val_loss_[-1]\n",
    "            \n",
    "            if best_model_test_loss_ < best_loss:\n",
    "                print('old loss:', best_loss)\n",
    "                print('new loss:', total_val_loss)\n",
    "                print('best model updated')\n",
    "                best_loss = best_model_test_loss_\n",
    "                self.best_model = best_model\n",
    "\n",
    "        tuning_dict = {'batch_size': bs_, 'layers':layers_, 'dropout':dropout_,\n",
    "                      'layer_size':layer_size_,'lr':lr_, 'iters':iters_, 'stop_it': stop_it_,\n",
    "                      'train_loss':train_loss, 'val_loss':val_loss, 'best_model_test_loss':best_losses}\n",
    "        \n",
    "        return tuning_dict, self.best_model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6fc57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "129eeea7",
   "metadata": {},
   "source": [
    "## 5. Run Hyperparameter Search\n",
    "\n",
    "Now we have everything we need, we can initialize the neural networks, run hyperparameter search to identify the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "475fa678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Trial 0 of 99 =========\n",
      "Batch size 44  Iters 53624  Lr 0.000652378214512967  Layers 4  Dropout 0.2867448829614486  Layer Size 18\n",
      "Test loss window average  0.5592107  increasing, breaking loop at iter  10190\n",
      "Best number of iterations:  10190 compared with total: 53624\n",
      "old loss: 10000000000.0\n",
      "new loss: 0.5595807433128357\n",
      "best model updated\n",
      "======== Trial 1 of 99 =========\n",
      "Batch size 51  Iters 43726  Lr 0.005433653261887084  Layers 5  Dropout 0.43129337234764165  Layer Size 13\n",
      "Test loss window average  0.55543405  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 43726\n",
      "old loss: 0.5589241\n",
      "new loss: 0.5533078908920288\n",
      "best model updated\n",
      "======== Trial 2 of 99 =========\n",
      "Batch size 20  Iters 94621  Lr 0.006576332728579949  Layers 4  Dropout 0.49387524017490037  Layer Size 21\n",
      "Test loss window average  0.55030763  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 94621\n",
      "old loss: 0.5535525\n",
      "new loss: 0.5440694093704224\n",
      "best model updated\n",
      "======== Trial 3 of 99 =========\n",
      "Batch size 51  Iters 16837  Lr 0.00525179612764436  Layers 3  Dropout 0.14681164130132915  Layer Size 6\n",
      "Test loss window average  0.520889  increasing, breaking loop at iter  3305\n",
      "Best number of iterations:  3305 compared with total: 16837\n",
      "old loss: 0.53706133\n",
      "new loss: 0.5189207792282104\n",
      "best model updated\n",
      "======== Trial 4 of 99 =========\n",
      "Batch size 61  Iters 53249  Lr 0.00268090542266513  Layers 2  Dropout 0.10224508386896214  Layer Size 27\n",
      "Test loss window average  0.5337938  increasing, breaking loop at iter  4835\n",
      "Best number of iterations:  4835 compared with total: 53249\n",
      "======== Trial 5 of 99 =========\n",
      "Batch size 17  Iters 99511  Lr 0.005055330655106307  Layers 3  Dropout 0.3671899978350509  Layer Size 8\n",
      "Test loss window average  0.5398026  increasing, breaking loop at iter  3050\n",
      "Best number of iterations:  3050 compared with total: 99511\n",
      "======== Trial 6 of 99 =========\n",
      "Batch size 60  Iters 77755  Lr 0.00023645792514606845  Layers 3  Dropout 0.3885121045759288  Layer Size 21\n",
      "Test loss window average  0.5480153  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 77755\n",
      "======== Trial 7 of 99 =========\n",
      "Batch size 20  Iters 58406  Lr 0.007058742636930999  Layers 3  Dropout 0.4907567998930614  Layer Size 24\n",
      "Test loss window average  0.54817206  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 58406\n",
      "======== Trial 8 of 99 =========\n",
      "Batch size 61  Iters 29104  Lr 0.009998324215181733  Layers 4  Dropout 0.2500656471718089  Layer Size 15\n",
      "Test loss window average  0.53678614  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 29104\n",
      "======== Trial 9 of 99 =========\n",
      "Batch size 26  Iters 48883  Lr 0.0006056602274567223  Layers 5  Dropout 0.394060597364879  Layer Size 30\n",
      "Test loss window average  0.55956894  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 48883\n",
      "======== Trial 10 of 99 =========\n",
      "Batch size 12  Iters 75267  Lr 0.003456099064798978  Layers 2  Dropout 0.4374606006636378  Layer Size 13\n",
      "Test loss window average  0.5610257  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 75267\n",
      "======== Trial 11 of 99 =========\n",
      "Batch size 47  Iters 80985  Lr 0.009751665995866817  Layers 3  Dropout 0.15326799195675012  Layer Size 7\n",
      "Test loss window average  0.54182696  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 80985\n",
      "======== Trial 12 of 99 =========\n",
      "Batch size 36  Iters 48737  Lr 0.006346589092953094  Layers 3  Dropout 0.28177863638930317  Layer Size 17\n",
      "Test loss window average  0.5181569  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 48737\n",
      "old loss: 0.518269\n",
      "new loss: 0.5126064419746399\n",
      "best model updated\n",
      "======== Trial 13 of 99 =========\n",
      "Batch size 53  Iters 51306  Lr 0.007647107608434676  Layers 2  Dropout 0.13430680420108435  Layer Size 8\n",
      "Test loss window average  0.5198094  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 51306\n",
      "======== Trial 14 of 99 =========\n",
      "Batch size 33  Iters 79097  Lr 0.00875374420922063  Layers 4  Dropout 0.26670157577253173  Layer Size 19\n",
      "Test loss window average  0.5307438  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 79097\n",
      "======== Trial 15 of 99 =========\n",
      "Batch size 43  Iters 82125  Lr 0.007843901547193263  Layers 5  Dropout 0.15646354317421196  Layer Size 29\n",
      "Test loss window average  0.5373697  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 82125\n",
      "======== Trial 16 of 99 =========\n",
      "Batch size 27  Iters 68564  Lr 0.009176139896513084  Layers 4  Dropout 0.4893817395956416  Layer Size 15\n",
      "Test loss window average  0.56160975  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 68564\n",
      "======== Trial 17 of 99 =========\n",
      "Batch size 10  Iters 30497  Lr 0.00226557361116969  Layers 3  Dropout 0.469017756671373  Layer Size 14\n",
      "Test loss window average  0.5637465  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 30497\n",
      "======== Trial 18 of 99 =========\n",
      "Batch size 17  Iters 43979  Lr 0.006553203906313056  Layers 2  Dropout 0.21166638930723947  Layer Size 9\n",
      "Test loss window average  0.5383919  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 43979\n",
      "======== Trial 19 of 99 =========\n",
      "Batch size 33  Iters 70939  Lr 0.0019980988046717297  Layers 2  Dropout 0.388016713133125  Layer Size 16\n",
      "Test loss window average  0.5594033  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 70939\n",
      "======== Trial 20 of 99 =========\n",
      "Batch size 13  Iters 66054  Lr 0.004312821333114503  Layers 5  Dropout 0.12223754000997343  Layer Size 11\n",
      "Test loss window average  0.53383076  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 66054\n",
      "======== Trial 21 of 99 =========\n",
      "Batch size 44  Iters 35225  Lr 0.005753491270662909  Layers 2  Dropout 0.3947634368272177  Layer Size 10\n",
      "Test loss window average  0.55371773  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 35225\n",
      "======== Trial 22 of 99 =========\n",
      "Batch size 48  Iters 95241  Lr 0.008210386896935479  Layers 5  Dropout 0.3088963587252201  Layer Size 22\n",
      "Test loss window average  0.54223675  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 95241\n",
      "======== Trial 23 of 99 =========\n",
      "Batch size 39  Iters 89938  Lr 0.002352902943737508  Layers 3  Dropout 0.1613853152626378  Layer Size 29\n",
      "Test loss window average  0.5265022  increasing, breaking loop at iter  4070\n",
      "Best number of iterations:  4070 compared with total: 89938\n",
      "======== Trial 24 of 99 =========\n",
      "Batch size 52  Iters 55483  Lr 0.007574686727215229  Layers 3  Dropout 0.4240402769828563  Layer Size 22\n",
      "Test loss window average  0.5601986  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 55483\n",
      "======== Trial 25 of 99 =========\n",
      "Batch size 62  Iters 65936  Lr 0.002428482633146483  Layers 5  Dropout 0.4968110966232431  Layer Size 23\n",
      "Test loss window average  0.56404144  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 65936\n",
      "======== Trial 26 of 99 =========\n",
      "Batch size 59  Iters 52665  Lr 0.007367890170598474  Layers 3  Dropout 0.12245088168874814  Layer Size 25\n",
      "Test loss window average  0.5074319  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 52665\n",
      "old loss: 0.5098168\n",
      "new loss: 0.5058887600898743\n",
      "best model updated\n",
      "======== Trial 27 of 99 =========\n",
      "Batch size 45  Iters 70016  Lr 0.0007823231735006544  Layers 2  Dropout 0.30672081159789977  Layer Size 10\n",
      "Test loss window average  0.54649657  increasing, breaking loop at iter  13760\n",
      "Best number of iterations:  13760 compared with total: 70016\n",
      "======== Trial 28 of 99 =========\n",
      "Batch size 59  Iters 58328  Lr 0.009449173727997957  Layers 2  Dropout 0.1746961769103879  Layer Size 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.5494791  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 58328\n",
      "======== Trial 29 of 99 =========\n",
      "Batch size 49  Iters 88415  Lr 0.006243970267646095  Layers 5  Dropout 0.37956252103414734  Layer Size 6\n",
      "Test loss window average  0.56529236  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 88415\n",
      "======== Trial 30 of 99 =========\n",
      "Batch size 52  Iters 22544  Lr 0.00011692071455346853  Layers 2  Dropout 0.2456259999655373  Layer Size 24\n",
      "Test loss window average  0.53690076  increasing, breaking loop at iter  3050\n",
      "Best number of iterations:  3050 compared with total: 22544\n",
      "======== Trial 31 of 99 =========\n",
      "Batch size 63  Iters 61873  Lr 0.007065701617915673  Layers 4  Dropout 0.3549362731740149  Layer Size 30\n",
      "Test loss window average  0.5330689  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 61873\n",
      "======== Trial 32 of 99 =========\n",
      "Batch size 10  Iters 31159  Lr 0.006534535915548478  Layers 2  Dropout 0.15697796752365525  Layer Size 22\n",
      "Test loss window average  0.5293031  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 31159\n",
      "======== Trial 33 of 99 =========\n",
      "Batch size 27  Iters 82975  Lr 0.007083653097512071  Layers 5  Dropout 0.12504678931559662  Layer Size 21\n",
      "Test loss window average  0.5144271  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 82975\n",
      "======== Trial 34 of 99 =========\n",
      "Batch size 16  Iters 52940  Lr 0.0036743892843501655  Layers 2  Dropout 0.2676987467485359  Layer Size 29\n",
      "Test loss window average  0.5415583  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 52940\n",
      "======== Trial 35 of 99 =========\n",
      "Batch size 60  Iters 26328  Lr 0.006219509051047259  Layers 3  Dropout 0.2130163030618981  Layer Size 22\n",
      "Test loss window average  0.5407826  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 26328\n",
      "======== Trial 36 of 99 =========\n",
      "Batch size 58  Iters 71730  Lr 0.002067047128431226  Layers 4  Dropout 0.13056284072002314  Layer Size 9\n",
      "Test loss window average  0.5578649  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 71730\n",
      "======== Trial 37 of 99 =========\n",
      "Batch size 45  Iters 61643  Lr 0.006341989337691665  Layers 3  Dropout 0.22391023388870776  Layer Size 28\n",
      "Test loss window average  0.51831603  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 61643\n",
      "======== Trial 38 of 99 =========\n",
      "Batch size 27  Iters 64716  Lr 0.00042932649295613636  Layers 2  Dropout 0.10094132448730431  Layer Size 14\n",
      "Test loss window average  0.5543859  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 64716\n",
      "======== Trial 39 of 99 =========\n",
      "Batch size 35  Iters 99100  Lr 0.00787096673240462  Layers 5  Dropout 0.1551757517116805  Layer Size 25\n",
      "Test loss window average  0.54305804  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 99100\n",
      "======== Trial 40 of 99 =========\n",
      "Batch size 41  Iters 25737  Lr 0.009071204006238051  Layers 2  Dropout 0.18447602004685548  Layer Size 15\n",
      "Test loss window average  0.52120334  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 25737\n",
      "======== Trial 41 of 99 =========\n",
      "Batch size 63  Iters 55421  Lr 0.008910213022794839  Layers 3  Dropout 0.1495448362896232  Layer Size 24\n",
      "Test loss window average  0.5260934  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 55421\n",
      "======== Trial 42 of 99 =========\n",
      "Batch size 30  Iters 90004  Lr 0.0038413628734588084  Layers 2  Dropout 0.3101670197593041  Layer Size 4\n",
      "Test loss window average  0.54760885  increasing, breaking loop at iter  4580\n",
      "Best number of iterations:  4580 compared with total: 90004\n",
      "======== Trial 43 of 99 =========\n",
      "Batch size 63  Iters 79263  Lr 0.007151697624785123  Layers 4  Dropout 0.31355424805114807  Layer Size 22\n",
      "Test loss window average  0.55308217  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 79263\n",
      "======== Trial 44 of 99 =========\n",
      "Batch size 29  Iters 36572  Lr 0.0087778610245976  Layers 3  Dropout 0.4171867410804416  Layer Size 18\n",
      "Test loss window average  0.5559876  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 36572\n",
      "======== Trial 45 of 99 =========\n",
      "Batch size 46  Iters 45938  Lr 0.008314779865512989  Layers 3  Dropout 0.4839676403462355  Layer Size 29\n",
      "Test loss window average  0.5428147  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 45938\n",
      "======== Trial 46 of 99 =========\n",
      "Batch size 16  Iters 22915  Lr 0.005145958804434004  Layers 4  Dropout 0.4506999603828442  Layer Size 22\n",
      "Test loss window average  0.56597805  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 22915\n",
      "======== Trial 47 of 99 =========\n",
      "Batch size 44  Iters 24221  Lr 0.003232387273424239  Layers 2  Dropout 0.49390732934753767  Layer Size 24\n",
      "Test loss window average  0.53864235  increasing, breaking loop at iter  3050\n",
      "Best number of iterations:  3050 compared with total: 24221\n",
      "======== Trial 48 of 99 =========\n",
      "Batch size 24  Iters 69966  Lr 0.003912983472617621  Layers 4  Dropout 0.36412299425741645  Layer Size 19\n",
      "Test loss window average  0.54496074  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 69966\n",
      "======== Trial 49 of 99 =========\n",
      "Batch size 54  Iters 94187  Lr 0.009976160418381499  Layers 3  Dropout 0.28832796648001296  Layer Size 25\n",
      "Test loss window average  0.52697736  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 94187\n",
      "======== Trial 50 of 99 =========\n",
      "Batch size 52  Iters 56018  Lr 0.007239473145421273  Layers 2  Dropout 0.2220004059678983  Layer Size 23\n",
      "Test loss window average  0.5370096  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 56018\n",
      "======== Trial 51 of 99 =========\n",
      "Batch size 44  Iters 56972  Lr 0.009933456894985612  Layers 2  Dropout 0.1669462842670925  Layer Size 24\n",
      "Test loss window average  0.50429606  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 56972\n",
      "old loss: 0.50294644\n",
      "new loss: 0.5039693713188171\n",
      "best model updated\n",
      "======== Trial 52 of 99 =========\n",
      "Batch size 33  Iters 60899  Lr 0.004725630161936744  Layers 5  Dropout 0.19527644049028045  Layer Size 13\n",
      "Test loss window average  0.52576405  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 60899\n",
      "======== Trial 53 of 99 =========\n",
      "Batch size 59  Iters 81796  Lr 0.007913073324264988  Layers 3  Dropout 0.15889365980164555  Layer Size 11\n",
      "Test loss window average  0.5316338  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 81796\n",
      "======== Trial 54 of 99 =========\n",
      "Batch size 55  Iters 49680  Lr 0.0027228043696020283  Layers 2  Dropout 0.1925833168489636  Layer Size 29\n",
      "Test loss window average  0.53013754  increasing, breaking loop at iter  4325\n",
      "Best number of iterations:  4325 compared with total: 49680\n",
      "======== Trial 55 of 99 =========\n",
      "Batch size 25  Iters 53697  Lr 0.0017165021404364025  Layers 5  Dropout 0.4551497666416643  Layer Size 4\n",
      "Test loss window average  0.5858948  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 53697\n",
      "======== Trial 56 of 99 =========\n",
      "Batch size 15  Iters 20345  Lr 0.006544242713371287  Layers 3  Dropout 0.38887506337367006  Layer Size 14\n",
      "Test loss window average  0.55305076  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 20345\n",
      "======== Trial 57 of 99 =========\n",
      "Batch size 43  Iters 55216  Lr 0.0024598245140707  Layers 2  Dropout 0.39237246145444427  Layer Size 21\n",
      "Test loss window average  0.5403391  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 55216\n",
      "======== Trial 58 of 99 =========\n",
      "Batch size 16  Iters 75469  Lr 0.008078254511288206  Layers 2  Dropout 0.2987223998329226  Layer Size 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.5446518  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 75469\n",
      "======== Trial 59 of 99 =========\n",
      "Batch size 50  Iters 40320  Lr 0.005069570395124123  Layers 2  Dropout 0.14609855266885635  Layer Size 25\n",
      "Test loss window average  0.5274913  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 40320\n",
      "======== Trial 60 of 99 =========\n",
      "Batch size 54  Iters 91920  Lr 0.006326766452905587  Layers 4  Dropout 0.3956077132069933  Layer Size 8\n",
      "Test loss window average  0.55905515  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 91920\n",
      "======== Trial 61 of 99 =========\n",
      "Batch size 56  Iters 76915  Lr 0.00915596119540175  Layers 5  Dropout 0.4922851301357082  Layer Size 4\n",
      "Test loss window average  0.55497634  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 76915\n",
      "======== Trial 62 of 99 =========\n",
      "Batch size 55  Iters 36705  Lr 0.009238878322146616  Layers 2  Dropout 0.15078155992376094  Layer Size 14\n",
      "Test loss window average  0.5457054  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 36705\n",
      "======== Trial 63 of 99 =========\n",
      "Batch size 50  Iters 27350  Lr 0.005711062172561481  Layers 4  Dropout 0.15595486489191013  Layer Size 10\n",
      "Test loss window average  0.5314579  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 27350\n",
      "======== Trial 64 of 99 =========\n",
      "Batch size 14  Iters 22909  Lr 0.0031416346399937506  Layers 5  Dropout 0.2890530245572598  Layer Size 29\n",
      "Test loss window average  0.55197483  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 22909\n",
      "======== Trial 65 of 99 =========\n",
      "Batch size 23  Iters 72861  Lr 0.007521694950469561  Layers 2  Dropout 0.48011805425178755  Layer Size 28\n",
      "Test loss window average  0.55859005  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 72861\n",
      "======== Trial 66 of 99 =========\n",
      "Batch size 20  Iters 64901  Lr 0.0040553274788564025  Layers 5  Dropout 0.2164867303622342  Layer Size 12\n",
      "Test loss window average  0.55153596  increasing, breaking loop at iter  2795\n",
      "Best number of iterations:  2795 compared with total: 64901\n",
      "======== Trial 67 of 99 =========\n",
      "Batch size 23  Iters 52050  Lr 0.005743301639496341  Layers 5  Dropout 0.31895570007382706  Layer Size 23\n",
      "Test loss window average  0.5621729  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 52050\n",
      "======== Trial 68 of 99 =========\n",
      "Batch size 24  Iters 37252  Lr 0.005660138109605365  Layers 3  Dropout 0.3074678603664873  Layer Size 27\n",
      "Test loss window average  0.5449551  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 37252\n",
      "======== Trial 69 of 99 =========\n",
      "Batch size 63  Iters 54689  Lr 0.009802688292605976  Layers 4  Dropout 0.1517080505575366  Layer Size 23\n",
      "Test loss window average  0.5225111  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 54689\n",
      "======== Trial 70 of 99 =========\n",
      "Batch size 48  Iters 92560  Lr 0.0006307390851504822  Layers 2  Dropout 0.458724986597413  Layer Size 12\n",
      "Test loss window average  0.54161406  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 92560\n",
      "======== Trial 71 of 99 =========\n",
      "Batch size 52  Iters 70803  Lr 0.0025740433204495153  Layers 4  Dropout 0.49655332167735  Layer Size 4\n",
      "Test loss window average  0.57758296  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 70803\n",
      "======== Trial 72 of 99 =========\n",
      "Batch size 48  Iters 69771  Lr 0.006224911625039382  Layers 4  Dropout 0.42057617821111704  Layer Size 9\n",
      "Test loss window average  0.5736159  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 69771\n",
      "======== Trial 73 of 99 =========\n",
      "Batch size 30  Iters 18794  Lr 0.0038843094242910874  Layers 5  Dropout 0.2858948391330922  Layer Size 4\n",
      "Test loss window average  0.57739115  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 18794\n",
      "======== Trial 74 of 99 =========\n",
      "Batch size 10  Iters 64692  Lr 0.008223924579565863  Layers 5  Dropout 0.14126518445559344  Layer Size 29\n",
      "Test loss window average  0.52508575  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 64692\n",
      "======== Trial 75 of 99 =========\n",
      "Batch size 14  Iters 17240  Lr 0.004186593604495904  Layers 5  Dropout 0.13081849652714875  Layer Size 30\n",
      "Test loss window average  0.53381485  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 17240\n",
      "======== Trial 76 of 99 =========\n",
      "Batch size 20  Iters 58075  Lr 0.006807717083946523  Layers 4  Dropout 0.12551538634748513  Layer Size 9\n",
      "Test loss window average  0.55456716  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 58075\n",
      "======== Trial 77 of 99 =========\n",
      "Batch size 32  Iters 81428  Lr 0.005431394209469657  Layers 4  Dropout 0.45747456057656555  Layer Size 22\n",
      "Test loss window average  0.55838484  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 81428\n",
      "======== Trial 78 of 99 =========\n",
      "Batch size 35  Iters 91385  Lr 0.009133308177537755  Layers 4  Dropout 0.390475650913337  Layer Size 9\n",
      "Test loss window average  0.5584798  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 91385\n",
      "======== Trial 79 of 99 =========\n",
      "Batch size 47  Iters 70687  Lr 0.001769394156636433  Layers 3  Dropout 0.21160640634575736  Layer Size 19\n",
      "Test loss window average  0.5264931  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 70687\n",
      "======== Trial 80 of 99 =========\n",
      "Batch size 43  Iters 53181  Lr 0.006152600908368273  Layers 5  Dropout 0.4330624284110348  Layer Size 17\n",
      "Test loss window average  0.55058604  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 53181\n",
      "======== Trial 81 of 99 =========\n",
      "Batch size 13  Iters 60517  Lr 0.001069684545915582  Layers 3  Dropout 0.4534720828978379  Layer Size 31\n",
      "Test loss window average  0.5503768  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 60517\n",
      "======== Trial 82 of 99 =========\n",
      "Batch size 29  Iters 20927  Lr 0.008460993555567248  Layers 5  Dropout 0.2859632066617961  Layer Size 18\n",
      "Test loss window average  0.55701494  increasing, breaking loop at iter  2795\n",
      "Best number of iterations:  2795 compared with total: 20927\n",
      "======== Trial 83 of 99 =========\n",
      "Batch size 46  Iters 62965  Lr 0.0006601304147755771  Layers 3  Dropout 0.22865074218508658  Layer Size 20\n",
      "Test loss window average  0.55101776  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 62965\n",
      "======== Trial 84 of 99 =========\n",
      "Batch size 51  Iters 82669  Lr 0.0069819670752061  Layers 2  Dropout 0.21804899426496888  Layer Size 17\n",
      "Test loss window average  0.5300324  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 82669\n",
      "======== Trial 85 of 99 =========\n",
      "Batch size 62  Iters 77216  Lr 0.005973488147165569  Layers 4  Dropout 0.3196658353905051  Layer Size 21\n",
      "Test loss window average  0.5290941  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 77216\n",
      "======== Trial 86 of 99 =========\n",
      "Batch size 29  Iters 85255  Lr 0.007582006942952537  Layers 3  Dropout 0.12812765549392344  Layer Size 13\n",
      "Test loss window average  0.5442668  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 85255\n",
      "======== Trial 87 of 99 =========\n",
      "Batch size 31  Iters 96597  Lr 0.0066587298650125715  Layers 5  Dropout 0.2582824456934987  Layer Size 20\n",
      "Test loss window average  0.5244317  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 96597\n",
      "======== Trial 88 of 99 =========\n",
      "Batch size 50  Iters 56673  Lr 0.0077146414783994316  Layers 5  Dropout 0.48027312441146564  Layer Size 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.56389505  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 56673\n",
      "======== Trial 89 of 99 =========\n",
      "Batch size 54  Iters 15583  Lr 0.005946712449003714  Layers 4  Dropout 0.2563889822306886  Layer Size 26\n",
      "Test loss window average  0.5412244  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 15583\n",
      "======== Trial 90 of 99 =========\n",
      "Batch size 62  Iters 60142  Lr 0.006345231492075481  Layers 5  Dropout 0.35216886534870906  Layer Size 28\n",
      "Test loss window average  0.5544638  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 60142\n",
      "======== Trial 91 of 99 =========\n",
      "Batch size 56  Iters 36659  Lr 0.007545046857162993  Layers 3  Dropout 0.4088750321118868  Layer Size 14\n",
      "Test loss window average  0.54534537  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 36659\n",
      "======== Trial 92 of 99 =========\n",
      "Batch size 30  Iters 62426  Lr 0.00039193682250053264  Layers 5  Dropout 0.2901091862640621  Layer Size 24\n",
      "Test loss window average  0.55947536  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 62426\n",
      "======== Trial 93 of 99 =========\n",
      "Batch size 54  Iters 98839  Lr 0.005745968227320935  Layers 4  Dropout 0.3583352060214866  Layer Size 22\n",
      "Test loss window average  0.54330343  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 98839\n",
      "======== Trial 94 of 99 =========\n",
      "Batch size 29  Iters 21760  Lr 0.007375971411475158  Layers 4  Dropout 0.18271725593802085  Layer Size 15\n",
      "Test loss window average  0.544895  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 21760\n",
      "======== Trial 95 of 99 =========\n",
      "Batch size 40  Iters 73131  Lr 0.008898257645128617  Layers 5  Dropout 0.26673223088982556  Layer Size 31\n",
      "Test loss window average  0.5524265  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 73131\n",
      "======== Trial 96 of 99 =========\n",
      "Batch size 41  Iters 79958  Lr 0.005913421407570406  Layers 3  Dropout 0.23387218502618606  Layer Size 17\n",
      "Test loss window average  0.52691627  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 79958\n",
      "======== Trial 97 of 99 =========\n",
      "Batch size 10  Iters 71913  Lr 0.004570562118280619  Layers 3  Dropout 0.4757798559022828  Layer Size 11\n",
      "Test loss window average  0.5599413  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 71913\n",
      "======== Trial 98 of 99 =========\n",
      "Batch size 31  Iters 76466  Lr 0.004965656091123765  Layers 3  Dropout 0.3988833058723461  Layer Size 30\n",
      "Test loss window average  0.55939656  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 76466\n",
      "======== Trial 99 of 99 =========\n",
      "Batch size 49  Iters 57829  Lr 0.009897049364806112  Layers 4  Dropout 0.18964355703572222  Layer Size 12\n",
      "Test loss window average  0.5328508  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 57829\n",
      "======== Trial 0 of 99 =========\n",
      "Batch size 59  Iters 61241  Lr 0.009946391043565126  Layers 5  Dropout 0.309873304185603  Layer Size 13\n",
      "Test loss window average  0.6089067  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 61241\n",
      "old loss: 10000000000.0\n",
      "new loss: 0.608264684677124\n",
      "best model updated\n",
      "======== Trial 1 of 99 =========\n",
      "Batch size 49  Iters 24711  Lr 0.009201702053644586  Layers 4  Dropout 0.38681407557312186  Layer Size 10\n",
      "Test loss window average  0.61608964  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 24711\n",
      "======== Trial 2 of 99 =========\n",
      "Batch size 15  Iters 26843  Lr 0.005298007724761759  Layers 4  Dropout 0.349180102683562  Layer Size 24\n",
      "Test loss window average  0.6112846  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 26843\n",
      "old loss: 0.6049304\n",
      "new loss: 0.6089919209480286\n",
      "best model updated\n",
      "======== Trial 3 of 99 =========\n",
      "Batch size 55  Iters 30588  Lr 0.009137441551546742  Layers 3  Dropout 0.4714985765988643  Layer Size 24\n",
      "Test loss window average  0.6246359  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 30588\n",
      "======== Trial 4 of 99 =========\n",
      "Batch size 18  Iters 70692  Lr 0.008660259921208998  Layers 4  Dropout 0.3113804669398802  Layer Size 18\n",
      "Test loss window average  0.61241347  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 70692\n",
      "======== Trial 5 of 99 =========\n",
      "Batch size 40  Iters 47735  Lr 0.0017052908447467642  Layers 5  Dropout 0.11912941789617162  Layer Size 9\n",
      "Test loss window average  0.6034928  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 47735\n",
      "old loss: 0.60460633\n",
      "new loss: 0.6045919060707092\n",
      "best model updated\n",
      "======== Trial 6 of 99 =========\n",
      "Batch size 36  Iters 94658  Lr 0.006921827246259986  Layers 5  Dropout 0.4625419164261215  Layer Size 14\n",
      "Test loss window average  0.6186455  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 94658\n",
      "======== Trial 7 of 99 =========\n",
      "Batch size 32  Iters 24383  Lr 0.008679602433269448  Layers 2  Dropout 0.418765317970206  Layer Size 20\n",
      "Test loss window average  0.6117702  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 24383\n",
      "======== Trial 8 of 99 =========\n",
      "Batch size 56  Iters 71635  Lr 0.0020537787640702433  Layers 3  Dropout 0.26317801404819074  Layer Size 11\n",
      "Test loss window average  0.6138035  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 71635\n",
      "======== Trial 9 of 99 =========\n",
      "Batch size 59  Iters 49958  Lr 0.006922940416502723  Layers 4  Dropout 0.2626733832606694  Layer Size 11\n",
      "Test loss window average  0.6015633  increasing, breaking loop at iter  3560\n",
      "Best number of iterations:  3560 compared with total: 49958\n",
      "old loss: 0.6013979\n",
      "new loss: 0.6018356084823608\n",
      "best model updated\n",
      "======== Trial 10 of 99 =========\n",
      "Batch size 56  Iters 46168  Lr 0.005531715522260809  Layers 3  Dropout 0.3256214385449956  Layer Size 21\n",
      "Test loss window average  0.6156933  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 46168\n",
      "======== Trial 11 of 99 =========\n",
      "Batch size 23  Iters 20624  Lr 0.003971907625686729  Layers 5  Dropout 0.38774525139284777  Layer Size 27\n",
      "Test loss window average  0.6307273  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 20624\n",
      "======== Trial 12 of 99 =========\n",
      "Batch size 46  Iters 72952  Lr 0.007620382640612953  Layers 4  Dropout 0.3539729177194726  Layer Size 31\n",
      "Test loss window average  0.6047621  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 72952\n",
      "old loss: 0.59685344\n",
      "new loss: 0.6056920289993286\n",
      "best model updated\n",
      "======== Trial 13 of 99 =========\n",
      "Batch size 17  Iters 53180  Lr 0.005627868163640905  Layers 4  Dropout 0.1571359638322666  Layer Size 10\n",
      "Test loss window average  0.615629  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 53180\n",
      "======== Trial 14 of 99 =========\n",
      "Batch size 42  Iters 92611  Lr 0.0032314895040762115  Layers 4  Dropout 0.13493725758169137  Layer Size 24\n",
      "Test loss window average  0.6192118  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 92611\n",
      "======== Trial 15 of 99 =========\n",
      "Batch size 23  Iters 77126  Lr 0.006554670168337952  Layers 3  Dropout 0.22628616473227817  Layer Size 24\n",
      "Test loss window average  0.61872464  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 77126\n",
      "======== Trial 16 of 99 =========\n",
      "Batch size 42  Iters 41508  Lr 0.009528445388992087  Layers 2  Dropout 0.26401479222184704  Layer Size 8\n",
      "Test loss window average  0.6060684  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 41508\n",
      "======== Trial 17 of 99 =========\n",
      "Batch size 48  Iters 56924  Lr 0.0018353056716815623  Layers 2  Dropout 0.2331020446470216  Layer Size 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.60176784  increasing, breaking loop at iter  3305\n",
      "Best number of iterations:  3305 compared with total: 56924\n",
      "======== Trial 18 of 99 =========\n",
      "Batch size 16  Iters 99762  Lr 0.008600681202293922  Layers 3  Dropout 0.44402917885362525  Layer Size 11\n",
      "Test loss window average  0.60615224  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 99762\n",
      "======== Trial 19 of 99 =========\n",
      "Batch size 28  Iters 25053  Lr 0.005563701769054107  Layers 2  Dropout 0.19387046500066749  Layer Size 5\n",
      "Test loss window average  0.6048202  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 25053\n",
      "======== Trial 20 of 99 =========\n",
      "Batch size 56  Iters 31501  Lr 0.0015189994556002728  Layers 5  Dropout 0.1869093648064332  Layer Size 30\n",
      "Test loss window average  0.6168704  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 31501\n",
      "======== Trial 21 of 99 =========\n",
      "Batch size 34  Iters 72358  Lr 0.006572687841856899  Layers 2  Dropout 0.48815160192548446  Layer Size 13\n",
      "Test loss window average  0.61460257  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 72358\n",
      "======== Trial 22 of 99 =========\n",
      "Batch size 34  Iters 61826  Lr 0.0031416997854355603  Layers 2  Dropout 0.16947452016149384  Layer Size 10\n",
      "Test loss window average  0.601037  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 61826\n",
      "======== Trial 23 of 99 =========\n",
      "Batch size 21  Iters 40028  Lr 0.00033861573205717807  Layers 4  Dropout 0.412620778140419  Layer Size 29\n",
      "Test loss window average  0.6211189  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 40028\n",
      "======== Trial 24 of 99 =========\n",
      "Batch size 13  Iters 42514  Lr 0.009367500887711384  Layers 2  Dropout 0.42419025010968425  Layer Size 20\n",
      "Test loss window average  0.58972377  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 42514\n",
      "old loss: 0.59423685\n",
      "new loss: 0.5893937349319458\n",
      "best model updated\n",
      "======== Trial 25 of 99 =========\n",
      "Batch size 16  Iters 16022  Lr 0.003825070488729074  Layers 2  Dropout 0.4790246454374758  Layer Size 10\n",
      "Test loss window average  0.6125854  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 16022\n",
      "======== Trial 26 of 99 =========\n",
      "Batch size 36  Iters 88759  Lr 0.0029790168120513885  Layers 2  Dropout 0.23304536470896908  Layer Size 16\n",
      "Test loss window average  0.62245095  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 88759\n",
      "======== Trial 27 of 99 =========\n",
      "Batch size 44  Iters 29502  Lr 0.005967127939729755  Layers 5  Dropout 0.4667661959523792  Layer Size 31\n",
      "Test loss window average  0.61917734  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 29502\n",
      "======== Trial 28 of 99 =========\n",
      "Batch size 32  Iters 68146  Lr 0.009649573902587314  Layers 4  Dropout 0.3921158984899219  Layer Size 4\n",
      "Test loss window average  0.63411325  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 68146\n",
      "======== Trial 29 of 99 =========\n",
      "Batch size 15  Iters 36970  Lr 0.007282289268656679  Layers 3  Dropout 0.31038360634981566  Layer Size 26\n",
      "Test loss window average  0.60039717  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 36970\n",
      "======== Trial 30 of 99 =========\n",
      "Batch size 42  Iters 51726  Lr 0.0075620767841576095  Layers 5  Dropout 0.19982950382388254  Layer Size 6\n",
      "Test loss window average  0.6115665  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 51726\n",
      "======== Trial 31 of 99 =========\n",
      "Batch size 21  Iters 68675  Lr 0.004219066939904096  Layers 4  Dropout 0.22693318117992353  Layer Size 13\n",
      "Test loss window average  0.61232793  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 68675\n",
      "======== Trial 32 of 99 =========\n",
      "Batch size 40  Iters 85296  Lr 0.007879324460908308  Layers 3  Dropout 0.11597340855572989  Layer Size 5\n",
      "Test loss window average  0.6062284  increasing, breaking loop at iter  3305\n",
      "Best number of iterations:  3305 compared with total: 85296\n",
      "======== Trial 33 of 99 =========\n",
      "Batch size 29  Iters 28121  Lr 0.006848299365602363  Layers 3  Dropout 0.33878905818402344  Layer Size 14\n",
      "Test loss window average  0.61327  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 28121\n",
      "======== Trial 34 of 99 =========\n",
      "Batch size 33  Iters 40471  Lr 0.0036445520432007977  Layers 3  Dropout 0.20878597061387383  Layer Size 29\n",
      "Test loss window average  0.62058115  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 40471\n",
      "======== Trial 35 of 99 =========\n",
      "Batch size 58  Iters 66718  Lr 0.002738689152802706  Layers 5  Dropout 0.20495565831466367  Layer Size 19\n",
      "Test loss window average  0.60680383  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 66718\n",
      "======== Trial 36 of 99 =========\n",
      "Batch size 10  Iters 72121  Lr 0.007704074079684774  Layers 5  Dropout 0.17440769313834087  Layer Size 10\n",
      "Test loss window average  0.60440576  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 72121\n",
      "======== Trial 37 of 99 =========\n",
      "Batch size 13  Iters 25095  Lr 0.0019447397561156085  Layers 5  Dropout 0.3557005438232389  Layer Size 14\n",
      "Test loss window average  0.5985642  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 25095\n",
      "======== Trial 38 of 99 =========\n",
      "Batch size 32  Iters 61563  Lr 0.0006060918531349776  Layers 4  Dropout 0.30902165344116184  Layer Size 20\n",
      "Test loss window average  0.61210287  increasing, breaking loop at iter  3305\n",
      "Best number of iterations:  3305 compared with total: 61563\n",
      "======== Trial 39 of 99 =========\n",
      "Batch size 13  Iters 26160  Lr 0.008357184897199008  Layers 4  Dropout 0.15272383407183765  Layer Size 19\n",
      "Test loss window average  0.6152324  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 26160\n",
      "======== Trial 40 of 99 =========\n",
      "Batch size 50  Iters 91190  Lr 0.0030036354832605307  Layers 2  Dropout 0.4618368189043687  Layer Size 17\n",
      "Test loss window average  0.6038753  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 91190\n",
      "======== Trial 41 of 99 =========\n",
      "Batch size 46  Iters 42530  Lr 0.000905828889766374  Layers 2  Dropout 0.442687318323877  Layer Size 13\n",
      "Test loss window average  0.62285393  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 42530\n",
      "======== Trial 42 of 99 =========\n",
      "Batch size 23  Iters 62311  Lr 0.0020226292614990912  Layers 4  Dropout 0.48408816524776765  Layer Size 29\n",
      "Test loss window average  0.6088465  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 62311\n",
      "======== Trial 43 of 99 =========\n",
      "Batch size 37  Iters 36761  Lr 0.008983152051071298  Layers 2  Dropout 0.3326699140089264  Layer Size 6\n",
      "Test loss window average  0.5990918  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 36761\n",
      "======== Trial 44 of 99 =========\n",
      "Batch size 20  Iters 68588  Lr 0.002632772657535824  Layers 2  Dropout 0.4378310377435417  Layer Size 26\n",
      "Test loss window average  0.6129227  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 68588\n",
      "======== Trial 45 of 99 =========\n",
      "Batch size 35  Iters 86742  Lr 0.005792918572394194  Layers 4  Dropout 0.2995867159628105  Layer Size 18\n",
      "Test loss window average  0.60363215  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 86742\n",
      "======== Trial 46 of 99 =========\n",
      "Batch size 27  Iters 44748  Lr 0.004844540515314613  Layers 4  Dropout 0.38286702010311835  Layer Size 6\n",
      "Test loss window average  0.62647027  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 44748\n",
      "======== Trial 47 of 99 =========\n",
      "Batch size 60  Iters 93891  Lr 0.008688355797449311  Layers 5  Dropout 0.38388396218801113  Layer Size 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.60400575  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 93891\n",
      "======== Trial 48 of 99 =========\n",
      "Batch size 13  Iters 87302  Lr 0.0012112498696272168  Layers 4  Dropout 0.34327641158155786  Layer Size 6\n",
      "Test loss window average  0.62694  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 87302\n",
      "======== Trial 49 of 99 =========\n",
      "Batch size 63  Iters 56946  Lr 0.005814293471555924  Layers 5  Dropout 0.3482267471183398  Layer Size 10\n",
      "Test loss window average  0.63742036  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 56946\n",
      "======== Trial 50 of 99 =========\n",
      "Batch size 57  Iters 72441  Lr 0.009553360013495868  Layers 5  Dropout 0.3057687082254204  Layer Size 18\n",
      "Test loss window average  0.59779716  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 72441\n",
      "======== Trial 51 of 99 =========\n",
      "Batch size 17  Iters 57205  Lr 0.0014318175224936986  Layers 2  Dropout 0.10517929738897576  Layer Size 14\n",
      "Test loss window average  0.59578687  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 57205\n",
      "======== Trial 52 of 99 =========\n",
      "Batch size 36  Iters 77809  Lr 0.004271554272783615  Layers 3  Dropout 0.4050523286206844  Layer Size 28\n",
      "Test loss window average  0.61782587  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 77809\n",
      "======== Trial 53 of 99 =========\n",
      "Batch size 46  Iters 16385  Lr 0.00403382760549747  Layers 2  Dropout 0.3340153426579893  Layer Size 7\n",
      "Test loss window average  0.60601956  increasing, breaking loop at iter  2795\n",
      "Best number of iterations:  2795 compared with total: 16385\n",
      "======== Trial 54 of 99 =========\n",
      "Batch size 60  Iters 95642  Lr 0.009717447689297146  Layers 4  Dropout 0.41770453699514754  Layer Size 19\n",
      "Test loss window average  0.62152183  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 95642\n",
      "======== Trial 55 of 99 =========\n",
      "Batch size 27  Iters 76380  Lr 0.008044889596424592  Layers 2  Dropout 0.3075221944782002  Layer Size 23\n",
      "Test loss window average  0.58813673  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 76380\n",
      "======== Trial 56 of 99 =========\n",
      "Batch size 24  Iters 75816  Lr 0.0021801501676933345  Layers 5  Dropout 0.22178997290566527  Layer Size 27\n",
      "Test loss window average  0.6156996  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 75816\n",
      "======== Trial 57 of 99 =========\n",
      "Batch size 22  Iters 56771  Lr 0.007978405054413036  Layers 2  Dropout 0.19745567006067974  Layer Size 18\n",
      "Test loss window average  0.60775834  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 56771\n",
      "======== Trial 58 of 99 =========\n",
      "Batch size 33  Iters 63352  Lr 0.009315661137963647  Layers 4  Dropout 0.49001971845683323  Layer Size 28\n",
      "Test loss window average  0.6280333  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 63352\n",
      "======== Trial 59 of 99 =========\n",
      "Batch size 12  Iters 71505  Lr 0.0031043094387374465  Layers 5  Dropout 0.27310201785097227  Layer Size 4\n",
      "Test loss window average  0.6295584  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 71505\n",
      "======== Trial 60 of 99 =========\n",
      "Batch size 45  Iters 55317  Lr 0.0058548352721734705  Layers 2  Dropout 0.4768714368716567  Layer Size 13\n",
      "Test loss window average  0.60421216  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 55317\n",
      "======== Trial 61 of 99 =========\n",
      "Batch size 42  Iters 65122  Lr 0.00749898774351288  Layers 4  Dropout 0.12771801503360966  Layer Size 26\n",
      "Test loss window average  0.5948183  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 65122\n",
      "======== Trial 62 of 99 =========\n",
      "Batch size 18  Iters 26172  Lr 0.005088544197649923  Layers 2  Dropout 0.11309697859011775  Layer Size 24\n",
      "Test loss window average  0.6077552  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 26172\n",
      "======== Trial 63 of 99 =========\n",
      "Batch size 63  Iters 59255  Lr 0.008605059559573927  Layers 5  Dropout 0.2595006185246836  Layer Size 7\n",
      "Test loss window average  0.62552154  increasing, breaking loop at iter  2795\n",
      "Best number of iterations:  2795 compared with total: 59255\n",
      "======== Trial 64 of 99 =========\n",
      "Batch size 43  Iters 60357  Lr 0.007770313353537774  Layers 4  Dropout 0.22999109960502337  Layer Size 9\n",
      "Test loss window average  0.6111923  increasing, breaking loop at iter  2795\n",
      "Best number of iterations:  2795 compared with total: 60357\n",
      "======== Trial 65 of 99 =========\n",
      "Batch size 62  Iters 88909  Lr 0.0003958094676550914  Layers 5  Dropout 0.23954279964549188  Layer Size 12\n",
      "Test loss window average  0.6338252  increasing, breaking loop at iter  3560\n",
      "Best number of iterations:  3560 compared with total: 88909\n",
      "======== Trial 66 of 99 =========\n",
      "Batch size 45  Iters 20394  Lr 0.006514510945490352  Layers 2  Dropout 0.2721932741596165  Layer Size 27\n",
      "Test loss window average  0.5827519  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 20394\n",
      "old loss: 0.5826606\n",
      "new loss: 0.5898142457008362\n",
      "best model updated\n",
      "======== Trial 67 of 99 =========\n",
      "Batch size 63  Iters 42933  Lr 0.006713102455929158  Layers 2  Dropout 0.2069663689244141  Layer Size 7\n",
      "Test loss window average  0.60904276  increasing, breaking loop at iter  2795\n",
      "Best number of iterations:  2795 compared with total: 42933\n",
      "======== Trial 68 of 99 =========\n",
      "Batch size 23  Iters 27068  Lr 0.007587738768256624  Layers 5  Dropout 0.2937810974434194  Layer Size 27\n",
      "Test loss window average  0.6058805  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 27068\n",
      "======== Trial 69 of 99 =========\n",
      "Batch size 45  Iters 15022  Lr 0.0030676547725265314  Layers 5  Dropout 0.2684158433680431  Layer Size 28\n",
      "Test loss window average  0.618888  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 15022\n",
      "======== Trial 70 of 99 =========\n",
      "Batch size 57  Iters 78680  Lr 0.003168666895709219  Layers 2  Dropout 0.16760213128514445  Layer Size 18\n",
      "Test loss window average  0.61265385  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 78680\n",
      "======== Trial 71 of 99 =========\n",
      "Batch size 17  Iters 83307  Lr 0.008547404320691492  Layers 2  Dropout 0.2027126008594308  Layer Size 19\n",
      "Test loss window average  0.60954136  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 83307\n",
      "======== Trial 72 of 99 =========\n",
      "Batch size 41  Iters 43669  Lr 0.006681894033299934  Layers 2  Dropout 0.4120897352258436  Layer Size 27\n",
      "Test loss window average  0.6213023  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 43669\n",
      "======== Trial 73 of 99 =========\n",
      "Batch size 34  Iters 73335  Lr 0.006378929274031649  Layers 3  Dropout 0.432971657701546  Layer Size 5\n",
      "Test loss window average  0.62541234  increasing, breaking loop at iter  2795\n",
      "Best number of iterations:  2795 compared with total: 73335\n",
      "======== Trial 74 of 99 =========\n",
      "Batch size 55  Iters 49150  Lr 0.00371369468479238  Layers 4  Dropout 0.44719109766524645  Layer Size 18\n",
      "Test loss window average  0.6344677  increasing, breaking loop at iter  2795\n",
      "Best number of iterations:  2795 compared with total: 49150\n",
      "======== Trial 75 of 99 =========\n",
      "Batch size 56  Iters 59792  Lr 0.005137478201364894  Layers 4  Dropout 0.4133072244561935  Layer Size 31\n",
      "Test loss window average  0.6196586  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 59792\n",
      "======== Trial 76 of 99 =========\n",
      "Batch size 11  Iters 65894  Lr 0.0017113490193020645  Layers 3  Dropout 0.4488150674295419  Layer Size 12\n",
      "Test loss window average  0.6077662  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 65894\n",
      "======== Trial 77 of 99 =========\n",
      "Batch size 30  Iters 54275  Lr 0.005312604497833137  Layers 4  Dropout 0.40521772239810494  Layer Size 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.6124605  increasing, breaking loop at iter  3560\n",
      "Best number of iterations:  3560 compared with total: 54275\n",
      "======== Trial 78 of 99 =========\n",
      "Batch size 56  Iters 31139  Lr 0.009462922974070786  Layers 4  Dropout 0.1642638226542005  Layer Size 18\n",
      "Test loss window average  0.5942726  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 31139\n",
      "======== Trial 79 of 99 =========\n",
      "Batch size 38  Iters 33433  Lr 0.00016781569185157416  Layers 5  Dropout 0.47789595359312176  Layer Size 5\n",
      "Test loss window average  0.62428164  increasing, breaking loop at iter  6365\n",
      "Best number of iterations:  6365 compared with total: 33433\n",
      "======== Trial 80 of 99 =========\n",
      "Batch size 40  Iters 54031  Lr 0.004275754442332037  Layers 5  Dropout 0.2396424525542548  Layer Size 4\n",
      "Test loss window average  0.61966735  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 54031\n",
      "======== Trial 81 of 99 =========\n",
      "Batch size 40  Iters 29221  Lr 0.005604358449293397  Layers 5  Dropout 0.18826471271728012  Layer Size 18\n",
      "Test loss window average  0.6169587  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 29221\n",
      "======== Trial 82 of 99 =========\n",
      "Batch size 21  Iters 95491  Lr 0.0017894801638889538  Layers 5  Dropout 0.4361661441184477  Layer Size 12\n",
      "Test loss window average  0.6325628  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 95491\n",
      "======== Trial 83 of 99 =========\n",
      "Batch size 31  Iters 69701  Lr 0.0007163597795928669  Layers 2  Dropout 0.2062902399462903  Layer Size 19\n",
      "Test loss window average  0.5944702  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 69701\n",
      "======== Trial 84 of 99 =========\n",
      "Batch size 19  Iters 88007  Lr 0.006792345466432394  Layers 3  Dropout 0.42497559933845186  Layer Size 28\n",
      "Test loss window average  0.61109585  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 88007\n",
      "======== Trial 85 of 99 =========\n",
      "Batch size 13  Iters 90278  Lr 0.006930467857753671  Layers 2  Dropout 0.3552890725293506  Layer Size 15\n",
      "Test loss window average  0.621246  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 90278\n",
      "======== Trial 86 of 99 =========\n",
      "Batch size 26  Iters 96439  Lr 0.0020874944721004985  Layers 5  Dropout 0.3054683605342477  Layer Size 18\n",
      "Test loss window average  0.6276758  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 96439\n",
      "======== Trial 87 of 99 =========\n",
      "Batch size 33  Iters 32567  Lr 0.006660638085436935  Layers 2  Dropout 0.25280848040550563  Layer Size 9\n",
      "Test loss window average  0.6039692  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 32567\n",
      "======== Trial 88 of 99 =========\n",
      "Batch size 33  Iters 31692  Lr 0.006431983998596306  Layers 5  Dropout 0.4601980570055957  Layer Size 25\n",
      "Test loss window average  0.637165  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 31692\n",
      "======== Trial 89 of 99 =========\n",
      "Batch size 12  Iters 90014  Lr 0.0038476764683913014  Layers 4  Dropout 0.4764358258178991  Layer Size 28\n",
      "Test loss window average  0.61888033  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 90014\n",
      "======== Trial 90 of 99 =========\n",
      "Batch size 41  Iters 78763  Lr 0.0057095906451768465  Layers 5  Dropout 0.12057251343443585  Layer Size 28\n",
      "Test loss window average  0.6047121  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 78763\n",
      "======== Trial 91 of 99 =========\n",
      "Batch size 63  Iters 35261  Lr 0.005208413387944667  Layers 2  Dropout 0.11959700320002127  Layer Size 21\n",
      "Test loss window average  0.5959637  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 35261\n",
      "======== Trial 92 of 99 =========\n",
      "Batch size 57  Iters 88982  Lr 0.005647872099871113  Layers 5  Dropout 0.4789998978472657  Layer Size 18\n",
      "Test loss window average  0.6063878  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 88982\n",
      "======== Trial 93 of 99 =========\n",
      "Batch size 25  Iters 38722  Lr 0.00412211291558192  Layers 3  Dropout 0.3735276474195389  Layer Size 10\n",
      "Test loss window average  0.60379416  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 38722\n",
      "======== Trial 94 of 99 =========\n",
      "Batch size 12  Iters 47403  Lr 0.0037975075871991447  Layers 3  Dropout 0.42766740593388197  Layer Size 15\n",
      "Test loss window average  0.6108602  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 47403\n",
      "======== Trial 95 of 99 =========\n",
      "Batch size 15  Iters 26880  Lr 0.005770505870221924  Layers 3  Dropout 0.23189560397971093  Layer Size 14\n",
      "Test loss window average  0.59896976  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 26880\n",
      "======== Trial 96 of 99 =========\n",
      "Batch size 13  Iters 49195  Lr 0.001725915592078492  Layers 2  Dropout 0.2168038318525699  Layer Size 29\n",
      "Test loss window average  0.59741366  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 49195\n",
      "======== Trial 97 of 99 =========\n",
      "Batch size 23  Iters 72959  Lr 0.004019584172374405  Layers 3  Dropout 0.15658320178519133  Layer Size 18\n",
      "Test loss window average  0.59176004  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 72959\n",
      "======== Trial 98 of 99 =========\n",
      "Batch size 35  Iters 67190  Lr 0.006944270302958629  Layers 5  Dropout 0.14761889822434054  Layer Size 24\n",
      "Test loss window average  0.5952597  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 67190\n",
      "======== Trial 99 of 99 =========\n",
      "Batch size 21  Iters 86974  Lr 0.004979201798194986  Layers 4  Dropout 0.28610679138099204  Layer Size 24\n",
      "Test loss window average  0.60682803  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 86974\n"
     ]
    }
   ],
   "source": [
    "# First establish ground truth treatment effect:\n",
    "N = 5000000\n",
    "Z, x, y, Y1, Y0 = generate_data(N, seed=0)\n",
    "true_psi = (Y1-Y0).mean()\n",
    "\n",
    "\n",
    "# Set some params\n",
    "N = 10000\n",
    "seed = 0\n",
    "num_tuning_trials = 100\n",
    "\n",
    "# data generation:\n",
    "z, x, y, _, _ = generate_data(N, 0)\n",
    "x = torch.tensor(x).type(torch.float32)\n",
    "z = torch.tensor(z).type(torch.float32)\n",
    "y = torch.tensor(y).type(torch.float32)\n",
    "    \n",
    "qtuner = Tuner(x=x,y=y,z=z, net_type='Q', trials=num_tuning_trials)\n",
    "qtuning_history, best_q = qtuner.tune()\n",
    "\n",
    "qtotal_losses = np.asarray(qtuning_history['best_model_test_loss'])\n",
    "qbest_index = np.argmin(qtotal_losses)\n",
    "\n",
    "qbest_params = {}\n",
    "for key in qtuning_history.keys():\n",
    "    qbest_params[key] = qtuning_history[key][qbest_index]\n",
    "    \n",
    "    \n",
    "gtuner = Tuner(x=x,y=y,z=z, net_type='G', trials=num_tuning_trials)\n",
    "gtuning_history, best_g = gtuner.tune()\n",
    "\n",
    "gtotal_losses = np.asarray(gtuning_history['best_model_test_loss'])\n",
    "gbest_index = np.argmin(gtotal_losses)\n",
    "\n",
    "gbest_params = {}\n",
    "for key in gtuning_history.keys():\n",
    "    gbest_params[key] = gtuning_history[key][gbest_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4090404",
   "metadata": {},
   "source": [
    "## 6. Run Simulation\n",
    "\n",
    "Now we have the best hyperparameters, we will run the simulations accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9709b014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Q params: {'batch_size': 44, 'layers': 2, 'dropout': 0.1669462842670925, 'layer_size': 24, 'lr': 0.009933456894985612, 'iters': 56972, 'stop_it': 1520, 'train_loss': 0.5332282781600952, 'val_loss': 0.5039693713188171, 'best_model_test_loss': array(0.49804965, dtype=float32)}\n",
      "Best G params: {'batch_size': 45, 'layers': 2, 'dropout': 0.2721932741596165, 'layer_size': 27, 'lr': 0.006514510945490352, 'iters': 20394, 'stop_it': 1010, 'train_loss': 0.6068882942199707, 'val_loss': 0.5898142457008362, 'best_model_test_loss': array(0.5776651, dtype=float32)}\n",
      "=====================RUN 0===================\n",
      "Test loss window average  0.5264258  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5920086  increasing, breaking loop at iter  1520\n",
      "=====================RUN 1===================\n",
      "Test loss window average  0.54358584  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6072991  increasing, breaking loop at iter  1520\n",
      "=====================RUN 2===================\n",
      "Test loss window average  0.5336688  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.59118104  increasing, breaking loop at iter  1265\n",
      "=====================RUN 3===================\n",
      "Test loss window average  0.5201708  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6134254  increasing, breaking loop at iter  1010\n",
      "=====================RUN 4===================\n",
      "Test loss window average  0.54233986  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.58950174  increasing, breaking loop at iter  1775\n",
      "=====================RUN 5===================\n",
      "Test loss window average  0.5089696  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60135436  increasing, breaking loop at iter  1010\n",
      "=====================RUN 6===================\n",
      "Test loss window average  0.550535  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.61921424  increasing, breaking loop at iter  2030\n",
      "=====================RUN 7===================\n",
      "Test loss window average  0.5367476  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.596858  increasing, breaking loop at iter  1010\n",
      "=====================RUN 8===================\n",
      "Test loss window average  0.5429135  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.59936327  increasing, breaking loop at iter  1265\n",
      "=====================RUN 9===================\n",
      "Test loss window average  0.5450719  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.61500657  increasing, breaking loop at iter  1010\n",
      "=====================RUN 10===================\n",
      "Test loss window average  0.5523238  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60123754  increasing, breaking loop at iter  1775\n",
      "=====================RUN 11===================\n",
      "Test loss window average  0.5342362  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.61667037  increasing, breaking loop at iter  1010\n",
      "=====================RUN 12===================\n",
      "Test loss window average  0.52800953  increasing, breaking loop at iter  2540\n",
      "Test loss window average  0.62763894  increasing, breaking loop at iter  1775\n",
      "=====================RUN 13===================\n",
      "Test loss window average  0.5191988  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6148386  increasing, breaking loop at iter  1520\n",
      "=====================RUN 14===================\n",
      "Test loss window average  0.5333303  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6276552  increasing, breaking loop at iter  1010\n",
      "=====================RUN 15===================\n",
      "Test loss window average  0.5397191  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.61699665  increasing, breaking loop at iter  1265\n",
      "=====================RUN 16===================\n",
      "Test loss window average  0.52471733  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.62600714  increasing, breaking loop at iter  755\n",
      "=====================RUN 17===================\n",
      "Test loss window average  0.53169614  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6125165  increasing, breaking loop at iter  1010\n",
      "=====================RUN 18===================\n",
      "Test loss window average  0.5443432  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6287056  increasing, breaking loop at iter  1265\n",
      "=====================RUN 19===================\n",
      "Test loss window average  0.552693  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60655785  increasing, breaking loop at iter  2285\n",
      "=====================RUN 20===================\n",
      "Test loss window average  0.54130286  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.60647166  increasing, breaking loop at iter  1520\n",
      "=====================RUN 21===================\n",
      "Test loss window average  0.5484194  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6135754  increasing, breaking loop at iter  1265\n",
      "=====================RUN 22===================\n",
      "Test loss window average  0.5384366  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.5979518  increasing, breaking loop at iter  1775\n",
      "=====================RUN 23===================\n",
      "Test loss window average  0.5215759  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6080755  increasing, breaking loop at iter  2030\n",
      "=====================RUN 24===================\n",
      "Test loss window average  0.52762556  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60963756  increasing, breaking loop at iter  1010\n",
      "=====================RUN 25===================\n",
      "Test loss window average  0.5268012  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.62162495  increasing, breaking loop at iter  1265\n",
      "=====================RUN 26===================\n",
      "Test loss window average  0.52709395  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60445356  increasing, breaking loop at iter  755\n",
      "=====================RUN 27===================\n",
      "Test loss window average  0.5276711  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6031454  increasing, breaking loop at iter  1775\n",
      "=====================RUN 28===================\n",
      "Test loss window average  0.5484921  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.5999447  increasing, breaking loop at iter  1265\n",
      "=====================RUN 29===================\n",
      "Test loss window average  0.51143724  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5994017  increasing, breaking loop at iter  2030\n",
      "=====================RUN 30===================\n",
      "Test loss window average  0.55194736  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6049095  increasing, breaking loop at iter  1520\n",
      "=====================RUN 31===================\n",
      "Test loss window average  0.5351385  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6101872  increasing, breaking loop at iter  1775\n",
      "=====================RUN 32===================\n",
      "Test loss window average  0.5320451  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5836559  increasing, breaking loop at iter  1265\n",
      "=====================RUN 33===================\n",
      "Test loss window average  0.5272697  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.61331546  increasing, breaking loop at iter  1520\n",
      "=====================RUN 34===================\n",
      "Test loss window average  0.520171  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60682094  increasing, breaking loop at iter  1520\n",
      "=====================RUN 35===================\n",
      "Test loss window average  0.54407007  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.61051327  increasing, breaking loop at iter  2285\n",
      "=====================RUN 36===================\n",
      "Test loss window average  0.5426144  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60780066  increasing, breaking loop at iter  755\n",
      "=====================RUN 37===================\n",
      "Test loss window average  0.5529883  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6061616  increasing, breaking loop at iter  2030\n",
      "=====================RUN 38===================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.5482755  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.62042594  increasing, breaking loop at iter  1265\n",
      "=====================RUN 39===================\n",
      "Test loss window average  0.513426  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5983608  increasing, breaking loop at iter  1520\n",
      "=====================RUN 40===================\n",
      "Test loss window average  0.52713364  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6057052  increasing, breaking loop at iter  2540\n",
      "=====================RUN 41===================\n",
      "Test loss window average  0.5495275  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.610735  increasing, breaking loop at iter  2285\n",
      "=====================RUN 42===================\n",
      "Test loss window average  0.5268857  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6079445  increasing, breaking loop at iter  755\n",
      "=====================RUN 43===================\n",
      "Test loss window average  0.50737524  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60289115  increasing, breaking loop at iter  2030\n",
      "=====================RUN 44===================\n",
      "Test loss window average  0.51695305  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.607621  increasing, breaking loop at iter  1010\n",
      "=====================RUN 45===================\n",
      "Test loss window average  0.5160496  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.5990349  increasing, breaking loop at iter  1520\n",
      "=====================RUN 46===================\n",
      "Test loss window average  0.5460176  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6023079  increasing, breaking loop at iter  755\n",
      "=====================RUN 47===================\n",
      "Test loss window average  0.5273748  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.5993765  increasing, breaking loop at iter  1775\n",
      "=====================RUN 48===================\n",
      "Test loss window average  0.5261819  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6102695  increasing, breaking loop at iter  2285\n",
      "=====================RUN 49===================\n",
      "Test loss window average  0.5351526  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5892772  increasing, breaking loop at iter  755\n",
      "=====================RUN 50===================\n",
      "Test loss window average  0.5155819  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.59176695  increasing, breaking loop at iter  1010\n",
      "=====================RUN 51===================\n",
      "Test loss window average  0.53099024  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.60743976  increasing, breaking loop at iter  1265\n",
      "=====================RUN 52===================\n",
      "Test loss window average  0.56712127  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6001234  increasing, breaking loop at iter  1520\n",
      "=====================RUN 53===================\n",
      "Test loss window average  0.54670405  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.5915916  increasing, breaking loop at iter  2030\n",
      "=====================RUN 54===================\n",
      "Test loss window average  0.5449335  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.59414876  increasing, breaking loop at iter  1010\n",
      "=====================RUN 55===================\n",
      "Test loss window average  0.53641003  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.612127  increasing, breaking loop at iter  1775\n",
      "=====================RUN 56===================\n",
      "Test loss window average  0.5345006  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60610473  increasing, breaking loop at iter  1520\n",
      "=====================RUN 57===================\n",
      "Test loss window average  0.5412322  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.60567826  increasing, breaking loop at iter  1265\n",
      "=====================RUN 58===================\n",
      "Test loss window average  0.54729205  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6090934  increasing, breaking loop at iter  1265\n",
      "=====================RUN 59===================\n",
      "Test loss window average  0.53152514  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.5953741  increasing, breaking loop at iter  1775\n",
      "=====================RUN 60===================\n",
      "Test loss window average  0.5144267  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60846394  increasing, breaking loop at iter  1775\n",
      "=====================RUN 61===================\n",
      "Test loss window average  0.51772016  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.57952076  increasing, breaking loop at iter  1265\n",
      "=====================RUN 62===================\n",
      "Test loss window average  0.53710914  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6198767  increasing, breaking loop at iter  2795\n",
      "=====================RUN 63===================\n",
      "Test loss window average  0.51010203  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6060515  increasing, breaking loop at iter  1010\n",
      "=====================RUN 64===================\n",
      "Test loss window average  0.51211786  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6036349  increasing, breaking loop at iter  1010\n",
      "=====================RUN 65===================\n",
      "Test loss window average  0.56275487  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.5989698  increasing, breaking loop at iter  2540\n",
      "=====================RUN 66===================\n",
      "Test loss window average  0.55130064  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60318065  increasing, breaking loop at iter  1775\n",
      "=====================RUN 67===================\n",
      "Test loss window average  0.55375266  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.607448  increasing, breaking loop at iter  755\n",
      "=====================RUN 68===================\n",
      "Test loss window average  0.54926956  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60929525  increasing, breaking loop at iter  1775\n",
      "=====================RUN 69===================\n",
      "Test loss window average  0.5158428  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.61266166  increasing, breaking loop at iter  1520\n",
      "=====================RUN 70===================\n",
      "Test loss window average  0.5294059  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60676646  increasing, breaking loop at iter  2030\n",
      "=====================RUN 71===================\n",
      "Test loss window average  0.5402734  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6228294  increasing, breaking loop at iter  1520\n",
      "=====================RUN 72===================\n",
      "Test loss window average  0.54526234  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.61360514  increasing, breaking loop at iter  1775\n",
      "=====================RUN 73===================\n",
      "Test loss window average  0.54339373  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6119558  increasing, breaking loop at iter  1775\n",
      "=====================RUN 74===================\n",
      "Test loss window average  0.5361808  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6098015  increasing, breaking loop at iter  1775\n",
      "=====================RUN 75===================\n",
      "Test loss window average  0.54244655  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.61288416  increasing, breaking loop at iter  755\n",
      "=====================RUN 76===================\n",
      "Test loss window average  0.53639024  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6059957  increasing, breaking loop at iter  2030\n",
      "=====================RUN 77===================\n",
      "Test loss window average  0.5312299  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.59132886  increasing, breaking loop at iter  755\n",
      "=====================RUN 78===================\n",
      "Test loss window average  0.53796  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.60604525  increasing, breaking loop at iter  755\n",
      "=====================RUN 79===================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.52606547  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6027126  increasing, breaking loop at iter  1010\n",
      "=====================RUN 80===================\n",
      "Test loss window average  0.5291263  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5826075  increasing, breaking loop at iter  2285\n",
      "=====================RUN 81===================\n",
      "Test loss window average  0.5364466  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.61230165  increasing, breaking loop at iter  1265\n",
      "=====================RUN 82===================\n",
      "Test loss window average  0.5335691  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.5957792  increasing, breaking loop at iter  755\n",
      "=====================RUN 83===================\n",
      "Test loss window average  0.5459232  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.62564856  increasing, breaking loop at iter  1520\n",
      "=====================RUN 84===================\n",
      "Test loss window average  0.5448962  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.60552764  increasing, breaking loop at iter  755\n",
      "=====================RUN 85===================\n",
      "Test loss window average  0.5320719  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6181976  increasing, breaking loop at iter  1265\n",
      "=====================RUN 86===================\n",
      "Test loss window average  0.5563024  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.61102027  increasing, breaking loop at iter  2030\n",
      "=====================RUN 87===================\n",
      "Test loss window average  0.5407225  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6072761  increasing, breaking loop at iter  2030\n",
      "=====================RUN 88===================\n",
      "Test loss window average  0.535413  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60641694  increasing, breaking loop at iter  1520\n",
      "=====================RUN 89===================\n",
      "Test loss window average  0.53825647  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6195407  increasing, breaking loop at iter  1265\n",
      "=====================RUN 90===================\n",
      "Test loss window average  0.5475155  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6125125  increasing, breaking loop at iter  1265\n",
      "=====================RUN 91===================\n",
      "Test loss window average  0.53939307  increasing, breaking loop at iter  2540\n",
      "Test loss window average  0.6020173  increasing, breaking loop at iter  1265\n",
      "=====================RUN 92===================\n",
      "Test loss window average  0.52638185  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.60232127  increasing, breaking loop at iter  1265\n",
      "=====================RUN 93===================\n",
      "Test loss window average  0.5467579  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.61334914  increasing, breaking loop at iter  2030\n",
      "=====================RUN 94===================\n",
      "Test loss window average  0.54174745  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.60348403  increasing, breaking loop at iter  1520\n",
      "=====================RUN 95===================\n",
      "Test loss window average  0.553007  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6039887  increasing, breaking loop at iter  1775\n",
      "=====================RUN 96===================\n",
      "Test loss window average  0.5259343  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.6012722  increasing, breaking loop at iter  1010\n",
      "=====================RUN 97===================\n",
      "Test loss window average  0.54390466  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6084177  increasing, breaking loop at iter  1520\n",
      "=====================RUN 98===================\n",
      "Test loss window average  0.53885245  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60142046  increasing, breaking loop at iter  1775\n",
      "=====================RUN 99===================\n",
      "Test loss window average  0.5262268  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.59998107  increasing, breaking loop at iter  2795\n",
      "=====================RUN 100===================\n",
      "Test loss window average  0.54539156  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6113147  increasing, breaking loop at iter  755\n",
      "=====================RUN 101===================\n",
      "Test loss window average  0.54501015  increasing, breaking loop at iter  2285\n",
      "Test loss window average  0.6048108  increasing, breaking loop at iter  1010\n",
      "=====================RUN 102===================\n",
      "Test loss window average  0.5527295  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.60480577  increasing, breaking loop at iter  1775\n",
      "=====================RUN 103===================\n",
      "Test loss window average  0.5300105  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.5946256  increasing, breaking loop at iter  1265\n",
      "=====================RUN 104===================\n",
      "Test loss window average  0.5155356  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6129829  increasing, breaking loop at iter  1520\n",
      "=====================RUN 105===================\n",
      "Test loss window average  0.5395003  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.61371714  increasing, breaking loop at iter  1265\n",
      "=====================RUN 106===================\n",
      "Test loss window average  0.53739554  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60026497  increasing, breaking loop at iter  1265\n",
      "=====================RUN 107===================\n",
      "Test loss window average  0.5280154  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.5973862  increasing, breaking loop at iter  1010\n",
      "=====================RUN 108===================\n",
      "Test loss window average  0.5155158  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6165394  increasing, breaking loop at iter  1010\n",
      "=====================RUN 109===================\n",
      "Test loss window average  0.54276544  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.614846  increasing, breaking loop at iter  1010\n",
      "=====================RUN 110===================\n",
      "Test loss window average  0.53613526  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.61521584  increasing, breaking loop at iter  1520\n",
      "=====================RUN 111===================\n",
      "Test loss window average  0.5318649  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.5858712  increasing, breaking loop at iter  1265\n",
      "=====================RUN 112===================\n",
      "Test loss window average  0.5537171  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.61327183  increasing, breaking loop at iter  1520\n",
      "=====================RUN 113===================\n",
      "Test loss window average  0.55417055  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.5862244  increasing, breaking loop at iter  1010\n",
      "=====================RUN 114===================\n",
      "Test loss window average  0.55528945  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6129613  increasing, breaking loop at iter  2030\n",
      "=====================RUN 115===================\n",
      "Test loss window average  0.5265339  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6001627  increasing, breaking loop at iter  1775\n",
      "=====================RUN 116===================\n",
      "Test loss window average  0.52817243  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60339165  increasing, breaking loop at iter  1010\n",
      "=====================RUN 117===================\n",
      "Test loss window average  0.54459196  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.5972759  increasing, breaking loop at iter  1265\n",
      "=====================RUN 118===================\n",
      "Test loss window average  0.5198993  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.613672  increasing, breaking loop at iter  1265\n",
      "=====================RUN 119===================\n",
      "Test loss window average  0.53231794  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.5915052  increasing, breaking loop at iter  1520\n",
      "=====================RUN 120===================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.52124965  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5953621  increasing, breaking loop at iter  1265\n",
      "=====================RUN 121===================\n",
      "Test loss window average  0.53599834  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.620479  increasing, breaking loop at iter  1010\n",
      "=====================RUN 122===================\n",
      "Test loss window average  0.5313515  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6074405  increasing, breaking loop at iter  1775\n",
      "=====================RUN 123===================\n",
      "Test loss window average  0.5311671  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60108066  increasing, breaking loop at iter  1520\n",
      "=====================RUN 124===================\n",
      "Test loss window average  0.55161417  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.59140974  increasing, breaking loop at iter  1265\n",
      "=====================RUN 125===================\n",
      "Test loss window average  0.54557663  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.59944403  increasing, breaking loop at iter  1265\n",
      "=====================RUN 126===================\n",
      "Test loss window average  0.5280773  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6012394  increasing, breaking loop at iter  2285\n",
      "=====================RUN 127===================\n",
      "Test loss window average  0.52361894  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6253091  increasing, breaking loop at iter  1520\n",
      "=====================RUN 128===================\n",
      "Test loss window average  0.55128354  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.602125  increasing, breaking loop at iter  2030\n",
      "=====================RUN 129===================\n",
      "Test loss window average  0.5485581  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.5976826  increasing, breaking loop at iter  1520\n",
      "=====================RUN 130===================\n",
      "Test loss window average  0.518789  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.62723845  increasing, breaking loop at iter  1520\n",
      "=====================RUN 131===================\n",
      "Test loss window average  0.5287171  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6052732  increasing, breaking loop at iter  1265\n",
      "=====================RUN 132===================\n",
      "Test loss window average  0.52551615  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.60492545  increasing, breaking loop at iter  1010\n",
      "=====================RUN 133===================\n",
      "Test loss window average  0.5469536  increasing, breaking loop at iter  2285\n",
      "Test loss window average  0.610312  increasing, breaking loop at iter  1520\n",
      "=====================RUN 134===================\n",
      "Test loss window average  0.5404848  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.61488414  increasing, breaking loop at iter  1520\n",
      "=====================RUN 135===================\n",
      "Test loss window average  0.54785085  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6142715  increasing, breaking loop at iter  2540\n",
      "=====================RUN 136===================\n",
      "Test loss window average  0.5417466  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.62712675  increasing, breaking loop at iter  1775\n",
      "=====================RUN 137===================\n",
      "Test loss window average  0.5158214  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6157247  increasing, breaking loop at iter  1265\n",
      "=====================RUN 138===================\n",
      "Test loss window average  0.54099256  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60442126  increasing, breaking loop at iter  755\n",
      "=====================RUN 139===================\n",
      "Test loss window average  0.53349966  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.61782384  increasing, breaking loop at iter  2285\n",
      "=====================RUN 140===================\n",
      "Test loss window average  0.5433958  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.60232717  increasing, breaking loop at iter  1775\n",
      "=====================RUN 141===================\n",
      "Test loss window average  0.50908643  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6030965  increasing, breaking loop at iter  1520\n",
      "=====================RUN 142===================\n",
      "Test loss window average  0.5438875  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6087091  increasing, breaking loop at iter  1010\n",
      "=====================RUN 143===================\n",
      "Test loss window average  0.53363687  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6071891  increasing, breaking loop at iter  2285\n",
      "=====================RUN 144===================\n",
      "Test loss window average  0.52823836  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.59230286  increasing, breaking loop at iter  1520\n",
      "=====================RUN 145===================\n",
      "Test loss window average  0.5346825  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5866557  increasing, breaking loop at iter  755\n",
      "=====================RUN 146===================\n",
      "Test loss window average  0.53171176  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6040955  increasing, breaking loop at iter  1010\n",
      "=====================RUN 147===================\n",
      "Test loss window average  0.5675221  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.5977369  increasing, breaking loop at iter  1265\n",
      "=====================RUN 148===================\n",
      "Test loss window average  0.54739714  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.5932644  increasing, breaking loop at iter  1520\n",
      "=====================RUN 149===================\n",
      "Test loss window average  0.5400807  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.60364485  increasing, breaking loop at iter  1775\n",
      "=====================RUN 150===================\n",
      "Test loss window average  0.54042923  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.58998317  increasing, breaking loop at iter  1520\n",
      "=====================RUN 151===================\n",
      "Test loss window average  0.550906  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6102132  increasing, breaking loop at iter  1010\n",
      "=====================RUN 152===================\n",
      "Test loss window average  0.52768517  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6106374  increasing, breaking loop at iter  1775\n",
      "=====================RUN 153===================\n",
      "Test loss window average  0.53008914  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.6121443  increasing, breaking loop at iter  755\n",
      "=====================RUN 154===================\n",
      "Test loss window average  0.5178062  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.5970716  increasing, breaking loop at iter  755\n",
      "=====================RUN 155===================\n",
      "Test loss window average  0.53511846  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.5985735  increasing, breaking loop at iter  1775\n",
      "=====================RUN 156===================\n",
      "Test loss window average  0.5183786  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.62106794  increasing, breaking loop at iter  1265\n",
      "=====================RUN 157===================\n",
      "Test loss window average  0.5259783  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.60674036  increasing, breaking loop at iter  1520\n",
      "=====================RUN 158===================\n",
      "Test loss window average  0.54354787  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60305274  increasing, breaking loop at iter  1265\n",
      "=====================RUN 159===================\n",
      "Test loss window average  0.53164625  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60229063  increasing, breaking loop at iter  1775\n",
      "=====================RUN 160===================\n",
      "Test loss window average  0.52890015  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.5997002  increasing, breaking loop at iter  1520\n",
      "=====================RUN 161===================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.52664596  increasing, breaking loop at iter  2795\n",
      "Test loss window average  0.62090594  increasing, breaking loop at iter  755\n",
      "=====================RUN 162===================\n",
      "Test loss window average  0.54356414  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6076623  increasing, breaking loop at iter  1265\n",
      "=====================RUN 163===================\n",
      "Test loss window average  0.5351823  increasing, breaking loop at iter  2285\n",
      "Test loss window average  0.60745496  increasing, breaking loop at iter  1265\n",
      "=====================RUN 164===================\n",
      "Test loss window average  0.53644025  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6040543  increasing, breaking loop at iter  1265\n",
      "=====================RUN 165===================\n",
      "Test loss window average  0.53604716  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6113143  increasing, breaking loop at iter  1265\n",
      "=====================RUN 166===================\n",
      "Test loss window average  0.54280055  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6033517  increasing, breaking loop at iter  1265\n",
      "=====================RUN 167===================\n",
      "Test loss window average  0.53564847  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.61765134  increasing, breaking loop at iter  1265\n",
      "=====================RUN 168===================\n",
      "Test loss window average  0.53900737  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.61571074  increasing, breaking loop at iter  1010\n",
      "=====================RUN 169===================\n",
      "Test loss window average  0.513366  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60935205  increasing, breaking loop at iter  1265\n",
      "=====================RUN 170===================\n",
      "Test loss window average  0.5142591  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6116748  increasing, breaking loop at iter  755\n",
      "=====================RUN 171===================\n",
      "Test loss window average  0.54835737  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60948676  increasing, breaking loop at iter  1520\n",
      "=====================RUN 172===================\n",
      "Test loss window average  0.54173106  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.58943117  increasing, breaking loop at iter  1010\n",
      "=====================RUN 173===================\n",
      "Test loss window average  0.53546584  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.5974556  increasing, breaking loop at iter  755\n",
      "=====================RUN 174===================\n",
      "Test loss window average  0.5468362  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60823375  increasing, breaking loop at iter  1265\n",
      "=====================RUN 175===================\n",
      "Test loss window average  0.5550036  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5915189  increasing, breaking loop at iter  1265\n",
      "=====================RUN 176===================\n",
      "Test loss window average  0.5299135  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.61390424  increasing, breaking loop at iter  1520\n",
      "=====================RUN 177===================\n",
      "Test loss window average  0.5455253  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.59607035  increasing, breaking loop at iter  1520\n",
      "=====================RUN 178===================\n",
      "Test loss window average  0.53277755  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6198629  increasing, breaking loop at iter  1265\n",
      "=====================RUN 179===================\n",
      "Test loss window average  0.53393  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6201985  increasing, breaking loop at iter  755\n",
      "=====================RUN 180===================\n",
      "Test loss window average  0.5528065  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60140646  increasing, breaking loop at iter  1265\n",
      "=====================RUN 181===================\n",
      "Test loss window average  0.53870714  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.61060405  increasing, breaking loop at iter  1010\n",
      "=====================RUN 182===================\n",
      "Test loss window average  0.57384497  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.618911  increasing, breaking loop at iter  2285\n",
      "=====================RUN 183===================\n",
      "Test loss window average  0.5369903  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60813576  increasing, breaking loop at iter  2030\n",
      "=====================RUN 184===================\n",
      "Test loss window average  0.5314719  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6168592  increasing, breaking loop at iter  1265\n",
      "=====================RUN 185===================\n",
      "Test loss window average  0.5394069  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60414106  increasing, breaking loop at iter  2030\n",
      "=====================RUN 186===================\n",
      "Test loss window average  0.5326424  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.58601093  increasing, breaking loop at iter  1010\n",
      "=====================RUN 187===================\n",
      "Test loss window average  0.51060396  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6009108  increasing, breaking loop at iter  1265\n",
      "=====================RUN 188===================\n",
      "Test loss window average  0.54499525  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.627293  increasing, breaking loop at iter  1265\n",
      "=====================RUN 189===================\n",
      "Test loss window average  0.5218613  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6177262  increasing, breaking loop at iter  1010\n",
      "=====================RUN 190===================\n",
      "Test loss window average  0.5596845  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.60422796  increasing, breaking loop at iter  1775\n",
      "=====================RUN 191===================\n",
      "Test loss window average  0.5395999  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60411626  increasing, breaking loop at iter  1010\n",
      "=====================RUN 192===================\n",
      "Test loss window average  0.53194255  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.61014616  increasing, breaking loop at iter  1265\n",
      "=====================RUN 193===================\n",
      "Test loss window average  0.54812396  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6210051  increasing, breaking loop at iter  1775\n",
      "=====================RUN 194===================\n",
      "Test loss window average  0.5469025  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.59002686  increasing, breaking loop at iter  1010\n",
      "=====================RUN 195===================\n",
      "Test loss window average  0.51904374  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.61707956  increasing, breaking loop at iter  2030\n",
      "=====================RUN 196===================\n",
      "Test loss window average  0.55107254  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.62452936  increasing, breaking loop at iter  1265\n",
      "=====================RUN 197===================\n",
      "Test loss window average  0.5306553  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6086066  increasing, breaking loop at iter  2030\n",
      "=====================RUN 198===================\n",
      "Test loss window average  0.5087442  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6197903  increasing, breaking loop at iter  1520\n",
      "=====================RUN 199===================\n",
      "Test loss window average  0.5530388  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.58785176  increasing, breaking loop at iter  1520\n",
      "=====================RUN 200===================\n",
      "Test loss window average  0.5409149  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.6075938  increasing, breaking loop at iter  1010\n",
      "=====================RUN 201===================\n",
      "Test loss window average  0.55519044  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6009188  increasing, breaking loop at iter  2540\n",
      "=====================RUN 202===================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.5155594  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6125317  increasing, breaking loop at iter  1265\n",
      "=====================RUN 203===================\n",
      "Test loss window average  0.5274644  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60362744  increasing, breaking loop at iter  2030\n",
      "=====================RUN 204===================\n",
      "Test loss window average  0.5284663  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6092725  increasing, breaking loop at iter  1520\n",
      "=====================RUN 205===================\n",
      "Test loss window average  0.5303476  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6190416  increasing, breaking loop at iter  1265\n",
      "=====================RUN 206===================\n",
      "Test loss window average  0.5412522  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6162496  increasing, breaking loop at iter  1010\n",
      "=====================RUN 207===================\n",
      "Test loss window average  0.552805  increasing, breaking loop at iter  2285\n",
      "Test loss window average  0.6050142  increasing, breaking loop at iter  1010\n",
      "=====================RUN 208===================\n",
      "Test loss window average  0.5183846  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60815805  increasing, breaking loop at iter  1265\n",
      "=====================RUN 209===================\n",
      "Test loss window average  0.50872153  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.5956898  increasing, breaking loop at iter  1010\n",
      "=====================RUN 210===================\n",
      "Test loss window average  0.57017416  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.5922811  increasing, breaking loop at iter  1775\n",
      "=====================RUN 211===================\n",
      "Test loss window average  0.53793985  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.59830916  increasing, breaking loop at iter  1775\n",
      "=====================RUN 212===================\n",
      "Test loss window average  0.53579265  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6099916  increasing, breaking loop at iter  755\n",
      "=====================RUN 213===================\n",
      "Test loss window average  0.5489427  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6279934  increasing, breaking loop at iter  1265\n",
      "=====================RUN 214===================\n",
      "Test loss window average  0.5310264  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60597014  increasing, breaking loop at iter  1265\n",
      "=====================RUN 215===================\n",
      "Test loss window average  0.54722553  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60305196  increasing, breaking loop at iter  1520\n",
      "=====================RUN 216===================\n",
      "Test loss window average  0.5530034  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6335298  increasing, breaking loop at iter  1010\n",
      "=====================RUN 217===================\n",
      "Test loss window average  0.54398847  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.63027364  increasing, breaking loop at iter  1010\n",
      "=====================RUN 218===================\n",
      "Test loss window average  0.5388282  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.59596276  increasing, breaking loop at iter  1520\n",
      "=====================RUN 219===================\n",
      "Test loss window average  0.5224696  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60711455  increasing, breaking loop at iter  1520\n",
      "=====================RUN 220===================\n",
      "Test loss window average  0.5521339  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6087554  increasing, breaking loop at iter  1265\n",
      "=====================RUN 221===================\n",
      "Test loss window average  0.54192847  increasing, breaking loop at iter  2285\n",
      "Test loss window average  0.6091125  increasing, breaking loop at iter  1775\n",
      "=====================RUN 222===================\n",
      "Test loss window average  0.5685725  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6166499  increasing, breaking loop at iter  1265\n",
      "=====================RUN 223===================\n",
      "Test loss window average  0.53785056  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.62396836  increasing, breaking loop at iter  1520\n",
      "=====================RUN 224===================\n",
      "Test loss window average  0.56055325  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.61823237  increasing, breaking loop at iter  1010\n",
      "=====================RUN 225===================\n",
      "Test loss window average  0.51750433  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.5925684  increasing, breaking loop at iter  1520\n",
      "=====================RUN 226===================\n",
      "Test loss window average  0.5338665  increasing, breaking loop at iter  2285\n",
      "Test loss window average  0.609565  increasing, breaking loop at iter  1010\n",
      "=====================RUN 227===================\n",
      "Test loss window average  0.5615621  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.62112904  increasing, breaking loop at iter  1775\n",
      "=====================RUN 228===================\n",
      "Test loss window average  0.55022776  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5974928  increasing, breaking loop at iter  1520\n",
      "=====================RUN 229===================\n",
      "Test loss window average  0.5243075  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.5944102  increasing, breaking loop at iter  1010\n",
      "=====================RUN 230===================\n",
      "Test loss window average  0.55335623  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.59627527  increasing, breaking loop at iter  2285\n",
      "=====================RUN 231===================\n",
      "Test loss window average  0.5333783  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.5882144  increasing, breaking loop at iter  1010\n",
      "=====================RUN 232===================\n",
      "Test loss window average  0.5211971  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.61383975  increasing, breaking loop at iter  1265\n",
      "=====================RUN 233===================\n",
      "Test loss window average  0.5392458  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6023639  increasing, breaking loop at iter  1010\n",
      "=====================RUN 234===================\n",
      "Test loss window average  0.5555572  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.62059236  increasing, breaking loop at iter  2030\n",
      "=====================RUN 235===================\n",
      "Test loss window average  0.56154835  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.6027098  increasing, breaking loop at iter  1520\n",
      "=====================RUN 236===================\n",
      "Test loss window average  0.49447852  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60463756  increasing, breaking loop at iter  1265\n",
      "=====================RUN 237===================\n",
      "Test loss window average  0.5224601  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.59912705  increasing, breaking loop at iter  1010\n",
      "=====================RUN 238===================\n",
      "Test loss window average  0.54353094  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.61459845  increasing, breaking loop at iter  1520\n",
      "=====================RUN 239===================\n",
      "Test loss window average  0.53412527  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60357153  increasing, breaking loop at iter  1520\n",
      "=====================RUN 240===================\n",
      "Test loss window average  0.5272671  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.60624546  increasing, breaking loop at iter  1265\n",
      "=====================RUN 241===================\n",
      "Test loss window average  0.52128494  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.61058193  increasing, breaking loop at iter  1010\n",
      "=====================RUN 242===================\n",
      "Test loss window average  0.52601635  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.61746573  increasing, breaking loop at iter  1265\n",
      "=====================RUN 243===================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.5505372  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6239275  increasing, breaking loop at iter  1010\n",
      "=====================RUN 244===================\n",
      "Test loss window average  0.53540015  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.5989091  increasing, breaking loop at iter  2030\n",
      "=====================RUN 245===================\n",
      "Test loss window average  0.5376913  increasing, breaking loop at iter  2285\n",
      "Test loss window average  0.5943837  increasing, breaking loop at iter  1265\n",
      "=====================RUN 246===================\n",
      "Test loss window average  0.53226465  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.62203306  increasing, breaking loop at iter  1520\n",
      "=====================RUN 247===================\n",
      "Test loss window average  0.54049265  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60099953  increasing, breaking loop at iter  1775\n",
      "=====================RUN 248===================\n",
      "Test loss window average  0.512252  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.617434  increasing, breaking loop at iter  1010\n",
      "=====================RUN 249===================\n",
      "Test loss window average  0.5310288  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.61801374  increasing, breaking loop at iter  1775\n",
      "=====================RUN 250===================\n",
      "Test loss window average  0.52236366  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6017848  increasing, breaking loop at iter  1010\n",
      "=====================RUN 251===================\n",
      "Test loss window average  0.52971524  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.59323144  increasing, breaking loop at iter  755\n",
      "=====================RUN 252===================\n",
      "Test loss window average  0.54973334  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.60536176  increasing, breaking loop at iter  1010\n",
      "=====================RUN 253===================\n",
      "Test loss window average  0.5403126  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.59726954  increasing, breaking loop at iter  1265\n",
      "=====================RUN 254===================\n",
      "Test loss window average  0.52514416  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.624245  increasing, breaking loop at iter  1520\n",
      "=====================RUN 255===================\n",
      "Test loss window average  0.54476464  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6116768  increasing, breaking loop at iter  755\n",
      "=====================RUN 256===================\n",
      "Test loss window average  0.52194846  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5997707  increasing, breaking loop at iter  1265\n",
      "=====================RUN 257===================\n",
      "Test loss window average  0.5090911  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.5992787  increasing, breaking loop at iter  1520\n",
      "=====================RUN 258===================\n",
      "Test loss window average  0.55183077  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5925849  increasing, breaking loop at iter  1010\n",
      "=====================RUN 259===================\n",
      "Test loss window average  0.5512452  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.5827772  increasing, breaking loop at iter  1010\n",
      "=====================RUN 260===================\n",
      "Test loss window average  0.53002954  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6154265  increasing, breaking loop at iter  2030\n",
      "=====================RUN 261===================\n",
      "Test loss window average  0.51522994  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6105325  increasing, breaking loop at iter  1265\n",
      "=====================RUN 262===================\n",
      "Test loss window average  0.52223253  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6129095  increasing, breaking loop at iter  1520\n",
      "=====================RUN 263===================\n",
      "Test loss window average  0.55010086  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6015408  increasing, breaking loop at iter  755\n",
      "=====================RUN 264===================\n",
      "Test loss window average  0.5301479  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.598799  increasing, breaking loop at iter  2285\n",
      "=====================RUN 265===================\n",
      "Test loss window average  0.54439837  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6183915  increasing, breaking loop at iter  1265\n",
      "=====================RUN 266===================\n",
      "Test loss window average  0.5435228  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.61475873  increasing, breaking loop at iter  1265\n",
      "=====================RUN 267===================\n",
      "Test loss window average  0.55978405  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60696006  increasing, breaking loop at iter  755\n",
      "=====================RUN 268===================\n",
      "Test loss window average  0.5332859  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.5898633  increasing, breaking loop at iter  1265\n",
      "=====================RUN 269===================\n",
      "Test loss window average  0.53617644  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60883594  increasing, breaking loop at iter  1010\n",
      "=====================RUN 270===================\n",
      "Test loss window average  0.5195347  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.605063  increasing, breaking loop at iter  1520\n",
      "=====================RUN 271===================\n",
      "Test loss window average  0.54710996  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60781777  increasing, breaking loop at iter  1520\n",
      "=====================RUN 272===================\n",
      "Test loss window average  0.5475655  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6141151  increasing, breaking loop at iter  1265\n",
      "=====================RUN 273===================\n",
      "Test loss window average  0.5326137  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.5975659  increasing, breaking loop at iter  1775\n",
      "=====================RUN 274===================\n",
      "Test loss window average  0.5520932  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.61661595  increasing, breaking loop at iter  2795\n",
      "=====================RUN 275===================\n",
      "Test loss window average  0.5389122  increasing, breaking loop at iter  3050\n",
      "Test loss window average  0.605396  increasing, breaking loop at iter  1265\n",
      "=====================RUN 276===================\n",
      "Test loss window average  0.54013675  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.5802153  increasing, breaking loop at iter  1010\n",
      "=====================RUN 277===================\n",
      "Test loss window average  0.5415104  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6103455  increasing, breaking loop at iter  1010\n",
      "=====================RUN 278===================\n",
      "Test loss window average  0.53463686  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.5907162  increasing, breaking loop at iter  1775\n",
      "=====================RUN 279===================\n",
      "Test loss window average  0.5256687  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6050989  increasing, breaking loop at iter  1010\n",
      "=====================RUN 280===================\n",
      "Test loss window average  0.51531756  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6179735  increasing, breaking loop at iter  1265\n",
      "=====================RUN 281===================\n",
      "Test loss window average  0.5213345  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.59288466  increasing, breaking loop at iter  1010\n",
      "=====================RUN 282===================\n",
      "Test loss window average  0.54394  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.6027653  increasing, breaking loop at iter  1010\n",
      "=====================RUN 283===================\n",
      "Test loss window average  0.5300825  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.59930646  increasing, breaking loop at iter  1265\n",
      "=====================RUN 284===================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.5549652  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60687786  increasing, breaking loop at iter  1265\n",
      "=====================RUN 285===================\n",
      "Test loss window average  0.5379919  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5947496  increasing, breaking loop at iter  1265\n",
      "=====================RUN 286===================\n",
      "Test loss window average  0.5338921  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.61356986  increasing, breaking loop at iter  1520\n",
      "=====================RUN 287===================\n",
      "Test loss window average  0.5347531  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.62482524  increasing, breaking loop at iter  1775\n",
      "=====================RUN 288===================\n",
      "Test loss window average  0.5599141  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.59289074  increasing, breaking loop at iter  755\n",
      "=====================RUN 289===================\n",
      "Test loss window average  0.5576683  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5968341  increasing, breaking loop at iter  1265\n",
      "=====================RUN 290===================\n",
      "Test loss window average  0.55608636  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60708314  increasing, breaking loop at iter  1010\n",
      "=====================RUN 291===================\n",
      "Test loss window average  0.5251337  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.5859846  increasing, breaking loop at iter  1520\n",
      "=====================RUN 292===================\n",
      "Test loss window average  0.5403812  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6003987  increasing, breaking loop at iter  2285\n",
      "=====================RUN 293===================\n",
      "Test loss window average  0.54099756  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60918325  increasing, breaking loop at iter  1010\n",
      "=====================RUN 294===================\n",
      "Test loss window average  0.5351636  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6130528  increasing, breaking loop at iter  1520\n",
      "=====================RUN 295===================\n",
      "Test loss window average  0.53319407  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.59224695  increasing, breaking loop at iter  1520\n",
      "=====================RUN 296===================\n",
      "Test loss window average  0.54047656  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.59656495  increasing, breaking loop at iter  2030\n",
      "=====================RUN 297===================\n",
      "Test loss window average  0.53621125  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.5964933  increasing, breaking loop at iter  2540\n",
      "=====================RUN 298===================\n",
      "Test loss window average  0.523002  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6192355  increasing, breaking loop at iter  1265\n",
      "=====================RUN 299===================\n",
      "Test loss window average  0.5406485  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.61366993  increasing, breaking loop at iter  1010\n",
      "=====================RUN 300===================\n",
      "Test loss window average  0.5494076  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6024  increasing, breaking loop at iter  1265\n",
      "=====================RUN 301===================\n",
      "Test loss window average  0.52785736  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6236473  increasing, breaking loop at iter  1775\n",
      "=====================RUN 302===================\n",
      "Test loss window average  0.5427885  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.59959227  increasing, breaking loop at iter  2030\n",
      "=====================RUN 303===================\n",
      "Test loss window average  0.5226189  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.62645036  increasing, breaking loop at iter  755\n",
      "=====================RUN 304===================\n",
      "Test loss window average  0.53722733  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6165345  increasing, breaking loop at iter  755\n",
      "=====================RUN 305===================\n",
      "Test loss window average  0.5464515  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6094279  increasing, breaking loop at iter  2030\n",
      "=====================RUN 306===================\n",
      "Test loss window average  0.5486209  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60148215  increasing, breaking loop at iter  1010\n",
      "=====================RUN 307===================\n",
      "Test loss window average  0.5267757  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6042852  increasing, breaking loop at iter  2285\n",
      "=====================RUN 308===================\n",
      "Test loss window average  0.5269005  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.59286076  increasing, breaking loop at iter  1265\n",
      "=====================RUN 309===================\n",
      "Test loss window average  0.54537225  increasing, breaking loop at iter  3305\n",
      "Test loss window average  0.5855726  increasing, breaking loop at iter  2285\n",
      "=====================RUN 310===================\n",
      "Test loss window average  0.49690816  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6006197  increasing, breaking loop at iter  2030\n",
      "=====================RUN 311===================\n",
      "Test loss window average  0.53357106  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6131033  increasing, breaking loop at iter  2030\n",
      "=====================RUN 312===================\n",
      "Test loss window average  0.55078197  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6128408  increasing, breaking loop at iter  1775\n",
      "=====================RUN 313===================\n",
      "Test loss window average  0.53837675  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.5994848  increasing, breaking loop at iter  1010\n",
      "=====================RUN 314===================\n",
      "Test loss window average  0.55597115  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.61867124  increasing, breaking loop at iter  2030\n",
      "=====================RUN 315===================\n",
      "Test loss window average  0.5346826  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.61575025  increasing, breaking loop at iter  1520\n",
      "=====================RUN 316===================\n",
      "Test loss window average  0.52350146  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.5970119  increasing, breaking loop at iter  1265\n",
      "=====================RUN 317===================\n",
      "Test loss window average  0.5310619  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.59711754  increasing, breaking loop at iter  1775\n",
      "=====================RUN 318===================\n",
      "Test loss window average  0.5373017  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6171379  increasing, breaking loop at iter  1520\n",
      "=====================RUN 319===================\n",
      "Test loss window average  0.5358063  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.5985296  increasing, breaking loop at iter  1520\n",
      "=====================RUN 320===================\n",
      "Test loss window average  0.5454993  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6015335  increasing, breaking loop at iter  1265\n",
      "=====================RUN 321===================\n",
      "Test loss window average  0.5373822  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.5931515  increasing, breaking loop at iter  1010\n",
      "=====================RUN 322===================\n",
      "Test loss window average  0.5283495  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.62144  increasing, breaking loop at iter  1010\n",
      "=====================RUN 323===================\n",
      "Test loss window average  0.55037344  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.60738426  increasing, breaking loop at iter  1775\n",
      "=====================RUN 324===================\n",
      "Test loss window average  0.53499687  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6015605  increasing, breaking loop at iter  1775\n",
      "=====================RUN 325===================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.5381989  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.61420697  increasing, breaking loop at iter  2285\n",
      "=====================RUN 326===================\n",
      "Test loss window average  0.54581606  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6041744  increasing, breaking loop at iter  1010\n",
      "=====================RUN 327===================\n",
      "Test loss window average  0.5390136  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.5917472  increasing, breaking loop at iter  1775\n",
      "=====================RUN 328===================\n",
      "Test loss window average  0.55619836  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.5710434  increasing, breaking loop at iter  2285\n",
      "=====================RUN 329===================\n",
      "Test loss window average  0.5362128  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6160214  increasing, breaking loop at iter  1265\n",
      "=====================RUN 330===================\n",
      "Test loss window average  0.5481754  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.5963629  increasing, breaking loop at iter  1775\n",
      "=====================RUN 331===================\n",
      "Test loss window average  0.53217137  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6000299  increasing, breaking loop at iter  2030\n",
      "=====================RUN 332===================\n",
      "Test loss window average  0.5390143  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.5885385  increasing, breaking loop at iter  1520\n",
      "=====================RUN 333===================\n",
      "Test loss window average  0.55323726  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6089833  increasing, breaking loop at iter  2285\n",
      "=====================RUN 334===================\n",
      "Test loss window average  0.53359896  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.61354244  increasing, breaking loop at iter  1010\n",
      "=====================RUN 335===================\n",
      "Test loss window average  0.50498915  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.6107634  increasing, breaking loop at iter  1010\n",
      "=====================RUN 336===================\n",
      "Test loss window average  0.5265091  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.5982298  increasing, breaking loop at iter  2030\n",
      "=====================RUN 337===================\n",
      "Test loss window average  0.5259436  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60065466  increasing, breaking loop at iter  755\n",
      "=====================RUN 338===================\n",
      "Test loss window average  0.53283477  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.61667114  increasing, breaking loop at iter  1265\n",
      "=====================RUN 339===================\n",
      "Test loss window average  0.5324979  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.62059504  increasing, breaking loop at iter  1010\n",
      "=====================RUN 340===================\n",
      "Test loss window average  0.5201875  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60086304  increasing, breaking loop at iter  1520\n",
      "=====================RUN 341===================\n",
      "Test loss window average  0.5102297  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.5974985  increasing, breaking loop at iter  1520\n",
      "=====================RUN 342===================\n",
      "Test loss window average  0.53017807  increasing, breaking loop at iter  2285\n",
      "Test loss window average  0.5880625  increasing, breaking loop at iter  2030\n",
      "=====================RUN 343===================\n",
      "Test loss window average  0.5577431  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.59893167  increasing, breaking loop at iter  1520\n",
      "=====================RUN 344===================\n",
      "Test loss window average  0.509277  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.61072797  increasing, breaking loop at iter  1775\n",
      "=====================RUN 345===================\n",
      "Test loss window average  0.5411131  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6201894  increasing, breaking loop at iter  1265\n",
      "=====================RUN 346===================\n",
      "Test loss window average  0.55101144  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.61209357  increasing, breaking loop at iter  2030\n",
      "=====================RUN 347===================\n",
      "Test loss window average  0.53684026  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6125188  increasing, breaking loop at iter  1775\n",
      "=====================RUN 348===================\n",
      "Test loss window average  0.56051785  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5978164  increasing, breaking loop at iter  1775\n",
      "=====================RUN 349===================\n",
      "Test loss window average  0.53932273  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.602477  increasing, breaking loop at iter  1010\n",
      "=====================RUN 350===================\n",
      "Test loss window average  0.5569684  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6141428  increasing, breaking loop at iter  1010\n",
      "=====================RUN 351===================\n",
      "Test loss window average  0.52218527  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6010371  increasing, breaking loop at iter  2030\n",
      "=====================RUN 352===================\n",
      "Test loss window average  0.5480146  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.605368  increasing, breaking loop at iter  1010\n",
      "=====================RUN 353===================\n",
      "Test loss window average  0.5416548  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.59905803  increasing, breaking loop at iter  1010\n",
      "=====================RUN 354===================\n",
      "Test loss window average  0.54284745  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.60488665  increasing, breaking loop at iter  1265\n",
      "=====================RUN 355===================\n",
      "Test loss window average  0.51831746  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6132726  increasing, breaking loop at iter  1010\n",
      "=====================RUN 356===================\n",
      "Test loss window average  0.55176294  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6265477  increasing, breaking loop at iter  2285\n",
      "=====================RUN 357===================\n",
      "Test loss window average  0.54295516  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.62088984  increasing, breaking loop at iter  1265\n",
      "=====================RUN 358===================\n",
      "Test loss window average  0.52491057  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.60268795  increasing, breaking loop at iter  2285\n",
      "=====================RUN 359===================\n",
      "Test loss window average  0.54981893  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.59472704  increasing, breaking loop at iter  1010\n",
      "=====================RUN 360===================\n",
      "Test loss window average  0.51982725  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.61881566  increasing, breaking loop at iter  1010\n",
      "=====================RUN 361===================\n",
      "Test loss window average  0.53190356  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.61345893  increasing, breaking loop at iter  1010\n",
      "=====================RUN 362===================\n",
      "Test loss window average  0.50690633  increasing, breaking loop at iter  2285\n",
      "Test loss window average  0.60496515  increasing, breaking loop at iter  1010\n",
      "=====================RUN 363===================\n",
      "Test loss window average  0.5250502  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60828525  increasing, breaking loop at iter  2540\n",
      "=====================RUN 364===================\n",
      "Test loss window average  0.5432857  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.61135256  increasing, breaking loop at iter  1010\n",
      "=====================RUN 365===================\n",
      "Test loss window average  0.53837526  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6060993  increasing, breaking loop at iter  1775\n",
      "=====================RUN 366===================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.5523678  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.59687346  increasing, breaking loop at iter  755\n",
      "=====================RUN 367===================\n",
      "Test loss window average  0.55887985  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.5930241  increasing, breaking loop at iter  1520\n",
      "=====================RUN 368===================\n",
      "Test loss window average  0.5358732  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.61494684  increasing, breaking loop at iter  1010\n",
      "=====================RUN 369===================\n",
      "Test loss window average  0.55261725  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6011371  increasing, breaking loop at iter  1265\n",
      "=====================RUN 370===================\n",
      "Test loss window average  0.5447117  increasing, breaking loop at iter  2285\n",
      "Test loss window average  0.61294675  increasing, breaking loop at iter  1010\n",
      "=====================RUN 371===================\n",
      "Test loss window average  0.5465558  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.58275837  increasing, breaking loop at iter  1520\n",
      "=====================RUN 372===================\n",
      "Test loss window average  0.5381897  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6013853  increasing, breaking loop at iter  1520\n",
      "=====================RUN 373===================\n",
      "Test loss window average  0.52391225  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.61102605  increasing, breaking loop at iter  1775\n",
      "=====================RUN 374===================\n",
      "Test loss window average  0.54105747  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.597358  increasing, breaking loop at iter  2285\n",
      "=====================RUN 375===================\n",
      "Test loss window average  0.5324294  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6201775  increasing, breaking loop at iter  1520\n",
      "=====================RUN 376===================\n",
      "Test loss window average  0.5305917  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.60027766  increasing, breaking loop at iter  1010\n",
      "=====================RUN 377===================\n",
      "Test loss window average  0.54436624  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.63161445  increasing, breaking loop at iter  1010\n",
      "=====================RUN 378===================\n",
      "Test loss window average  0.55736876  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5969177  increasing, breaking loop at iter  755\n",
      "=====================RUN 379===================\n",
      "Test loss window average  0.5308601  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.63321596  increasing, breaking loop at iter  1520\n",
      "=====================RUN 380===================\n",
      "Test loss window average  0.5545303  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.61015284  increasing, breaking loop at iter  1265\n",
      "=====================RUN 381===================\n",
      "Test loss window average  0.532246  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6333351  increasing, breaking loop at iter  2285\n",
      "=====================RUN 382===================\n",
      "Test loss window average  0.5315291  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6073039  increasing, breaking loop at iter  1265\n",
      "=====================RUN 383===================\n",
      "Test loss window average  0.52521354  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6227558  increasing, breaking loop at iter  1265\n",
      "=====================RUN 384===================\n",
      "Test loss window average  0.5360947  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.59865373  increasing, breaking loop at iter  1265\n",
      "=====================RUN 385===================\n",
      "Test loss window average  0.51693016  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6087471  increasing, breaking loop at iter  1520\n",
      "=====================RUN 386===================\n",
      "Test loss window average  0.5182177  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.608816  increasing, breaking loop at iter  1520\n",
      "=====================RUN 387===================\n",
      "Test loss window average  0.56522757  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6091497  increasing, breaking loop at iter  1010\n",
      "=====================RUN 388===================\n",
      "Test loss window average  0.537772  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6195497  increasing, breaking loop at iter  1010\n",
      "=====================RUN 389===================\n",
      "Test loss window average  0.55241126  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.57713324  increasing, breaking loop at iter  1265\n",
      "=====================RUN 390===================\n",
      "Test loss window average  0.5359337  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.5987946  increasing, breaking loop at iter  1010\n",
      "=====================RUN 391===================\n",
      "Test loss window average  0.53803927  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60742176  increasing, breaking loop at iter  1520\n",
      "=====================RUN 392===================\n",
      "Test loss window average  0.5188449  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.597234  increasing, breaking loop at iter  1010\n",
      "=====================RUN 393===================\n",
      "Test loss window average  0.53307396  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6002594  increasing, breaking loop at iter  1265\n",
      "=====================RUN 394===================\n",
      "Test loss window average  0.53950757  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6105097  increasing, breaking loop at iter  2030\n",
      "=====================RUN 395===================\n",
      "Test loss window average  0.54733735  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.62404984  increasing, breaking loop at iter  1520\n",
      "=====================RUN 396===================\n",
      "Test loss window average  0.53264815  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60947376  increasing, breaking loop at iter  1520\n",
      "=====================RUN 397===================\n",
      "Test loss window average  0.53170836  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.62234044  increasing, breaking loop at iter  755\n",
      "=====================RUN 398===================\n",
      "Test loss window average  0.5449497  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.5935845  increasing, breaking loop at iter  1265\n",
      "=====================RUN 399===================\n",
      "Test loss window average  0.537065  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.5967791  increasing, breaking loop at iter  1520\n",
      "=====================RUN 400===================\n",
      "Test loss window average  0.54596704  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.61810654  increasing, breaking loop at iter  1520\n",
      "=====================RUN 401===================\n",
      "Test loss window average  0.52288306  increasing, breaking loop at iter  2285\n",
      "Test loss window average  0.59786063  increasing, breaking loop at iter  1775\n",
      "=====================RUN 402===================\n",
      "Test loss window average  0.5316862  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.60113555  increasing, breaking loop at iter  1520\n",
      "=====================RUN 403===================\n",
      "Test loss window average  0.52015716  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6122376  increasing, breaking loop at iter  1265\n",
      "=====================RUN 404===================\n",
      "Test loss window average  0.53129596  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6182632  increasing, breaking loop at iter  1775\n",
      "=====================RUN 405===================\n",
      "Test loss window average  0.5467052  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6001543  increasing, breaking loop at iter  2285\n",
      "=====================RUN 406===================\n",
      "Test loss window average  0.54288775  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6099212  increasing, breaking loop at iter  1010\n",
      "=====================RUN 407===================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.550555  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6076985  increasing, breaking loop at iter  1775\n",
      "=====================RUN 408===================\n",
      "Test loss window average  0.54080707  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6064102  increasing, breaking loop at iter  755\n",
      "=====================RUN 409===================\n",
      "Test loss window average  0.5427624  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6280127  increasing, breaking loop at iter  1010\n",
      "=====================RUN 410===================\n",
      "Test loss window average  0.54122376  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6040313  increasing, breaking loop at iter  1265\n",
      "=====================RUN 411===================\n",
      "Test loss window average  0.5301175  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.59206355  increasing, breaking loop at iter  1265\n",
      "=====================RUN 412===================\n",
      "Test loss window average  0.5333405  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.5892348  increasing, breaking loop at iter  1265\n",
      "=====================RUN 413===================\n",
      "Test loss window average  0.5473009  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6139392  increasing, breaking loop at iter  755\n",
      "=====================RUN 414===================\n",
      "Test loss window average  0.5272394  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6148168  increasing, breaking loop at iter  755\n",
      "=====================RUN 415===================\n",
      "Test loss window average  0.5441388  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.59015596  increasing, breaking loop at iter  1520\n",
      "=====================RUN 416===================\n",
      "Test loss window average  0.5501703  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.5995281  increasing, breaking loop at iter  1520\n",
      "=====================RUN 417===================\n",
      "Test loss window average  0.547969  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5949873  increasing, breaking loop at iter  1520\n",
      "=====================RUN 418===================\n",
      "Test loss window average  0.5328322  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60153645  increasing, breaking loop at iter  1265\n",
      "=====================RUN 419===================\n",
      "Test loss window average  0.53809214  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5946853  increasing, breaking loop at iter  1775\n",
      "=====================RUN 420===================\n",
      "Test loss window average  0.5115259  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6330928  increasing, breaking loop at iter  1265\n",
      "=====================RUN 421===================\n",
      "Test loss window average  0.5425474  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.5995524  increasing, breaking loop at iter  755\n",
      "=====================RUN 422===================\n",
      "Test loss window average  0.5413415  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6173987  increasing, breaking loop at iter  1265\n",
      "=====================RUN 423===================\n",
      "Test loss window average  0.54156786  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60385376  increasing, breaking loop at iter  755\n",
      "=====================RUN 424===================\n",
      "Test loss window average  0.5085118  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60819936  increasing, breaking loop at iter  1265\n",
      "=====================RUN 425===================\n",
      "Test loss window average  0.53760517  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5908032  increasing, breaking loop at iter  2285\n",
      "=====================RUN 426===================\n",
      "Test loss window average  0.5207543  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.597195  increasing, breaking loop at iter  1265\n",
      "=====================RUN 427===================\n",
      "Test loss window average  0.53262526  increasing, breaking loop at iter  2285\n",
      "Test loss window average  0.61924624  increasing, breaking loop at iter  1010\n",
      "=====================RUN 428===================\n",
      "Test loss window average  0.5490048  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60362303  increasing, breaking loop at iter  1520\n",
      "=====================RUN 429===================\n",
      "Test loss window average  0.54399174  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.6100188  increasing, breaking loop at iter  755\n",
      "=====================RUN 430===================\n",
      "Test loss window average  0.54280597  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.61540735  increasing, breaking loop at iter  2795\n",
      "=====================RUN 431===================\n",
      "Test loss window average  0.548242  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60042393  increasing, breaking loop at iter  1265\n",
      "=====================RUN 432===================\n",
      "Test loss window average  0.5445111  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.6032894  increasing, breaking loop at iter  1010\n",
      "=====================RUN 433===================\n",
      "Test loss window average  0.53846544  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.5929472  increasing, breaking loop at iter  1775\n",
      "=====================RUN 434===================\n",
      "Test loss window average  0.54250467  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6125509  increasing, breaking loop at iter  1010\n",
      "=====================RUN 435===================\n",
      "Test loss window average  0.5296952  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60921234  increasing, breaking loop at iter  1520\n",
      "=====================RUN 436===================\n",
      "Test loss window average  0.5456031  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6093893  increasing, breaking loop at iter  1775\n",
      "=====================RUN 437===================\n",
      "Test loss window average  0.52053636  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.5966078  increasing, breaking loop at iter  1265\n",
      "=====================RUN 438===================\n",
      "Test loss window average  0.5393822  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6092512  increasing, breaking loop at iter  1010\n",
      "=====================RUN 439===================\n",
      "Test loss window average  0.5072651  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6197317  increasing, breaking loop at iter  1520\n",
      "=====================RUN 440===================\n",
      "Test loss window average  0.53662837  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.60675323  increasing, breaking loop at iter  2030\n",
      "=====================RUN 441===================\n",
      "Test loss window average  0.5401255  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6043496  increasing, breaking loop at iter  1520\n",
      "=====================RUN 442===================\n",
      "Test loss window average  0.53976893  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6080184  increasing, breaking loop at iter  755\n",
      "=====================RUN 443===================\n",
      "Test loss window average  0.5436222  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.60676515  increasing, breaking loop at iter  1775\n",
      "=====================RUN 444===================\n",
      "Test loss window average  0.5476574  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6012508  increasing, breaking loop at iter  1265\n",
      "=====================RUN 445===================\n",
      "Test loss window average  0.53385276  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6136116  increasing, breaking loop at iter  1010\n",
      "=====================RUN 446===================\n",
      "Test loss window average  0.540622  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.61347574  increasing, breaking loop at iter  3050\n",
      "=====================RUN 447===================\n",
      "Test loss window average  0.5332527  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60141605  increasing, breaking loop at iter  1265\n",
      "=====================RUN 448===================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.5337578  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6231312  increasing, breaking loop at iter  1265\n",
      "=====================RUN 449===================\n",
      "Test loss window average  0.5553689  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.6373255  increasing, breaking loop at iter  1010\n",
      "=====================RUN 450===================\n",
      "Test loss window average  0.52774143  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6051798  increasing, breaking loop at iter  1775\n",
      "=====================RUN 451===================\n",
      "Test loss window average  0.5439083  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.61869735  increasing, breaking loop at iter  1010\n",
      "=====================RUN 452===================\n",
      "Test loss window average  0.53169954  increasing, breaking loop at iter  2285\n",
      "Test loss window average  0.6342585  increasing, breaking loop at iter  755\n",
      "=====================RUN 453===================\n",
      "Test loss window average  0.5319979  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6091789  increasing, breaking loop at iter  1010\n",
      "=====================RUN 454===================\n",
      "Test loss window average  0.55288815  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.60244894  increasing, breaking loop at iter  755\n",
      "=====================RUN 455===================\n",
      "Test loss window average  0.5308867  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.62005633  increasing, breaking loop at iter  1520\n",
      "=====================RUN 456===================\n",
      "Test loss window average  0.52547723  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.5964446  increasing, breaking loop at iter  755\n",
      "=====================RUN 457===================\n",
      "Test loss window average  0.53511757  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6163335  increasing, breaking loop at iter  1265\n",
      "=====================RUN 458===================\n",
      "Test loss window average  0.52452356  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.59376705  increasing, breaking loop at iter  755\n",
      "=====================RUN 459===================\n",
      "Test loss window average  0.5224774  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6101303  increasing, breaking loop at iter  2540\n",
      "=====================RUN 460===================\n",
      "Test loss window average  0.536355  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6072576  increasing, breaking loop at iter  2030\n",
      "=====================RUN 461===================\n",
      "Test loss window average  0.51326364  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.59583056  increasing, breaking loop at iter  1775\n",
      "=====================RUN 462===================\n",
      "Test loss window average  0.5455076  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6096636  increasing, breaking loop at iter  1265\n",
      "=====================RUN 463===================\n",
      "Test loss window average  0.53294885  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6104996  increasing, breaking loop at iter  2030\n",
      "=====================RUN 464===================\n",
      "Test loss window average  0.5552871  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6192846  increasing, breaking loop at iter  755\n",
      "=====================RUN 465===================\n",
      "Test loss window average  0.52675223  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6074014  increasing, breaking loop at iter  1010\n",
      "=====================RUN 466===================\n",
      "Test loss window average  0.53803  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6127899  increasing, breaking loop at iter  1010\n",
      "=====================RUN 467===================\n",
      "Test loss window average  0.53362286  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.5988357  increasing, breaking loop at iter  1010\n",
      "=====================RUN 468===================\n",
      "Test loss window average  0.52057356  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.611705  increasing, breaking loop at iter  2285\n",
      "=====================RUN 469===================\n",
      "Test loss window average  0.5272914  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.62480766  increasing, breaking loop at iter  755\n",
      "=====================RUN 470===================\n",
      "Test loss window average  0.5313374  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.61507285  increasing, breaking loop at iter  1265\n",
      "=====================RUN 471===================\n",
      "Test loss window average  0.55758697  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6077244  increasing, breaking loop at iter  1265\n",
      "=====================RUN 472===================\n",
      "Test loss window average  0.5554147  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6257204  increasing, breaking loop at iter  1010\n",
      "=====================RUN 473===================\n",
      "Test loss window average  0.5409697  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6152235  increasing, breaking loop at iter  1520\n",
      "=====================RUN 474===================\n",
      "Test loss window average  0.53010017  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.5957627  increasing, breaking loop at iter  1520\n",
      "=====================RUN 475===================\n",
      "Test loss window average  0.5412952  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60206914  increasing, breaking loop at iter  755\n",
      "=====================RUN 476===================\n",
      "Test loss window average  0.54137087  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.59993035  increasing, breaking loop at iter  1520\n",
      "=====================RUN 477===================\n",
      "Test loss window average  0.5340312  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.60747546  increasing, breaking loop at iter  1775\n",
      "=====================RUN 478===================\n",
      "Test loss window average  0.5308993  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.6006502  increasing, breaking loop at iter  2030\n",
      "=====================RUN 479===================\n",
      "Test loss window average  0.5490217  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.62278086  increasing, breaking loop at iter  1265\n",
      "=====================RUN 480===================\n",
      "Test loss window average  0.5233276  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.59679985  increasing, breaking loop at iter  1010\n",
      "=====================RUN 481===================\n",
      "Test loss window average  0.54286784  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.60299724  increasing, breaking loop at iter  1520\n",
      "=====================RUN 482===================\n",
      "Test loss window average  0.5294405  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.61464155  increasing, breaking loop at iter  1010\n",
      "=====================RUN 483===================\n",
      "Test loss window average  0.5314666  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.6011187  increasing, breaking loop at iter  1010\n",
      "=====================RUN 484===================\n",
      "Test loss window average  0.52406853  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.62021387  increasing, breaking loop at iter  1520\n",
      "=====================RUN 485===================\n",
      "Test loss window average  0.5342027  increasing, breaking loop at iter  1775\n",
      "Test loss window average  0.6084203  increasing, breaking loop at iter  1265\n",
      "=====================RUN 486===================\n",
      "Test loss window average  0.5463116  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5911396  increasing, breaking loop at iter  1775\n",
      "=====================RUN 487===================\n",
      "Test loss window average  0.5508928  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.60022795  increasing, breaking loop at iter  1010\n",
      "=====================RUN 488===================\n",
      "Test loss window average  0.51926166  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5934593  increasing, breaking loop at iter  1775\n",
      "=====================RUN 489===================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.5212054  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5959798  increasing, breaking loop at iter  1520\n",
      "=====================RUN 490===================\n",
      "Test loss window average  0.5223196  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5964142  increasing, breaking loop at iter  1265\n",
      "=====================RUN 491===================\n",
      "Test loss window average  0.5248216  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.6007955  increasing, breaking loop at iter  1010\n",
      "=====================RUN 492===================\n",
      "Test loss window average  0.5307166  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.58792645  increasing, breaking loop at iter  1010\n",
      "=====================RUN 493===================\n",
      "Test loss window average  0.538756  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.59510803  increasing, breaking loop at iter  1775\n",
      "=====================RUN 494===================\n",
      "Test loss window average  0.5421593  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.61527336  increasing, breaking loop at iter  2795\n",
      "=====================RUN 495===================\n",
      "Test loss window average  0.5458106  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.61543214  increasing, breaking loop at iter  1520\n",
      "=====================RUN 496===================\n",
      "Test loss window average  0.5136414  increasing, breaking loop at iter  2030\n",
      "Test loss window average  0.6012558  increasing, breaking loop at iter  755\n",
      "=====================RUN 497===================\n",
      "Test loss window average  0.52846605  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.61053675  increasing, breaking loop at iter  1520\n",
      "=====================RUN 498===================\n",
      "Test loss window average  0.562963  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.60402685  increasing, breaking loop at iter  1520\n",
      "=====================RUN 499===================\n",
      "Test loss window average  0.5034707  increasing, breaking loop at iter  1520\n",
      "Test loss window average  0.604123  increasing, breaking loop at iter  1520\n"
     ]
    }
   ],
   "source": [
    "print('Best Q params:', qbest_params)\n",
    "print('Best G params:', gbest_params)\n",
    "N = 10000\n",
    "seed = 0\n",
    "num_runs = 500\n",
    "\n",
    "output_type_Q = 'categorical'\n",
    "output_size_Q = 1\n",
    "output_type_G = 'categorical'\n",
    "output_size_G = 1\n",
    "input_size_Q = z.shape[-1] + 1  # we will concatenate the treatment var inside the qnet class\n",
    "input_size_G = z.shape[-1]\n",
    "qlayers = qbest_params['layers']\n",
    "qdropout = qbest_params['dropout']\n",
    "qlayer_size = qbest_params['layer_size']\n",
    "qiters = 100000  # use the early stopping iter\n",
    "qlr = qbest_params['lr']\n",
    "qbatch_size = qbest_params['batch_size']\n",
    "\n",
    "glayers = gbest_params['layers']\n",
    "gdropout = gbest_params['dropout']\n",
    "glayer_size = gbest_params['layer_size']\n",
    "giters = 100000  # use the early stopping iter\n",
    "glr = gbest_params['lr']\n",
    "gbatch_size = gbest_params['batch_size']\n",
    "\n",
    "estimates_naive = []\n",
    "estimates_upd_one_steps = []\n",
    "estimates_upd_submodels = []\n",
    "for i in range(num_runs):\n",
    "    print('=====================RUN {}==================='.format(i))\n",
    "    seed += 1\n",
    "    # data generation:\n",
    "    z, x, y, _, _ = generate_data(N, seed=seed)\n",
    "    x = torch.tensor(x).type(torch.float32)\n",
    "    z = torch.tensor(z).type(torch.float32)\n",
    "    y = torch.tensor(y).type(torch.float32)\n",
    "    x_int1 = torch.ones_like(x)  # this is the 'intervention data'\n",
    "    x_int0 = torch.zeros_like(x)    \n",
    "    \n",
    "\n",
    "    qnet = QNet(input_size=input_size_Q, num_layers=qlayers,\n",
    "                          layers_size=qlayer_size, output_size=output_size_Q,\n",
    "                         output_type=output_type_Q, dropout=qdropout)\n",
    "\n",
    "    gnet = GNet(input_size=input_size_G, num_layers=glayers,\n",
    "                          layers_size=glayer_size, output_size=output_size_G,\n",
    "                         output_type=output_type_G, dropout=gdropout)\n",
    "\n",
    "\n",
    "    qtrainer = Trainer(net=qnet, net_type='Q', iterations=qiters, outcome_type=output_type_Q,\n",
    "                  batch_size=qbatch_size, test_iter=5, lr=qlr)\n",
    "    \n",
    "    gtrainer = Trainer(net=gnet, net_type='G', iterations=giters, outcome_type=output_type_G,\n",
    "                  batch_size=gbatch_size, test_iter=5, lr=glr)\n",
    "\n",
    "    train_loss_q_,  val_loss_q_, stop_it_q, best_model_q, best_model_test_loss_q = qtrainer.train(x, y, z)\n",
    "    train_loss_g_, val_loss_g_, stop_it_g, best_model_g, best_model_test_loss_g = gtrainer.train(x, y, z)\n",
    "    \n",
    "    _, y_pred = qtrainer.test(best_model_q, x, y, z)\n",
    "    _, x_pred = gtrainer.test(best_model_g, x, y, z)\n",
    "    \n",
    "    x_pred, y_pred = x_pred.detach().numpy(), y_pred.detach().numpy()\n",
    "    \n",
    "    _,  Q10 = qtrainer.test(best_model_q, x, y, z)\n",
    "    _,  Q1 = qtrainer.test(best_model_q, x_int1, y, z)\n",
    "    _, Q0 = qtrainer.test(best_model_q, x_int0, y, z)\n",
    "\n",
    "    _, G10 = gtrainer.test(best_model_g, x, y, z)\n",
    "\n",
    "\n",
    "    Q1 = Q1.detach().numpy()\n",
    "    Q0 = Q0.detach().numpy()\n",
    "    Q10 = Q10.detach().numpy()\n",
    "    G10 = np.clip(G10.detach().numpy(), a_min=0.001, a_max=0.999)\n",
    "    \n",
    "    biased_psi = (Q1 - Q0).mean()\n",
    "    estimates_naive.append(biased_psi)\n",
    "\n",
    "    \n",
    "    # one step approach\n",
    "    H1 = 1/(G10)\n",
    "    H0 = 1 / (1 - G10)\n",
    "\n",
    "    x_ = x.detach().numpy()\n",
    "    y_ = y.detach().numpy()\n",
    "    D1 = x_ * H1 * (y_ - Q1) + Q1 - Q1.mean()\n",
    "    D0 = (1 - x_) * H0 * (y_ - Q0) + Q0 - Q0.mean()\n",
    "\n",
    "    Q1_star = Q1 + D1\n",
    "    Q0_star = Q0 + D0\n",
    "\n",
    "    upd_psi_one_step = (Q1_star - Q0_star).mean()\n",
    "\n",
    "   \n",
    "    estimates_upd_one_steps.append(upd_psi_one_step)\n",
    " \n",
    "# submodel approach https://github.com/migariane/SIM-TMLE-tutorial\n",
    "    H1 = 1/(G10)\n",
    "    H0 = -1 / (1 - G10)\n",
    "    H10 = x_ * H1 + (1-x_) * H0\n",
    "\n",
    "    eps = sm.GLM(y_, H10, offset=inv_sigm(Q10[:,0]),\n",
    "                        family=sm.families.Binomial()).fit().params\n",
    "\n",
    "    Q0_star = sigm(inv_sigm(Q0) + eps * H0)\n",
    "    Q1_star = sigm(inv_sigm(Q1) + eps * H1)\n",
    "    \n",
    "\n",
    "    upd_psi_submodel = (Q1_star - Q0_star).mean()\n",
    "    \n",
    "    estimates_upd_submodels.append(upd_psi_submodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e37c9e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ONE STEP==============\n",
      "True psi:  0.1956508\n",
      "naive psi:  0.20319404  relative bias: 3.855459359129164 %\n",
      "updated TMLE psi:  0.1989004  relative bias: 1.6609191438290185 %\n",
      "Reduction in bias: 2.1945402153001456 %\n",
      "naive psi var: 0.0008032604\n",
      "updated psi var: 0.00010994954\n",
      "Average of reductions: 7.307586 %\n"
     ]
    }
   ],
   "source": [
    "print('============ONE STEP==============')\n",
    "      \n",
    "estimates_upd_one_steps = np.asarray(estimates_upd_one_steps)\n",
    "estimates_naive = np.asarray(estimates_naive)\n",
    "\n",
    "print('True psi: ', true_psi)\n",
    "print('naive psi: ', estimates_naive.mean(), ' relative bias:',\n",
    "      (estimates_naive.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('updated TMLE psi: ', estimates_upd_one_steps.mean(), ' relative bias:',\n",
    "      (estimates_upd_one_steps.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('Reduction in bias:', np.abs(estimates_naive.mean() - true_psi)/true_psi * 100 - \n",
    "     np.abs(estimates_upd_one_steps.mean() - true_psi)/true_psi * 100, '%')\n",
    "\n",
    "# This takes the reduction in relative bias for each simulation first, then takes an average\n",
    "# (Owing to the nonlinearity of the ||x|| function, this gives different results which are\n",
    "# worth considering.)\n",
    "print('naive psi var:', estimates_naive.var())\n",
    "print('updated psi var:', estimates_upd_one_steps.var())\n",
    "errors_naive = (estimates_naive - true_psi)/true_psi *100\n",
    "errors_updated = (estimates_upd_one_steps - true_psi)/true_psi *100\n",
    "diff_errors = np.abs(errors_naive) - np.abs(errors_updated)\n",
    "print('Average of reductions:', diff_errors.mean(), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4a92660",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============SUBMODEL==============\n",
      "True psi:  0.1956508\n",
      "naive psi:  0.20319404  relative bias: 3.855459359129164 %\n",
      "updated TMLE psi:  0.19888286473520775  relative bias: 1.6519557983957818 %\n",
      "Reduction in bias: 2.203503560733382 %\n",
      "naive psi var: 0.0008032604\n",
      "updated psi var: 0.0001091319515824547\n",
      "Average of reductions: 7.354823593887008 %\n"
     ]
    }
   ],
   "source": [
    "print('============SUBMODEL==============')\n",
    "      \n",
    "estimates_upd_submodels = np.asarray(estimates_upd_submodels)\n",
    "estimates_naive = np.asarray(estimates_naive)\n",
    "\n",
    "print('True psi: ', true_psi)\n",
    "print('naive psi: ', estimates_naive.mean(), ' relative bias:',\n",
    "      (estimates_naive.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('updated TMLE psi: ', estimates_upd_submodels.mean(), ' relative bias:',\n",
    "      (estimates_upd_submodels.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('Reduction in bias:', np.abs(estimates_naive.mean() - true_psi)/true_psi * 100 - \n",
    "     np.abs(estimates_upd_submodels.mean() - true_psi)/true_psi * 100, '%')\n",
    "\n",
    "# This takes the reduction in relative bias for each simulation first, then takes an average\n",
    "# (Owing to the nonlinearity of the ||x|| function, this gives different results which are\n",
    "# worth considering.)\n",
    "print('naive psi var:', estimates_naive.var())\n",
    "print('updated psi var:', estimates_upd_submodels.var())\n",
    "errors_naive = (estimates_naive - true_psi)/true_psi *100\n",
    "errors_updated = (estimates_upd_submodels - true_psi)/true_psi *100\n",
    "diff_errors = np.abs(errors_naive) - np.abs(errors_updated)\n",
    "print('Average of reductions:', diff_errors.mean(), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3d5d74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.6162456, dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(np.asarray(val_loss_g_))\n",
    "best_model_test_loss_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fef13a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.5274386, dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(np.asarray(val_loss_q_))\n",
    "best_model_test_loss_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0eca62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b57e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03db0ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f47f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
