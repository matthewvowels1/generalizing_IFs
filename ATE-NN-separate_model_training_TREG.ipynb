{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ec6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8ec6e",
   "metadata": {},
   "source": [
    "## Summary of Results:\n",
    "\n",
    "$\\hat Q$ is the outcome estimator, $\\hat G$ is the propensity score estimator. Their respective columns tell us which estimators are use e.g. NN means a neural network was used.\n",
    "\n",
    "'Reduction' is the relative percent error reduction when compared against the plug-in estimator using the outcome model alone. The results are averages over 60 simulations.\n",
    "\n",
    "\n",
    "| Method | $\\hat Q$ | $\\hat G$ | Reduction $\\%$ | Rel. Error $\\%$ |\n",
    "| --- | --- | --- | --- |--- |\n",
    "| Naive | $NN$ | - |- |  4.059|\n",
    "| TMLE | $NN$ | $NN$ | 1.450 | 2.608 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1217b",
   "metadata": {},
   "source": [
    "## Problem Setup:\n",
    "\n",
    "This example is taken from https://arxiv.org/abs/2107.00681 by Hines, Dukes, Diaz-Ordaz, and Vansteelandt (2021) and the empirical evaluation follows https://onlinelibrary.wiley.com/doi/full/10.1002/sim.7628 by Miguel Angel Luque-Fernandez, Michael Schomaker, Bernard Rachet, Mireille E. Schnitzer (2018).\n",
    "\n",
    "\n",
    "The following experiments are very similar to the ones in ATE.ipynb, but this time we will fit the estimators using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049bda1",
   "metadata": {},
   "source": [
    "## 1. Define the DGP and some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f134282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigm(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def inv_sigm(x):\n",
    "    return np.log(x/(1-x))\n",
    "\n",
    "def generate_data(N, seed):\n",
    "    np.random.seed(seed=seed)\n",
    "    z1 = np.random.binomial(1, 0.5, (N,1))\n",
    "    z2 = np.random.binomial(1, 0.65, (N,1))\n",
    "    z3 = np.round(np.random.uniform(0, 4, (N,1)),3)\n",
    "    z4 = np.round(np.random.uniform(0, 5, (N,1)),3)\n",
    "    X = np.random.binomial(1, sigm(-0.4 + 0.2*z2 + 0.15*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y1 = np.random.binomial(1, sigm(-1 + 1 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y0 = np.random.binomial(1, sigm(-1 + 0 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y = Y1 * X + Y0 * (1-X)\n",
    "    Z = np.concatenate([z1,z2,z3,z4],1)\n",
    "    return Z, X, Y, Y1, Y0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed127b9",
   "metadata": {},
   "source": [
    "## 2. Define the Neural Network Objects/Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be134faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)     \n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layers_size, output_size, output_type, dropout):\n",
    "        super(QNet, self).__init__()      \n",
    "        \n",
    "        self.epsilon = nn.Parameter(torch.tensor([0.0]), requires_grad=True)\n",
    "        \n",
    "        layers = []\n",
    "        layers.extend([nn.Linear(input_size, layers_size), nn.ReLU()])\n",
    "        for i in range(num_layers-1):\n",
    "            layers.extend([nn.Linear(layers_size, layers_size), nn.ReLU(), nn.Dropout(p=dropout)])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        pos_arm = []\n",
    "        pos_arm.extend([nn.Linear(layers_size, layers_size), nn.ReLU()])\n",
    "        pos_arm.extend([nn.Linear(layers_size, output_size)])     \n",
    "        \n",
    "        neg_arm = []\n",
    "        neg_arm.extend([nn.Linear(layers_size, layers_size), nn.ReLU()])\n",
    "        neg_arm.extend([nn.Linear(layers_size, output_size)])    \n",
    "        \n",
    "        if output_type == 'categorical':\n",
    "            pos_arm.append(nn.Sigmoid())\n",
    "            neg_arm.append(nn.Sigmoid())\n",
    "        elif output_type == 'continuous':\n",
    "            pass\n",
    "        self.pos_arm = nn.Sequential(*pos_arm)\n",
    "        self.neg_arm = nn.Sequential(*neg_arm)\n",
    "    \n",
    "        self.net.apply(init_weights) \n",
    "        self.neg_arm.apply(init_weights) \n",
    "        self.pos_arm.apply(init_weights) \n",
    "\n",
    "\n",
    "    def forward(self, X, Z):\n",
    "        \n",
    "        out = self.net(torch.cat([X,Z],1))\n",
    "        out0 = self.neg_arm(out)\n",
    "        out1 = self.pos_arm(out)\n",
    "        cond = X.bool()\n",
    "        return torch.where(cond, out1, out0)\n",
    "\n",
    "    \n",
    "    \n",
    "class GNet(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layers_size, output_size, output_type, dropout):\n",
    "        super(GNet, self).__init__()      \n",
    "        self.output_type = output_type\n",
    "        layers = []\n",
    "        layers.extend([nn.Linear(input_size, layers_size), nn.ReLU()])\n",
    "        for i in range(num_layers-1):\n",
    "            layers.extend([nn.Linear(layers_size, layers_size), nn.ReLU(), nn.Dropout(p=dropout)])\n",
    "        layers.extend([nn.Linear(layers_size, output_size)])\n",
    "\n",
    "        if output_type == 'categorical':\n",
    "            layers.append(nn.Sigmoid())\n",
    "        elif output_type == 'continuous':\n",
    "            pass\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(init_weights) \n",
    "        \n",
    "    def forward(self, Z):\n",
    "        if self.output_type == 'categorical':\n",
    "            out = (0.01 + self.net(Z))/1.02\n",
    "#             out = self.net(Z)\n",
    "        elif self.output_type == 'continuous':\n",
    "            out = self.net(Z)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7283e3",
   "metadata": {},
   "source": [
    "## 3. Create a Neural Network training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "696557f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_(p):\n",
    "    return torch.log(p / (1 - p))\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, net, net_type='Q', beta=1.0, outcome_type='categorical', iterations=None, batch_size=None, test_iter=None, lr=None):\n",
    "        self.net_type = net_type\n",
    "        self.net = net\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.test_iter = test_iter\n",
    "        self.outcome_type = outcome_type\n",
    "        \n",
    "        if lr is not None:\n",
    "            self.optimizer = optim.SGD(self.net.parameters(), lr=lr, momentum=0.9)\n",
    "        \n",
    "        self.bce_loss = nn.BCELoss(reduction='none')\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "        \n",
    "    def train(self, x, y, z, x_pred=None):\n",
    "        \n",
    "        # create a small validation set\n",
    "        indices = np.arange(len(x))\n",
    "        np.random.shuffle(indices)\n",
    "        val_inds = indices[:len(x)//8]\n",
    "        train_inds = indices[len(x)//8:]\n",
    "        x_val, y_val, z_val = x[val_inds], y[val_inds], z[val_inds]\n",
    "        x_train, y_train, z_train = x[train_inds], y[train_inds], z[train_inds]\n",
    "        \n",
    "        if self.net_type == 'Q':\n",
    "            x_pred_train, x_pred_val = x_pred[train_inds], x_pred[val_inds]\n",
    "        \n",
    "        indices = np.arange(len(x_train))\n",
    "        \n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        epsilons = [] \n",
    "        best_model = None\n",
    "        best_model_test_loss = 1e10\n",
    "        best_early_stop_test_loss = 1e10\n",
    "        test_loss_window = []\n",
    "        window_length = 50  # number of measures of loss over which to determine early stopping\n",
    "        stopping_iteration = self.iterations  # initialise early stopping iter as the total iters\n",
    "        \n",
    "        for it in range(self.iterations):\n",
    "            inds = np.random.choice(indices, self.batch_size)\n",
    "            x_batch, y_batch, z_batch = x_train[inds], y_train[inds], z_train[inds]\n",
    "            \n",
    "            if self.net_type == 'Q':\n",
    "                x_pred_batch = x_pred_train[inds]\n",
    "                pred = self.net(x_batch, z_batch)\n",
    "                treg_loss = self.beta * self.treg(x_batch, x_pred_batch, y_batch, pred)\n",
    "                \n",
    "                if self.outcome_type == 'categorical':\n",
    "                    loss = self.bce_loss(pred, y_batch).mean()\n",
    "                else:\n",
    "                    loss = self.mse_loss(pred, y_batch)\n",
    "                    \n",
    "                loss += treg_loss\n",
    "                \n",
    "                \n",
    "            elif self.net_type == 'G':\n",
    "                pred = self.net(z_batch)\n",
    "                if self.outcome_type == 'categorical':\n",
    "                    loss = self.bce_loss(pred, x_batch).mean()\n",
    "                else:\n",
    "                    loss = self.mse_loss(pred, x_batch)\n",
    "\n",
    "            loss.backward(retain_graph=True if self.net_type == 'Q' else False)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            if (it % self.test_iter == 0) or (it == (self.iterations-1)):\n",
    "                self.net.eval()\n",
    "\n",
    "                if self.net_type == 'Q':\n",
    "                    pred = self.net(x_train[:800], z_train[:800])\n",
    "                    epsilons.append(self.net.epsilon.detach().numpy())\n",
    "\n",
    "                    if self.outcome_type == 'categorical':\n",
    "                        loss = self.bce_loss(pred, y_train[:800]).mean()\n",
    "                    else:\n",
    "                        loss = self.mse_loss(pred, y_train[:800])\n",
    "\n",
    "                elif self.net_type == 'G':\n",
    "                    pred = self.net(z_train[:800])\n",
    "                    if self.outcome_type == 'categorical':\n",
    "                        loss = self.bce_loss(pred, x_train[:800]).mean()\n",
    "                    else:\n",
    "                        loss = self.mse_loss(pred, x_train[:800])\n",
    "\n",
    "\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "                loss_test, _ = self.test(self.net, x_val, y_val, z_val)\n",
    "                loss_test = loss_test.detach().numpy()\n",
    "                test_losses.append(loss_test.item())\n",
    "\n",
    "                self.net.train()\n",
    "   \n",
    "                # Early Stopping Code part 1\n",
    "                if len(test_loss_window) > window_length:  # reset window\n",
    "                    test_loss_window = [] \n",
    "                test_loss_window.append(loss_test)\n",
    "                # Early Stopping Code part 2\n",
    "                if len(test_loss_window) == window_length:  # if we have a complete window\n",
    "                    av_loss_window = np.mean(test_loss_window)  # take average\n",
    "                    if av_loss_window < best_early_stop_test_loss:\n",
    "                        best_early_stop_test_loss = av_loss_window\n",
    "                    else:\n",
    "                        print('Test loss window average ',av_loss_window, ' increasing, breaking loop at iter ', it)\n",
    "                        stopping_iteration = it\n",
    "                        break\n",
    "        \n",
    "                if (loss_test < best_model_test_loss):\n",
    "                    best_model_test_loss = loss_test\n",
    "                    best_model = self.net\n",
    "                \n",
    "        return train_losses, test_losses, stopping_iteration, best_model, best_model_test_loss, epsilons\n",
    "    \n",
    "    \n",
    "    def test(self, model, x, y, z):\n",
    "        model.eval()\n",
    "        \n",
    "        if self.net_type == 'Q':\n",
    "            pred = model(x, z)\n",
    "\n",
    "            if self.outcome_type == 'categorical':\n",
    "                loss = self.bce_loss(pred, y).mean()\n",
    "            else:\n",
    "                loss = self.mse_loss(pred, y)\n",
    "\n",
    "        elif self.net_type == 'G':\n",
    "            pred = model(z)\n",
    "            if self.outcome_type == 'categorical':\n",
    "                loss = self.bce_loss(pred, x).mean()\n",
    "            else:\n",
    "                loss = self.mse_loss(pred, x)\n",
    "        \n",
    "        return loss, pred\n",
    "    \n",
    "    def treg(self, x, pred_x, y, pred_y): \n",
    "        pred_x = torch.clip(pred_x, 0.05, 0.99)\n",
    "        h = x / pred_x - (1 - x) / (1 - pred_x)\n",
    "        \n",
    "        if self.outcome_type == 'categorical':\n",
    "            y_pert = torch.sigmoid(logit_(p=pred_y) + self.net.epsilon * h)\n",
    "            t_reg = torch.sum(\n",
    "                    - y * torch.log(y_pert) - (1 - y) * torch.log(1 - y_pert))\n",
    "        else:\n",
    "            print('continuous treg on TODO list')\n",
    "        return t_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e57e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4e2e5e6",
   "metadata": {},
   "source": [
    "## 4. Create a hyperparameter tuning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0658fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tuner(object):\n",
    "    def __init__(self, x, y, z, trials, x_pred=None, net_type='Q', best_params=None):\n",
    "        self.net_type = net_type\n",
    "        self.best_params = best_params\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "        self.x_pred = x_pred\n",
    "        self.trials = trials\n",
    "        self.test_iter = 5\n",
    "        self.best_params = best_params\n",
    "        self.net = None\n",
    "        self.best_model = None\n",
    "        \n",
    "    def tune(self):\n",
    "\n",
    "        output_type = 'categorical'\n",
    "        output_size = 1\n",
    "        \n",
    "        if self.net_type == 'Q':\n",
    "            input_size = z.shape[-1] + 1  # we will concatenate the treatment var inside the qnet class\n",
    "        elif self.net_type == 'G':\n",
    "            input_size = z.shape[-1] \n",
    "            \n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        bs_ = []\n",
    "        iters_ = []\n",
    "        lr_ = []\n",
    "        stop_it_ = []   # list for early stopping iteration\n",
    "        layers_ = []\n",
    "        dropout_ = []\n",
    "        beta_ = []\n",
    "        layer_size_ = []\n",
    "        best_loss = 1e10\n",
    "        best_losses = []\n",
    "        for trial in range(self.trials):\n",
    "            # sample hyper params and store the history\n",
    "            bs = np.random.randint(10,64) if self.best_params == None else self.best_params['batch_size']\n",
    "            bs_.append(bs)\n",
    "            iters = np.random.randint(15000,100000) if self.best_params == None else self.best_params['iters']\n",
    "            iters_.append(iters)\n",
    "            lr = np.random.uniform(0.0001, 0.01) if self.best_params == None else self.best_params['lr']\n",
    "            lr_.append(lr)\n",
    "            beta = np.random.uniform(0.01, 2.0) if self.best_params == None else self.best_params['beta']\n",
    "            beta_.append(beta)\n",
    "            layers = np.random.randint(2, 6) if self.best_params == None else self.best_params['layers']\n",
    "            layers_.append(layers)\n",
    "            dropout = np.random.uniform(0.1,0.5) if self.best_params == None else self.best_params['dropout']\n",
    "            dropout_.append(dropout)\n",
    "            layer_size = np.random.randint(4, 32) if self.best_params == None else self.best_params['layer_size']\n",
    "            layer_size_.append(layer_size)\n",
    "            print('======== Trial {} of {} ========='.format(trial, self.trials-1))\n",
    "            print('Batch size', bs, ' Iters', iters, ' Lr', lr, ' Layers', layers,\n",
    "                 ' Dropout', dropout, ' Layer Size', layer_size, 'beta', beta)\n",
    "\n",
    "            if self.net_type == 'Q':\n",
    "                self.net = QNet(input_size=input_size, num_layers=layers,\n",
    "                          layers_size=layer_size, output_size=output_size,\n",
    "                         output_type=output_type, dropout=dropout)\n",
    "            elif self.net_type == 'G': \n",
    "                self.net = GNet(input_size=input_size, num_layers=layers,\n",
    "                          layers_size=layer_size, output_size=output_size,\n",
    "                         output_type=output_type, dropout=dropout)\n",
    "\n",
    "            trainer = Trainer(net=self.net, net_type=self.net_type, beta=beta, outcome_type=output_type,\n",
    "                              iterations=iters, batch_size=bs, test_iter=self.test_iter, lr=lr)\n",
    "            train_loss_, val_loss_, stop_it, best_model, best_model_test_loss_, epsilons_ = trainer.train(self.x,\n",
    "                                                                                                          self.y,\n",
    "                                                                                                          self.z,\n",
    "                                                                                                         x_pred=self.x_pred)\n",
    "            \n",
    "            \n",
    "            print('Best number of iterations: ', stop_it, 'compared with total:', iters)\n",
    "            stop_it_.append(stop_it)\n",
    "            train_loss.append(train_loss_[-1])\n",
    "            val_loss.append(val_loss_[-1])\n",
    "            best_losses.append(best_model_test_loss_)\n",
    "            \n",
    "            total_val_loss = val_loss_[-1]\n",
    "            \n",
    "            if best_model_test_loss_ < best_loss:\n",
    "                print('old loss:', best_loss)\n",
    "                print('new loss:', total_val_loss)\n",
    "                print('best model updated')\n",
    "                best_loss = best_model_test_loss_\n",
    "                self.best_model = best_model\n",
    "\n",
    "        tuning_dict = {'batch_size': bs_, 'layers':layers_, 'dropout':dropout_, 'beta':beta_,\n",
    "                      'layer_size':layer_size_,'lr':lr_, 'iters':iters_, 'stop_it': stop_it_,\n",
    "                      'train_loss':train_loss, 'val_loss':val_loss, 'best_model_test_loss':best_losses,\n",
    "                      }\n",
    "        \n",
    "        if self.net_type == 'G':\n",
    "            _, best_model_preds = trainer.test(self.best_model, self.x, self.y, self.z)\n",
    "        else:\n",
    "            best_model_preds = None\n",
    "        return tuning_dict, self.best_model, best_model_preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6fc57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "129eeea7",
   "metadata": {},
   "source": [
    "## 5. Run Hyperparameter Search\n",
    "\n",
    "Now we have everything we need, we can initialize the neural networks, run hyperparameter search to identify the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475fa678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Trial 0 of 99 =========\n",
      "Batch size 44  Iters 53624  Lr 0.000652378214512967  Layers 2  Dropout 0.3967010199864429  Layer Size 7 beta 1.7949885810096247\n",
      "Test loss window average  0.6316156  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 53624\n",
      "old loss: 10000000000.0\n",
      "new loss: 0.6320985555648804\n",
      "best model updated\n",
      "======== Trial 1 of 99 =========\n",
      "Batch size 31  Iters 90732  Lr 0.0075889220199047095  Layers 2  Dropout 0.49773013313887626  Layer Size 15 beta 1.5913676725973758\n",
      "Test loss window average  0.60121894  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 90732\n",
      "old loss: 0.63119274\n",
      "new loss: 0.6074954867362976\n",
      "best model updated\n",
      "======== Trial 2 of 99 =========\n",
      "Batch size 51  Iters 28937  Lr 0.0074672798032895315  Layers 2  Dropout 0.288086633216721  Layer Size 14 beta 1.482143147489634\n",
      "Test loss window average  0.5996925  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 28937\n",
      "old loss: 0.59833515\n",
      "new loss: 0.6080123782157898\n",
      "best model updated\n",
      "======== Trial 3 of 99 =========\n",
      "Batch size 54  Iters 32346  Lr 0.009442143577076416  Layers 3  Dropout 0.4217113302886356  Layer Size 14 beta 1.5537152554768199\n",
      "Test loss window average  0.5980056  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 32346\n",
      "old loss: 0.5970526\n",
      "new loss: 0.6007124781608582\n",
      "best model updated\n",
      "======== Trial 4 of 99 =========\n",
      "Batch size 40  Iters 40791  Lr 0.0063401265285927206  Layers 5  Dropout 0.3663109214521255  Layer Size 7 beta 0.08190969092213608\n",
      "Test loss window average  0.6452752  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 40791\n",
      "======== Trial 5 of 99 =========\n",
      "Batch size 38  Iters 18650  Lr 0.005152281507106171  Layers 4  Dropout 0.30053656066869805  Layer Size 17 beta 1.6891263431422747\n",
      "Test loss window average  0.6218274  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 18650\n",
      "======== Trial 6 of 99 =========\n",
      "Batch size 12  Iters 57372  Lr 0.004898703418555397  Layers 5  Dropout 0.47301253137532584  Layer Size 17 beta 0.37079191706892617\n",
      "Test loss window average  0.6318973  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 57372\n",
      "======== Trial 7 of 99 =========\n",
      "Batch size 37  Iters 43425  Lr 0.003597430893881499  Layers 5  Dropout 0.2292495987428029  Layer Size 26 beta 0.8941919742156823\n",
      "Test loss window average  0.6054206  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 43425\n",
      "======== Trial 8 of 99 =========\n",
      "Batch size 31  Iters 33542  Lr 0.007159072097283411  Layers 4  Dropout 0.407334379339927  Layer Size 17 beta 0.8558824666581546\n",
      "Test loss window average  0.61455435  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 33542\n",
      "======== Trial 9 of 99 =========\n",
      "Batch size 51  Iters 85037  Lr 0.003095311856410391  Layers 3  Dropout 0.2293398296469453  Layer Size 14 beta 0.7999676084059739\n",
      "Test loss window average  0.6062572  increasing, breaking loop at iter  4325\n",
      "Best number of iterations:  4325 compared with total: 85037\n",
      "======== Trial 10 of 99 =========\n",
      "Batch size 52  Iters 75532  Lr 0.008042759423975279  Layers 3  Dropout 0.10707776567898925  Layer Size 17 beta 0.40209632596182754\n",
      "Test loss window average  0.6009999  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 75532\n",
      "======== Trial 11 of 99 =========\n",
      "Batch size 45  Iters 50963  Lr 0.0011336043493228333  Layers 3  Dropout 0.4511272462347361  Layer Size 28 beta 1.909895173137863\n",
      "Test loss window average  0.61292124  increasing, breaking loop at iter  4835\n",
      "Best number of iterations:  4835 compared with total: 50963\n",
      "======== Trial 12 of 99 =========\n",
      "Batch size 62  Iters 38325  Lr 0.0014625592787095912  Layers 2  Dropout 0.17602397751009777  Layer Size 26 beta 0.9606682003732595\n",
      "Test loss window average  0.60319746  increasing, breaking loop at iter  4325\n",
      "Best number of iterations:  4325 compared with total: 38325\n",
      "======== Trial 13 of 99 =========\n",
      "Batch size 54  Iters 90384  Lr 0.005170651907072482  Layers 5  Dropout 0.23833797414975266  Layer Size 16 beta 1.5703629865157263\n",
      "Test loss window average  0.60265166  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 90384\n",
      "======== Trial 14 of 99 =========\n",
      "Batch size 25  Iters 46070  Lr 0.0017019090884213888  Layers 2  Dropout 0.27139208187459274  Layer Size 20 beta 0.9636967577504785\n",
      "Test loss window average  0.6112588  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 46070\n",
      "======== Trial 15 of 99 =========\n",
      "Batch size 12  Iters 71253  Lr 0.001858441245530456  Layers 5  Dropout 0.38738800228917636  Layer Size 9 beta 1.4418283789464572\n",
      "Test loss window average  0.6317811  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 71253\n",
      "======== Trial 16 of 99 =========\n",
      "Batch size 38  Iters 55264  Lr 0.00046181018189642023  Layers 4  Dropout 0.29877768978812413  Layer Size 17 beta 0.7189467918664918\n",
      "Test loss window average  0.60953754  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 55264\n",
      "======== Trial 17 of 99 =========\n",
      "Batch size 58  Iters 22396  Lr 0.00873884212565844  Layers 3  Dropout 0.2496770767255918  Layer Size 28 beta 1.7164404007734226\n",
      "Test loss window average  0.6079646  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 22396\n",
      "======== Trial 18 of 99 =========\n",
      "Batch size 46  Iters 96354  Lr 0.00358008918535688  Layers 3  Dropout 0.13193501097760074  Layer Size 29 beta 0.48289027721097855\n",
      "Test loss window average  0.591873  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 96354\n",
      "old loss: 0.5926195\n",
      "new loss: 0.5925210118293762\n",
      "best model updated\n",
      "======== Trial 19 of 99 =========\n",
      "Batch size 35  Iters 22993  Lr 0.007962094174432216  Layers 2  Dropout 0.1948428316566866  Layer Size 14 beta 1.1757924347368742\n",
      "Test loss window average  0.5913561  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 22993\n",
      "old loss: 0.59021\n",
      "new loss: 0.5910249948501587\n",
      "best model updated\n",
      "======== Trial 20 of 99 =========\n",
      "Batch size 18  Iters 46978  Lr 0.009087057182486973  Layers 5  Dropout 0.11267931000221028  Layer Size 27 beta 1.7479639819622341\n",
      "Test loss window average  0.6048963  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 46978\n",
      "======== Trial 21 of 99 =========\n",
      "Batch size 62  Iters 61235  Lr 0.004675605534518615  Layers 2  Dropout 0.24914107914699513  Layer Size 15 beta 1.1490401438890283\n",
      "Test loss window average  0.59408766  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 61235\n",
      "======== Trial 22 of 99 =========\n",
      "Batch size 18  Iters 42166  Lr 0.009628837028105509  Layers 2  Dropout 0.16739502416370827  Layer Size 13 beta 0.7793888569210796\n",
      "Test loss window average  0.6009303  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 42166\n",
      "======== Trial 23 of 99 =========\n",
      "Batch size 36  Iters 81506  Lr 0.006378866722835635  Layers 2  Dropout 0.2933666690850817  Layer Size 25 beta 0.296463606813745\n",
      "Test loss window average  0.59235907  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 81506\n",
      "old loss: 0.5887325\n",
      "new loss: 0.5922510027885437\n",
      "best model updated\n",
      "======== Trial 24 of 99 =========\n",
      "Batch size 50  Iters 71585  Lr 0.000498083953235169  Layers 4  Dropout 0.2851557268040402  Layer Size 25 beta 0.5488141696716825\n",
      "Test loss window average  0.6199639  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 71585\n",
      "======== Trial 25 of 99 =========\n",
      "Batch size 23  Iters 92237  Lr 0.0060210063364579715  Layers 3  Dropout 0.3931012538587686  Layer Size 5 beta 0.7395183312857526\n",
      "Test loss window average  0.6211445  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 92237\n",
      "======== Trial 26 of 99 =========\n",
      "Batch size 54  Iters 54211  Lr 0.003376738352195345  Layers 2  Dropout 0.11353057718160704  Layer Size 26 beta 1.9937655114120927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.5840888  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 54211\n",
      "old loss: 0.5882702\n",
      "new loss: 0.5852778553962708\n",
      "best model updated\n",
      "======== Trial 27 of 99 =========\n",
      "Batch size 14  Iters 45832  Lr 0.002359435769227197  Layers 4  Dropout 0.3165546718566515  Layer Size 27 beta 1.6027506165172132\n",
      "Test loss window average  0.6166235  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 45832\n",
      "======== Trial 28 of 99 =========\n",
      "Batch size 58  Iters 53332  Lr 0.007683093013319783  Layers 4  Dropout 0.11423679151101931  Layer Size 30 beta 0.5193367695486307\n",
      "Test loss window average  0.6045199  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 53332\n",
      "======== Trial 29 of 99 =========\n",
      "Batch size 50  Iters 22598  Lr 0.008343132935514334  Layers 3  Dropout 0.34214149507770586  Layer Size 31 beta 0.7697316549956844\n",
      "Test loss window average  0.5933211  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 22598\n",
      "======== Trial 30 of 99 =========\n",
      "Batch size 46  Iters 85909  Lr 0.006006046351607826  Layers 3  Dropout 0.26015044139712795  Layer Size 13 beta 1.7134843997536777\n",
      "Test loss window average  0.612446  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 85909\n",
      "======== Trial 31 of 99 =========\n",
      "Batch size 13  Iters 96770  Lr 0.004014029847385024  Layers 2  Dropout 0.2847668941540926  Layer Size 22 beta 1.8803646439893782\n",
      "Test loss window average  0.6190088  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 96770\n",
      "======== Trial 32 of 99 =========\n",
      "Batch size 27  Iters 27206  Lr 0.00021433914965671798  Layers 4  Dropout 0.4243750548295452  Layer Size 11 beta 1.777170727488833\n",
      "Test loss window average  0.6167042  increasing, breaking loop at iter  6620\n",
      "Best number of iterations:  6620 compared with total: 27206\n",
      "======== Trial 33 of 99 =========\n",
      "Batch size 33  Iters 85270  Lr 0.007905918790570244  Layers 5  Dropout 0.49062335354712194  Layer Size 16 beta 1.5463535624231595\n",
      "Test loss window average  0.62416774  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 85270\n",
      "======== Trial 34 of 99 =========\n",
      "Batch size 16  Iters 59898  Lr 0.008507579246516253  Layers 2  Dropout 0.20405161940430916  Layer Size 29 beta 0.7649109383677374\n",
      "Test loss window average  0.61349285  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 59898\n",
      "======== Trial 35 of 99 =========\n",
      "Batch size 17  Iters 96601  Lr 0.009467117034785725  Layers 4  Dropout 0.4140342984187182  Layer Size 14 beta 1.7593490631538018\n",
      "Test loss window average  0.62682986  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 96601\n",
      "======== Trial 36 of 99 =========\n",
      "Batch size 38  Iters 64604  Lr 0.002117094212361366  Layers 3  Dropout 0.11332353541600466  Layer Size 30 beta 1.697989237581314\n",
      "Test loss window average  0.6009805  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 64604\n",
      "======== Trial 37 of 99 =========\n",
      "Batch size 19  Iters 26306  Lr 0.00850434847289179  Layers 3  Dropout 0.47720266628985397  Layer Size 29 beta 0.6536348233523186\n",
      "Test loss window average  0.61199784  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 26306\n",
      "======== Trial 38 of 99 =========\n",
      "Batch size 61  Iters 24731  Lr 0.000370356864878386  Layers 3  Dropout 0.2628160460485261  Layer Size 18 beta 1.302965203164226\n",
      "Test loss window average  0.59987336  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 24731\n",
      "======== Trial 39 of 99 =========\n",
      "Batch size 28  Iters 61934  Lr 0.003959706593584129  Layers 3  Dropout 0.39378017714134894  Layer Size 5 beta 1.4994065502487055\n",
      "Test loss window average  0.6067308  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 61934\n",
      "======== Trial 40 of 99 =========\n",
      "Batch size 51  Iters 83313  Lr 0.0015095150391559729  Layers 5  Dropout 0.46599006440899793  Layer Size 8 beta 0.6554653930362043\n",
      "Test loss window average  0.6412117  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 83313\n",
      "======== Trial 41 of 99 =========\n",
      "Batch size 28  Iters 54680  Lr 0.005053925359509063  Layers 3  Dropout 0.31043773500103644  Layer Size 10 beta 1.419423264420198\n",
      "Test loss window average  0.5963546  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 54680\n",
      "======== Trial 42 of 99 =========\n",
      "Batch size 32  Iters 34143  Lr 0.005205981944599396  Layers 2  Dropout 0.44544758323005196  Layer Size 14 beta 0.9792544531254241\n",
      "Test loss window average  0.6139203  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 34143\n",
      "======== Trial 43 of 99 =========\n",
      "Batch size 53  Iters 53433  Lr 0.0004945903527080587  Layers 2  Dropout 0.18512444029226038  Layer Size 12 beta 1.689116790042646\n",
      "Test loss window average  0.6243565  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 53433\n",
      "======== Trial 44 of 99 =========\n",
      "Batch size 45  Iters 89335  Lr 0.008770181502111393  Layers 3  Dropout 0.4933475046295518  Layer Size 17 beta 1.8279105351166136\n",
      "Test loss window average  0.6087339  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 89335\n",
      "======== Trial 45 of 99 =========\n",
      "Batch size 27  Iters 47490  Lr 0.005168087264088249  Layers 5  Dropout 0.44944112567704064  Layer Size 16 beta 1.9874243836979693\n",
      "Test loss window average  0.63798934  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 47490\n",
      "======== Trial 46 of 99 =========\n",
      "Batch size 63  Iters 97936  Lr 0.0065750488516436514  Layers 3  Dropout 0.20802484620431244  Layer Size 21 beta 0.4148586217848854\n",
      "Test loss window average  0.6081308  increasing, breaking loop at iter  3305\n",
      "Best number of iterations:  3305 compared with total: 97936\n",
      "======== Trial 47 of 99 =========\n",
      "Batch size 14  Iters 29157  Lr 0.003898637041449265  Layers 2  Dropout 0.35105234772607796  Layer Size 25 beta 0.18145473711351365\n",
      "Test loss window average  0.59404874  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 29157\n",
      "======== Trial 48 of 99 =========\n",
      "Batch size 23  Iters 39404  Lr 0.003652008408875897  Layers 5  Dropout 0.20166280924694835  Layer Size 19 beta 0.7307852536539974\n",
      "Test loss window average  0.6232065  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 39404\n",
      "======== Trial 49 of 99 =========\n",
      "Batch size 51  Iters 38001  Lr 0.007286921879492971  Layers 2  Dropout 0.1027182360769471  Layer Size 27 beta 1.803628184047456\n",
      "Test loss window average  0.5974124  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 38001\n",
      "======== Trial 50 of 99 =========\n",
      "Batch size 17  Iters 53619  Lr 0.0039663673200793755  Layers 3  Dropout 0.3288584846799334  Layer Size 28 beta 1.8613328325253367\n",
      "Test loss window average  0.60449654  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 53619\n",
      "======== Trial 51 of 99 =========\n",
      "Batch size 55  Iters 98379  Lr 0.004105392297716543  Layers 4  Dropout 0.26064000119290565  Layer Size 8 beta 0.16775187234708894\n",
      "Test loss window average  0.62891424  increasing, breaking loop at iter  2795\n",
      "Best number of iterations:  2795 compared with total: 98379\n",
      "======== Trial 52 of 99 =========\n",
      "Batch size 40  Iters 44811  Lr 0.003613637016552574  Layers 2  Dropout 0.43373664879005014  Layer Size 15 beta 1.6898026579373544\n",
      "Test loss window average  0.5939467  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 44811\n",
      "======== Trial 53 of 99 =========\n",
      "Batch size 55  Iters 72715  Lr 0.003988378538268369  Layers 4  Dropout 0.32211558287031333  Layer Size 5 beta 0.3782491900878761\n",
      "Test loss window average  0.6105659  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 72715\n",
      "======== Trial 54 of 99 =========\n",
      "Batch size 37  Iters 68689  Lr 0.005843009314286175  Layers 5  Dropout 0.14787166045639744  Layer Size 18 beta 0.4383474088760575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.6026154  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 68689\n",
      "======== Trial 55 of 99 =========\n",
      "Batch size 59  Iters 91598  Lr 0.004640325728170311  Layers 2  Dropout 0.23316871561157965  Layer Size 22 beta 1.7081923359304778\n",
      "Test loss window average  0.60020846  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 91598\n",
      "======== Trial 56 of 99 =========\n",
      "Batch size 37  Iters 85372  Lr 0.007606580103597133  Layers 3  Dropout 0.450861818793308  Layer Size 21 beta 1.4895566433422478\n",
      "Test loss window average  0.6107341  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 85372\n",
      "======== Trial 57 of 99 =========\n",
      "Batch size 46  Iters 70356  Lr 0.0008828948429599377  Layers 2  Dropout 0.11901157724180078  Layer Size 26 beta 1.50698013770574\n",
      "Test loss window average  0.61450106  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 70356\n",
      "======== Trial 58 of 99 =========\n",
      "Batch size 26  Iters 24407  Lr 0.0049761773826389735  Layers 5  Dropout 0.3048306403328147  Layer Size 22 beta 1.0964242969448588\n",
      "Test loss window average  0.6331879  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 24407\n",
      "======== Trial 59 of 99 =========\n",
      "Batch size 14  Iters 15128  Lr 0.008966173163145217  Layers 3  Dropout 0.19290144834575856  Layer Size 10 beta 1.116388310163437\n",
      "Test loss window average  0.6036057  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 15128\n",
      "======== Trial 60 of 99 =========\n",
      "Batch size 54  Iters 74550  Lr 0.008335280074008215  Layers 3  Dropout 0.33824869647744604  Layer Size 15 beta 0.5612804193692863\n",
      "Test loss window average  0.62084436  increasing, breaking loop at iter  3050\n",
      "Best number of iterations:  3050 compared with total: 74550\n",
      "======== Trial 61 of 99 =========\n",
      "Batch size 40  Iters 81288  Lr 0.008351643507174968  Layers 5  Dropout 0.4380404920675486  Layer Size 15 beta 1.6469759103719865\n",
      "Test loss window average  0.62328565  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 81288\n",
      "======== Trial 62 of 99 =========\n",
      "Batch size 22  Iters 18099  Lr 0.0019359481606935165  Layers 4  Dropout 0.3345653194590996  Layer Size 15 beta 0.3393852412402413\n",
      "Test loss window average  0.6283781  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 18099\n",
      "======== Trial 63 of 99 =========\n",
      "Batch size 20  Iters 45660  Lr 0.005909907852153312  Layers 3  Dropout 0.3213577933000531  Layer Size 30 beta 0.07679405751711249\n",
      "Test loss window average  0.6141657  increasing, breaking loop at iter  2795\n",
      "Best number of iterations:  2795 compared with total: 45660\n",
      "======== Trial 64 of 99 =========\n",
      "Batch size 31  Iters 46904  Lr 0.0019067095452846823  Layers 4  Dropout 0.24977261495198194  Layer Size 9 beta 1.9415581194102762\n",
      "Test loss window average  0.617025  increasing, breaking loop at iter  3305\n",
      "Best number of iterations:  3305 compared with total: 46904\n",
      "======== Trial 65 of 99 =========\n",
      "Batch size 16  Iters 28739  Lr 0.0017849452352977455  Layers 3  Dropout 0.24742785345531498  Layer Size 15 beta 1.0959077248425857\n",
      "Test loss window average  0.60755473  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 28739\n",
      "======== Trial 66 of 99 =========\n",
      "Batch size 26  Iters 40310  Lr 0.0031491946613409387  Layers 2  Dropout 0.39624952739871233  Layer Size 14 beta 0.34898271587305557\n",
      "Test loss window average  0.60181916  increasing, breaking loop at iter  3560\n",
      "Best number of iterations:  3560 compared with total: 40310\n",
      "======== Trial 67 of 99 =========\n",
      "Batch size 62  Iters 69633  Lr 0.006306310304880193  Layers 4  Dropout 0.4710709899698382  Layer Size 27 beta 0.8890401423772499\n",
      "Test loss window average  0.6046088  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 69633\n",
      "======== Trial 68 of 99 =========\n",
      "Batch size 53  Iters 89184  Lr 0.009739573156348677  Layers 3  Dropout 0.26286763714231753  Layer Size 10 beta 0.29835680136950526\n",
      "Test loss window average  0.60014457  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 89184\n",
      "======== Trial 69 of 99 =========\n",
      "Batch size 20  Iters 45093  Lr 0.00715256286647452  Layers 3  Dropout 0.18099887863000466  Layer Size 4 beta 0.2757344523724802\n",
      "Test loss window average  0.61487615  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 45093\n",
      "======== Trial 70 of 99 =========\n",
      "Batch size 32  Iters 94403  Lr 0.0030477691168118233  Layers 3  Dropout 0.446084367212371  Layer Size 19 beta 0.775492215469028\n",
      "Test loss window average  0.6034104  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 94403\n",
      "======== Trial 71 of 99 =========\n",
      "Batch size 39  Iters 59183  Lr 0.007909902639204902  Layers 2  Dropout 0.42329348708849424  Layer Size 29 beta 0.12627175026695564\n",
      "Test loss window average  0.6036834  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 59183\n",
      "======== Trial 72 of 99 =========\n",
      "Batch size 50  Iters 15160  Lr 0.005258378170210427  Layers 4  Dropout 0.17583999243383672  Layer Size 12 beta 1.026093795386654\n",
      "Test loss window average  0.6253569  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 15160\n",
      "======== Trial 73 of 99 =========\n",
      "Batch size 39  Iters 89192  Lr 0.0034151568510616915  Layers 4  Dropout 0.36214278789473986  Layer Size 18 beta 0.6632256609180281\n",
      "Test loss window average  0.6086896  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 89192\n",
      "======== Trial 74 of 99 =========\n",
      "Batch size 57  Iters 38479  Lr 0.006097903251038383  Layers 4  Dropout 0.3085919501608211  Layer Size 25 beta 1.4132638364018135\n",
      "Test loss window average  0.6140044  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 38479\n",
      "======== Trial 75 of 99 =========\n",
      "Batch size 52  Iters 49228  Lr 0.009436834142419291  Layers 3  Dropout 0.33682014535313887  Layer Size 8 beta 1.310033962597363\n",
      "Test loss window average  0.6318839  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 49228\n",
      "======== Trial 76 of 99 =========\n",
      "Batch size 50  Iters 42437  Lr 0.0026947838746123453  Layers 4  Dropout 0.21744474003879818  Layer Size 30 beta 1.3942381730295557\n",
      "Test loss window average  0.6060064  increasing, breaking loop at iter  1775\n",
      "Best number of iterations:  1775 compared with total: 42437\n",
      "======== Trial 77 of 99 =========\n",
      "Batch size 17  Iters 76686  Lr 0.006527490197484412  Layers 2  Dropout 0.3377571629010211  Layer Size 7 beta 1.3376191628737701\n",
      "Test loss window average  0.607732  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 76686\n",
      "======== Trial 78 of 99 =========\n",
      "Batch size 10  Iters 34254  Lr 0.006691599373676117  Layers 4  Dropout 0.10834681892009367  Layer Size 23 beta 0.470375151757079\n",
      "Test loss window average  0.601353  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 34254\n",
      "======== Trial 79 of 99 =========\n",
      "Batch size 58  Iters 41688  Lr 0.006704262763116194  Layers 2  Dropout 0.23690957020797634  Layer Size 16 beta 0.43878321059719544\n",
      "Test loss window average  0.6052759  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 41688\n",
      "======== Trial 80 of 99 =========\n",
      "Batch size 15  Iters 58284  Lr 0.009577510836863416  Layers 5  Dropout 0.4057028305648507  Layer Size 27 beta 1.2940411594277126\n",
      "Test loss window average  0.62400836  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 58284\n",
      "======== Trial 81 of 99 =========\n",
      "Batch size 20  Iters 71420  Lr 0.009439022496244417  Layers 4  Dropout 0.47352658357569655  Layer Size 15 beta 1.3537740564878973\n",
      "Test loss window average  0.6384  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 71420\n",
      "======== Trial 82 of 99 =========\n",
      "Batch size 57  Iters 78926  Lr 0.007947170253175399  Layers 5  Dropout 0.11348293583068464  Layer Size 7 beta 0.3519791398637302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.610504  increasing, breaking loop at iter  4070\n",
      "Best number of iterations:  4070 compared with total: 78926\n",
      "======== Trial 83 of 99 =========\n",
      "Batch size 28  Iters 94343  Lr 0.003165444300190518  Layers 4  Dropout 0.35402830211116165  Layer Size 8 beta 0.6947050156389142\n",
      "Test loss window average  0.61083245  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 94343\n",
      "======== Trial 84 of 99 =========\n",
      "Batch size 63  Iters 19031  Lr 0.00267482309033904  Layers 3  Dropout 0.3041652218365427  Layer Size 31 beta 1.1704650251538373\n",
      "Test loss window average  0.6088924  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 19031\n",
      "======== Trial 85 of 99 =========\n",
      "Batch size 51  Iters 87949  Lr 0.008405938017368086  Layers 3  Dropout 0.47911866268483816  Layer Size 22 beta 1.2368973240166163\n",
      "Test loss window average  0.62120575  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 87949\n",
      "======== Trial 86 of 99 =========\n",
      "Batch size 18  Iters 17553  Lr 0.005580096887003922  Layers 5  Dropout 0.23108118740516714  Layer Size 28 beta 1.7515857034191975\n",
      "Test loss window average  0.6263803  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 17553\n",
      "======== Trial 87 of 99 =========\n",
      "Batch size 30  Iters 28372  Lr 0.0024085682140268035  Layers 2  Dropout 0.48802376791532376  Layer Size 15 beta 0.3380048411338602\n",
      "Test loss window average  0.6248078  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 28372\n",
      "======== Trial 88 of 99 =========\n",
      "Batch size 34  Iters 34915  Lr 0.005262574022535954  Layers 3  Dropout 0.4005014066232482  Layer Size 9 beta 0.20408927336292987\n",
      "Test loss window average  0.60234046  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 34915\n",
      "======== Trial 89 of 99 =========\n",
      "Batch size 11  Iters 66842  Lr 0.0029932672283290405  Layers 3  Dropout 0.11895885305346199  Layer Size 11 beta 1.9753390022141553\n",
      "Test loss window average  0.6174269  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 66842\n",
      "======== Trial 90 of 99 =========\n",
      "Batch size 61  Iters 18036  Lr 0.004911483971131033  Layers 2  Dropout 0.22606002898782684  Layer Size 29 beta 1.5610170340108465\n",
      "Test loss window average  0.6124958  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 18036\n",
      "======== Trial 91 of 99 =========\n",
      "Batch size 31  Iters 27313  Lr 0.008190345895786618  Layers 2  Dropout 0.42223259073996333  Layer Size 20 beta 1.6723154690276512\n",
      "Test loss window average  0.60734844  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 27313\n",
      "======== Trial 92 of 99 =========\n",
      "Batch size 12  Iters 32839  Lr 0.0028923163648386084  Layers 4  Dropout 0.3022847501851234  Layer Size 28 beta 0.520493258396187\n",
      "Test loss window average  0.6207985  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 32839\n",
      "======== Trial 93 of 99 =========\n",
      "Batch size 60  Iters 54455  Lr 0.004315507733604548  Layers 3  Dropout 0.4667022008258398  Layer Size 28 beta 0.9186497058215545\n",
      "Test loss window average  0.6179667  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 54455\n",
      "======== Trial 94 of 99 =========\n",
      "Batch size 50  Iters 31620  Lr 0.008722649259244548  Layers 3  Dropout 0.49218967918074663  Layer Size 28 beta 1.6895574948106185\n",
      "Test loss window average  0.60461843  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 31620\n",
      "======== Trial 95 of 99 =========\n",
      "Batch size 43  Iters 15075  Lr 0.0017829493488265267  Layers 3  Dropout 0.4429445974480011  Layer Size 17 beta 1.5499638377115394\n",
      "Test loss window average  0.6087274  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 15075\n",
      "======== Trial 96 of 99 =========\n",
      "Batch size 61  Iters 91704  Lr 0.0020092833478090972  Layers 5  Dropout 0.2422022992535504  Layer Size 24 beta 1.0581006311643522\n",
      "Test loss window average  0.62038326  increasing, breaking loop at iter  3305\n",
      "Best number of iterations:  3305 compared with total: 91704\n",
      "======== Trial 97 of 99 =========\n",
      "Batch size 30  Iters 38701  Lr 0.005863095106986452  Layers 4  Dropout 0.4045605811920988  Layer Size 5 beta 0.25382009856325727\n",
      "Test loss window average  0.6228348  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 38701\n",
      "======== Trial 98 of 99 =========\n",
      "Batch size 42  Iters 95037  Lr 0.007902766867670096  Layers 3  Dropout 0.3941292255763841  Layer Size 25 beta 0.8122002919615718\n",
      "Test loss window average  0.6232257  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 95037\n",
      "======== Trial 99 of 99 =========\n",
      "Batch size 62  Iters 69415  Lr 0.0022583234029146197  Layers 3  Dropout 0.11388596732310839  Layer Size 11 beta 0.2815679480883379\n",
      "Test loss window average  0.6016017  increasing, breaking loop at iter  2540\n",
      "Best number of iterations:  2540 compared with total: 69415\n",
      "======== Trial 0 of 99 =========\n",
      "Batch size 18  Iters 32787  Lr 0.008610200658014234  Layers 3  Dropout 0.43174338192508843  Layer Size 23 beta 1.5160443498390268\n",
      "Test loss window average  0.676382  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 32787\n",
      "old loss: 10000000000.0\n",
      "new loss: 0.6516479253768921\n",
      "best model updated\n",
      "======== Trial 1 of 99 =========\n",
      "Batch size 25  Iters 70423  Lr 0.00938314503481013  Layers 5  Dropout 0.48939773861225977  Layer Size 30 beta 1.448180651226517\n",
      "Test loss window average  0.79388344  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 70423\n",
      "======== Trial 2 of 99 =========\n",
      "Batch size 39  Iters 79256  Lr 0.00467253055663015  Layers 5  Dropout 0.16216210289184635  Layer Size 18 beta 1.4129262988779778\n",
      "Test loss window average  0.73234236  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 79256\n",
      "======== Trial 3 of 99 =========\n",
      "Batch size 36  Iters 73559  Lr 0.002596994139803627  Layers 4  Dropout 0.406667369614653  Layer Size 25 beta 0.3641212680208464\n",
      "Test loss window average  0.58728796  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 73559\n",
      "======== Trial 4 of 99 =========\n",
      "Batch size 13  Iters 39520  Lr 0.005308081278147674  Layers 3  Dropout 0.38755546743153935  Layer Size 6 beta 0.23079270410477262\n",
      "Test loss window average  0.57466  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 39520\n",
      "======== Trial 5 of 99 =========\n",
      "Batch size 52  Iters 42045  Lr 0.009104902344842769  Layers 2  Dropout 0.17618851611780226  Layer Size 17 beta 1.0935638097117968\n",
      "Test loss window average  0.7048447  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 42045\n",
      "old loss: 0.54638606\n",
      "new loss: 0.7894101142883301\n",
      "best model updated\n",
      "======== Trial 6 of 99 =========\n",
      "Batch size 53  Iters 75274  Lr 0.00034321182283899904  Layers 4  Dropout 0.23940218638026237  Layer Size 24 beta 0.3112227435559111\n",
      "Test loss window average  0.55459917  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 75274\n",
      "======== Trial 7 of 99 =========\n",
      "Batch size 48  Iters 56117  Lr 0.0020340176252488943  Layers 2  Dropout 0.3740206871804477  Layer Size 27 beta 0.2604899454277876\n",
      "Test loss window average  0.5556215  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 56117\n",
      "======== Trial 8 of 99 =========\n",
      "Batch size 33  Iters 57212  Lr 0.001709670655974065  Layers 2  Dropout 0.3618370799888021  Layer Size 17 beta 0.3585433874941257\n",
      "Test loss window average  0.5507516  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 57212\n",
      "======== Trial 9 of 99 =========\n",
      "Batch size 11  Iters 31507  Lr 0.0018525332742439682  Layers 4  Dropout 0.22669304725477316  Layer Size 20 beta 0.27025866966023215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.54468286  increasing, breaking loop at iter  2030\n",
      "Best number of iterations:  2030 compared with total: 31507\n",
      "======== Trial 10 of 99 =========\n",
      "Batch size 24  Iters 75296  Lr 0.004153693901282643  Layers 3  Dropout 0.19291989549562244  Layer Size 29 beta 1.484639110669337\n",
      "Test loss window average  0.6460081  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 75296\n",
      "======== Trial 11 of 99 =========\n",
      "Batch size 62  Iters 73944  Lr 0.000989954818078564  Layers 5  Dropout 0.4635915657132851  Layer Size 26 beta 1.1413234802869827\n",
      "Test loss window average  0.65376973  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 73944\n",
      "======== Trial 12 of 99 =========\n",
      "Batch size 29  Iters 59506  Lr 0.005798893039960468  Layers 2  Dropout 0.2267502551452052  Layer Size 9 beta 0.3866289465149163\n",
      "Test loss window average  0.5358733  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 59506\n",
      "old loss: 0.53744924\n",
      "new loss: 0.5394327640533447\n",
      "best model updated\n",
      "======== Trial 13 of 99 =========\n",
      "Batch size 61  Iters 31659  Lr 0.009709412928956166  Layers 4  Dropout 0.10836786186803092  Layer Size 27 beta 0.18059585505280146\n",
      "Test loss window average  0.54766005  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 31659\n",
      "======== Trial 14 of 99 =========\n",
      "Batch size 12  Iters 93836  Lr 0.0014145934088253907  Layers 3  Dropout 0.43055031285337075  Layer Size 9 beta 0.24524006376015622\n",
      "Test loss window average  0.5411863  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 93836\n",
      "======== Trial 15 of 99 =========\n",
      "Batch size 61  Iters 91730  Lr 0.003118241453248881  Layers 3  Dropout 0.39105246767833735  Layer Size 8 beta 0.5521564900549417\n",
      "Test loss window average  0.6706526  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 91730\n",
      "======== Trial 16 of 99 =========\n",
      "Batch size 61  Iters 21904  Lr 0.004267817920501188  Layers 4  Dropout 0.12659210124037026  Layer Size 24 beta 0.655037768465039\n",
      "Test loss window average  0.68268526  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 21904\n",
      "======== Trial 17 of 99 =========\n",
      "Batch size 34  Iters 80294  Lr 0.006945612914277498  Layers 4  Dropout 0.13257290989712234  Layer Size 19 beta 1.609305372291878\n",
      "Test loss window average  0.8029282  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 80294\n",
      "======== Trial 18 of 99 =========\n",
      "Batch size 33  Iters 68769  Lr 0.006835594864617094  Layers 2  Dropout 0.27392449806222624  Layer Size 11 beta 1.5191402605189643\n",
      "Test loss window average  0.867827  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 68769\n",
      "======== Trial 19 of 99 =========\n",
      "Batch size 45  Iters 71385  Lr 0.003450542257518878  Layers 3  Dropout 0.3550619831568633  Layer Size 21 beta 1.3335923795404934\n",
      "Test loss window average  0.6787475  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 71385\n",
      "======== Trial 20 of 99 =========\n",
      "Batch size 11  Iters 53578  Lr 0.00016510153490389772  Layers 4  Dropout 0.3834456067678661  Layer Size 11 beta 1.8013847807731125\n",
      "Test loss window average  0.56642896  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 53578\n",
      "======== Trial 21 of 99 =========\n",
      "Batch size 34  Iters 72139  Lr 0.007841649868135755  Layers 2  Dropout 0.18406431556174355  Layer Size 18 beta 0.7841257147809567\n",
      "Test loss window average  0.6367061  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 72139\n",
      "======== Trial 22 of 99 =========\n",
      "Batch size 32  Iters 81747  Lr 0.008065901390630736  Layers 4  Dropout 0.35675888328821626  Layer Size 22 beta 0.5710268434103214\n",
      "Test loss window average  0.6109898  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 81747\n",
      "======== Trial 23 of 99 =========\n",
      "Batch size 34  Iters 65824  Lr 0.00526221962502717  Layers 5  Dropout 0.1531969015225553  Layer Size 21 beta 0.07741766010632929\n",
      "Test loss window average  0.54517764  increasing, breaking loop at iter  1520\n",
      "Best number of iterations:  1520 compared with total: 65824\n",
      "======== Trial 24 of 99 =========\n",
      "Batch size 63  Iters 99993  Lr 0.008293617426927628  Layers 2  Dropout 0.4776242427814523  Layer Size 30 beta 1.4073093367040723\n",
      "Test loss window average  0.79382044  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 99993\n",
      "======== Trial 25 of 99 =========\n",
      "Batch size 32  Iters 94037  Lr 0.009829759342179  Layers 2  Dropout 0.4552430969205684  Layer Size 4 beta 0.49591539093590103\n",
      "Test loss window average  0.69174683  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 94037\n",
      "======== Trial 26 of 99 =========\n",
      "Batch size 39  Iters 21855  Lr 0.007915253086508056  Layers 5  Dropout 0.1980035669509424  Layer Size 12 beta 0.7632695887379602\n",
      "Test loss window average  0.6634696  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 21855\n",
      "======== Trial 27 of 99 =========\n",
      "Batch size 11  Iters 64099  Lr 0.0008983928824700361  Layers 2  Dropout 0.3938450706437371  Layer Size 15 beta 1.6521398252540043\n",
      "Test loss window average  0.55845267  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 64099\n",
      "======== Trial 28 of 99 =========\n",
      "Batch size 33  Iters 41293  Lr 0.005954341963716827  Layers 5  Dropout 0.10014425307548303  Layer Size 9 beta 1.1305909435244936\n",
      "Test loss window average  0.69884974  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 41293\n",
      "======== Trial 29 of 99 =========\n",
      "Batch size 13  Iters 88003  Lr 0.007911013456635672  Layers 4  Dropout 0.3027896467486184  Layer Size 26 beta 1.3859148647260122\n",
      "Test loss window average  0.5787679  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 88003\n",
      "old loss: 0.5197088\n",
      "new loss: 0.5933458805084229\n",
      "best model updated\n",
      "======== Trial 30 of 99 =========\n",
      "Batch size 38  Iters 15763  Lr 0.008093074390017281  Layers 5  Dropout 0.21843888851975912  Layer Size 27 beta 0.739724368989612\n",
      "Test loss window average  0.63854903  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 15763\n",
      "======== Trial 31 of 99 =========\n",
      "Batch size 58  Iters 99979  Lr 0.003993852658717099  Layers 4  Dropout 0.2811529584546695  Layer Size 22 beta 0.6549121908688881\n",
      "Test loss window average  0.6231067  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 99979\n",
      "======== Trial 32 of 99 =========\n",
      "Batch size 11  Iters 60313  Lr 0.0004287506636245805  Layers 3  Dropout 0.21212850483801007  Layer Size 26 beta 0.33223743670575684\n",
      "Test loss window average  0.54939246  increasing, breaking loop at iter  2285\n",
      "Best number of iterations:  2285 compared with total: 60313\n",
      "======== Trial 33 of 99 =========\n",
      "Batch size 21  Iters 40689  Lr 0.0036915242841773824  Layers 2  Dropout 0.4008836273630323  Layer Size 8 beta 1.403348890628374\n",
      "Test loss window average  0.69338834  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 40689\n",
      "======== Trial 34 of 99 =========\n",
      "Batch size 42  Iters 49963  Lr 0.003813152170971842  Layers 4  Dropout 0.44109798195421857  Layer Size 7 beta 1.7161791716818648\n",
      "Test loss window average  0.7548613  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 49963\n",
      "======== Trial 35 of 99 =========\n",
      "Batch size 58  Iters 40930  Lr 0.0003068054790118264  Layers 2  Dropout 0.49221245402565517  Layer Size 23 beta 0.09297777371252097\n",
      "Test loss window average  0.5513551  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 40930\n",
      "======== Trial 36 of 99 =========\n",
      "Batch size 18  Iters 32219  Lr 0.008933092333495703  Layers 3  Dropout 0.45705704194537344  Layer Size 23 beta 0.1518765716421783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.56554174  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 32219\n",
      "======== Trial 37 of 99 =========\n",
      "Batch size 58  Iters 74052  Lr 0.008682715422414793  Layers 4  Dropout 0.24038041020039738  Layer Size 23 beta 0.9427475133710144\n",
      "Test loss window average  0.70681185  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 74052\n",
      "======== Trial 38 of 99 =========\n",
      "Batch size 57  Iters 62520  Lr 0.005602804109515495  Layers 3  Dropout 0.1329302126017672  Layer Size 31 beta 1.2038329691610632\n",
      "Test loss window average  0.7603988  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 62520\n",
      "======== Trial 39 of 99 =========\n",
      "Batch size 14  Iters 20889  Lr 0.0031585869451442577  Layers 4  Dropout 0.18337713912471526  Layer Size 9 beta 0.23246570464509853\n",
      "Test loss window average  0.5241232  increasing, breaking loop at iter  3050\n",
      "Best number of iterations:  3050 compared with total: 20889\n",
      "old loss: 0.51877433\n",
      "new loss: 0.5253559947013855\n",
      "best model updated\n",
      "======== Trial 40 of 99 =========\n",
      "Batch size 37  Iters 52544  Lr 0.00420434064618155  Layers 2  Dropout 0.11612167701413255  Layer Size 22 beta 0.44513124953804994\n",
      "Test loss window average  0.5355807  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 52544\n",
      "old loss: 0.51768416\n",
      "new loss: 0.544375479221344\n",
      "best model updated\n",
      "======== Trial 41 of 99 =========\n",
      "Batch size 44  Iters 55002  Lr 0.005742066115766995  Layers 4  Dropout 0.18430186434863072  Layer Size 11 beta 1.7770476671785806\n",
      "Test loss window average  0.73565805  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 55002\n",
      "======== Trial 42 of 99 =========\n",
      "Batch size 37  Iters 17329  Lr 0.003603005601393555  Layers 5  Dropout 0.4723266544043273  Layer Size 16 beta 0.618534172497946\n",
      "Test loss window average  0.65803444  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 17329\n",
      "======== Trial 43 of 99 =========\n",
      "Batch size 55  Iters 30186  Lr 0.009607890660048763  Layers 5  Dropout 0.18805770600105529  Layer Size 9 beta 0.19931656620556\n",
      "Test loss window average  0.60902935  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 30186\n",
      "======== Trial 44 of 99 =========\n",
      "Batch size 22  Iters 91940  Lr 0.008207663226633495  Layers 5  Dropout 0.2474321312039598  Layer Size 6 beta 1.9638622329673168\n",
      "Test loss window average  0.7875547  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 91940\n",
      "======== Trial 45 of 99 =========\n",
      "Batch size 56  Iters 87155  Lr 0.0056715907203454445  Layers 5  Dropout 0.16206548754199368  Layer Size 9 beta 0.19800631737381902\n",
      "Test loss window average  0.61023235  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 87155\n",
      "======== Trial 46 of 99 =========\n",
      "Batch size 25  Iters 34826  Lr 0.0019916153807634355  Layers 3  Dropout 0.3714233991966134  Layer Size 13 beta 1.2349197357152273\n",
      "Test loss window average  0.624406  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 34826\n",
      "======== Trial 47 of 99 =========\n",
      "Batch size 63  Iters 26086  Lr 0.0011187180374476727  Layers 5  Dropout 0.43812950038948584  Layer Size 26 beta 1.3516418380799102\n",
      "Test loss window average  0.65751565  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 26086\n",
      "======== Trial 48 of 99 =========\n",
      "Batch size 28  Iters 89070  Lr 0.0012898680707505114  Layers 3  Dropout 0.3694554843545407  Layer Size 7 beta 1.3563629661156897\n",
      "Test loss window average  0.65211797  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 89070\n",
      "======== Trial 49 of 99 =========\n",
      "Batch size 27  Iters 56803  Lr 0.0026295796612770626  Layers 5  Dropout 0.20450927966526541  Layer Size 6 beta 0.14940273985305083\n",
      "Test loss window average  0.55886  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 56803\n",
      "======== Trial 50 of 99 =========\n",
      "Batch size 45  Iters 61311  Lr 0.002057460843063601  Layers 4  Dropout 0.23838690549029598  Layer Size 5 beta 1.3020169502777628\n",
      "Test loss window average  0.6636406  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 61311\n",
      "======== Trial 51 of 99 =========\n",
      "Batch size 38  Iters 70066  Lr 0.003326831190389066  Layers 5  Dropout 0.23908565362385664  Layer Size 16 beta 0.053405090444992045\n",
      "Test loss window average  0.55633044  increasing, breaking loop at iter  1265\n",
      "Best number of iterations:  1265 compared with total: 70066\n",
      "======== Trial 52 of 99 =========\n",
      "Batch size 42  Iters 43971  Lr 0.0034316617095944474  Layers 2  Dropout 0.4664097847672277  Layer Size 10 beta 0.9524074767263377\n",
      "Test loss window average  0.6363731  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 43971\n",
      "======== Trial 53 of 99 =========\n",
      "Batch size 27  Iters 80924  Lr 0.0036990559424538378  Layers 3  Dropout 0.322297241305582  Layer Size 30 beta 1.7550318802172418\n",
      "Test loss window average  0.75064766  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 80924\n",
      "======== Trial 54 of 99 =========\n",
      "Batch size 29  Iters 69540  Lr 0.0041964580212584006  Layers 4  Dropout 0.2799876378845604  Layer Size 10 beta 1.3257224286513813\n",
      "Test loss window average  0.71265817  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 69540\n",
      "======== Trial 55 of 99 =========\n",
      "Batch size 27  Iters 93228  Lr 0.007857067214794606  Layers 4  Dropout 0.2789120930731368  Layer Size 17 beta 1.0894840000691532\n",
      "Test loss window average  0.6837217  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 93228\n",
      "======== Trial 56 of 99 =========\n",
      "Batch size 14  Iters 80701  Lr 0.0010131899792945607  Layers 3  Dropout 0.241244795056881  Layer Size 19 beta 0.5369007884090037\n",
      "Test loss window average  0.53478545  increasing, breaking loop at iter  1010\n",
      "Best number of iterations:  1010 compared with total: 80701\n",
      "======== Trial 57 of 99 =========\n",
      "Batch size 13  Iters 84901  Lr 0.007909537646165457  Layers 4  Dropout 0.4193958570792342  Layer Size 9 beta 1.6031936536807403\n",
      "Test loss window average  0.6395409  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 84901\n",
      "======== Trial 58 of 99 =========\n",
      "Batch size 55  Iters 90044  Lr 0.005323929777718238  Layers 5  Dropout 0.2585413357915275  Layer Size 4 beta 1.7650424110699088\n",
      "Test loss window average  0.7804427  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 90044\n",
      "======== Trial 59 of 99 =========\n",
      "Batch size 57  Iters 47430  Lr 0.00837152870246784  Layers 4  Dropout 0.35426168994877916  Layer Size 18 beta 1.0061013166619415\n",
      "Test loss window average  0.73770803  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 47430\n",
      "======== Trial 60 of 99 =========\n",
      "Batch size 43  Iters 43757  Lr 0.001229218235831192  Layers 4  Dropout 0.3392114821734956  Layer Size 25 beta 1.645044093771444\n",
      "Test loss window average  0.67159945  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 43757\n",
      "======== Trial 61 of 99 =========\n",
      "Batch size 27  Iters 66918  Lr 0.003360464119154929  Layers 4  Dropout 0.31163452979206224  Layer Size 18 beta 0.981896102883784\n",
      "Test loss window average  0.66376245  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 66918\n",
      "======== Trial 62 of 99 =========\n",
      "Batch size 20  Iters 89147  Lr 0.009081251133171753  Layers 3  Dropout 0.14308889767800947  Layer Size 7 beta 0.7118416712165866\n",
      "Test loss window average  0.6357058  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 89147\n",
      "======== Trial 63 of 99 =========\n",
      "Batch size 36  Iters 74869  Lr 0.009038056933794041  Layers 5  Dropout 0.2527364042686119  Layer Size 13 beta 0.5059136018198948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss window average  0.61308396  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 74869\n",
      "======== Trial 64 of 99 =========\n",
      "Batch size 57  Iters 78995  Lr 0.0002461990459760442  Layers 5  Dropout 0.10847887859565053  Layer Size 15 beta 1.927096061914784\n",
      "Test loss window average  0.55583936  increasing, breaking loop at iter  755\n",
      "Best number of iterations:  755 compared with total: 78995\n",
      "======== Trial 65 of 99 =========\n",
      "Batch size 20  Iters 99617  Lr 0.0027571887702228792  Layers 2  Dropout 0.3757055105901296  Layer Size 24 beta 1.8442025562895688\n",
      "Test loss window average  0.7443093  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 99617\n",
      "======== Trial 66 of 99 =========\n",
      "Batch size 28  Iters 35552  Lr 0.009160124283408407  Layers 2  Dropout 0.3189642541155697  Layer Size 19 beta 0.9850237043621632\n",
      "Test loss window average  0.64043343  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 35552\n",
      "======== Trial 67 of 99 =========\n",
      "Batch size 55  Iters 38896  Lr 0.005245067017971509  Layers 4  Dropout 0.2649314620382882  Layer Size 17 beta 0.795708131304997\n",
      "Test loss window average  0.662456  increasing, breaking loop at iter  500\n",
      "Best number of iterations:  500 compared with total: 38896\n",
      "======== Trial 68 of 99 =========\n",
      "Batch size 18  Iters 28916  Lr 0.0035844373559995755  Layers 3  Dropout 0.2746279803892351  Layer Size 29 beta 0.31172573809132825\n"
     ]
    }
   ],
   "source": [
    "# First establish ground truth treatment effect:\n",
    "N = 5000000\n",
    "Z, x, y, Y1, Y0 = generate_data(N, seed=0)\n",
    "true_psi = (Y1-Y0).mean()\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Set some params\n",
    "N = 10000\n",
    "seed = 0\n",
    "num_tuning_trials = 100\n",
    "\n",
    "# data generation:\n",
    "z, x, y, _, _ = generate_data(N, 0)\n",
    "x = torch.tensor(x).type(torch.float32)\n",
    "z = torch.tensor(z).type(torch.float32)\n",
    "y = torch.tensor(y).type(torch.float32)\n",
    "\n",
    "gtuner = Tuner(x=x,y=y,z=z, net_type='G', trials=num_tuning_trials)\n",
    "gtuning_history, best_g, x_pred = gtuner.tune()\n",
    "\n",
    "gtotal_losses = np.asarray(gtuning_history['best_model_test_loss'])\n",
    "gbest_index = np.argmin(gtotal_losses)\n",
    "\n",
    "gbest_params = {}\n",
    "for key in gtuning_history.keys():\n",
    "    gbest_params[key] = gtuning_history[key][gbest_index]\n",
    "    \n",
    "    \n",
    "qtuner = Tuner(x=x,y=y,z=z, x_pred=x_pred, net_type='Q', trials=num_tuning_trials)\n",
    "qtuning_history, best_q, _ = qtuner.tune()\n",
    "\n",
    "qtotal_losses = np.asarray(qtuning_history['best_model_test_loss'])\n",
    "qbest_index = np.argmin(qtotal_losses)\n",
    "\n",
    "qbest_params = {}\n",
    "for key in qtuning_history.keys():\n",
    "    qbest_params[key] = qtuning_history[key][qbest_index]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4090404",
   "metadata": {},
   "source": [
    "## 6. Run Simulation\n",
    "\n",
    "Now we have the best hyperparameters, we will run the simulations accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9709b014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Q params: {'batch_size': 36, 'layers': 2, 'dropout': 0.28408836655342484, 'beta': 1.2150404599988116, 'layer_size': 5, 'lr': 0.0004759374263958064, 'iters': 20790, 'stop_it': 1010, 'train_loss': 0.5432111620903015, 'val_loss': 0.5650272369384766, 'best_model_test_loss': array(0.5628734, dtype=float32)}\n",
      "Best G params: {'batch_size': 44, 'layers': 2, 'dropout': 0.3967010199864429, 'beta': 1.7949885810096247, 'layer_size': 7, 'lr': 0.000652378214512967, 'iters': 53624, 'stop_it': 755, 'train_loss': 0.6335910558700562, 'val_loss': 0.6356383562088013, 'best_model_test_loss': array(0.6343429, dtype=float32)}\n",
      "=====================RUN 0===================\n",
      "Test loss window average  0.6117008  increasing, breaking loop at iter  1265\n",
      "Test loss window average  0.5480575  increasing, breaking loop at iter  8405\n",
      "Test loss window average  0.5740512  increasing, breaking loop at iter  500\n",
      "=====================RUN 1===================\n",
      "Test loss window average  0.6185158  increasing, breaking loop at iter  755\n",
      "Test loss window average  0.5612036  increasing, breaking loop at iter  1010\n",
      "Test loss window average  0.56207776  increasing, breaking loop at iter  1265\n",
      "=====================RUN 2===================\n",
      "Test loss window average  0.61153734  increasing, breaking loop at iter  1520\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25402/2193893655.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# train Q  (no treg)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     train_loss_q_,  val_loss_q_, stop_it_q, best_model_q, best_model_test_loss_q, eps = qtrainer.train(x,\n\u001b[0;32m---> 62\u001b[0;31m                                                                                                   y, z, x_pred=x_pred)   \n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;31m# generate counterfactual preds (no treg)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mQ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_int1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25402/1138716475.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, z, x_pred)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mloss_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0mloss_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mtest_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25402/1138716475.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, x, y, z)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Q'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutcome_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'categorical'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/PhD_part_1/anaconda3/envs/my-torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25402/3245221715.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, Z)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mout0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_arm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_arm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/PhD_part_1/anaconda3/envs/my-torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/PhD_part_1/anaconda3/envs/my-torch/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/PhD_part_1/anaconda3/envs/my-torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/PhD_part_1/anaconda3/envs/my-torch/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/PhD_part_1/anaconda3/envs/my-torch/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/PhD_part_1/anaconda3/envs/my-torch/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/PhD_part_1/anaconda3/envs/my-torch/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0mfnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m             \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazycache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;31m# Must defer line lookups until we have called checkcache.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Best Q params:', qbest_params)\n",
    "print('Best G params:', gbest_params)\n",
    "N = 10000\n",
    "seed = 0\n",
    "num_runs = 500\n",
    "\n",
    "output_type_Q = 'categorical'\n",
    "output_size_Q = 1\n",
    "output_type_G = 'categorical'\n",
    "output_size_G = 1\n",
    "input_size_Q = z.shape[-1] + 1  # we will concatenate the treatment var inside the qnet class\n",
    "input_size_G = z.shape[-1]\n",
    "qlayers = qbest_params['layers']\n",
    "qdropout = qbest_params['dropout']\n",
    "qlayer_size = qbest_params['layer_size']\n",
    "qiters = 100000  # use the early stopping iter\n",
    "qlr = qbest_params['lr']\n",
    "beta = qbest_params['beta']\n",
    "qbatch_size = qbest_params['batch_size']\n",
    "\n",
    "glayers = gbest_params['layers']\n",
    "gdropout = gbest_params['dropout']\n",
    "glayer_size = gbest_params['layer_size']\n",
    "giters = 100000  # use the early stopping iter\n",
    "glr = gbest_params['lr']\n",
    "gbatch_size = gbest_params['batch_size']\n",
    "\n",
    "estimates_naive = []\n",
    "estimates_upd_treg = []\n",
    "for i in range(num_runs):\n",
    "    print('=====================RUN {}==================='.format(i))\n",
    "    seed += 1\n",
    "    # data generation:\n",
    "    z, x, y, _, _ = generate_data(N, seed=seed)\n",
    "    x = torch.tensor(x).type(torch.float32)\n",
    "    z = torch.tensor(z).type(torch.float32)\n",
    "    y = torch.tensor(y).type(torch.float32)\n",
    "    x_int1 = torch.ones_like(x)  # this is the 'intervention data'\n",
    "    x_int0 = torch.zeros_like(x)    \n",
    "    \n",
    "\n",
    "    qnet = QNet(input_size=input_size_Q, num_layers=qlayers,\n",
    "                          layers_size=qlayer_size, output_size=output_size_Q,\n",
    "                         output_type=output_type_Q, dropout=qdropout)\n",
    "\n",
    "    gnet = GNet(input_size=input_size_G, num_layers=glayers,\n",
    "                          layers_size=glayer_size, output_size=output_size_G,\n",
    "                         output_type=output_type_G, dropout=gdropout)\n",
    "    \n",
    "    # def G trainer\n",
    "    gtrainer = Trainer(net=gnet, net_type='G', beta=beta, iterations=giters, outcome_type=output_type_G,\n",
    "                  batch_size=gbatch_size, test_iter=5, lr=glr)\n",
    "    # train G\n",
    "    train_loss_g_, val_loss_g_, stop_it_g, best_model_g, best_model_test_loss_g, eps = gtrainer.train(x, y, z)\n",
    "    # Get x_preds from G\n",
    "    _, x_pred = gtrainer.test(best_model_g, x, y, z)\n",
    "    # def Q trainer (no treg)\n",
    "    qtrainer = Trainer(net=qnet, net_type='Q',  beta=0.0, iterations=qiters, outcome_type=output_type_Q,\n",
    "                  batch_size=qbatch_size, test_iter=5, lr=qlr)\n",
    "    # train Q  (no treg)\n",
    "    train_loss_q_,  val_loss_q_, stop_it_q, best_model_q, best_model_test_loss_q, eps = qtrainer.train(x,\n",
    "                                                                                                  y, z, x_pred=x_pred)   \n",
    "    # generate counterfactual preds (no treg)\n",
    "    _,  Q1 = qtrainer.test(best_model_q, x_int1, y, z)\n",
    "    _, Q0 = qtrainer.test(best_model_q, x_int0, y, z)\n",
    "    Q1 = Q1.detach().numpy()\n",
    "    Q0 = Q0.detach().numpy()\n",
    "\n",
    "    # record initial estimate\n",
    "    biased_psi = (Q1 - Q0).mean()\n",
    "    estimates_naive.append(biased_psi)\n",
    "    \n",
    "    # redefine Q trainer (treg enabled)\n",
    "    qtrainer = Trainer(net=qnet, net_type='Q',  beta=beta, iterations=qiters, outcome_type=output_type_Q,\n",
    "                  batch_size=qbatch_size, test_iter=5, lr=qlr)\n",
    "    # retrain Q using same x_preds\n",
    "    train_loss_q_,  val_loss_q_, stop_it_q, best_model_q, best_model_test_loss_q, eps = qtrainer.train(x,y, z, x_pred=x_pred)\n",
    "\n",
    "    # generate counterfactual preds (treg enabled)\n",
    "    _,  Q1 = qtrainer.test(best_model_q, x_int1, y, z)\n",
    "    _, Q0 = qtrainer.test(best_model_q, x_int0, y, z)\n",
    "    Q1 = Q1.detach().numpy()\n",
    "    Q0 = Q0.detach().numpy()\n",
    "        \n",
    "    # record updated estimate\n",
    "    upd_psi_treg = (Q1 - Q0).mean()\n",
    "    estimates_upd_treg.append(upd_psi_treg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e37c9e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============TREG==============\n",
      "True psi:  0.1956508\n",
      "naive psi:  0.20965378  relative bias: 7.157128856263917 %\n",
      "updated TMLE psi:  0.30602002  relative bias: 56.41133153485629 %\n",
      "Reduction in bias: -49.25420267859237 %\n",
      "naive psi var: 0.00137303\n",
      "updated psi var: 0.005907191\n",
      "Average of reductions: -37.472275 %\n"
     ]
    }
   ],
   "source": [
    "print('============TREG==============')\n",
    "      \n",
    "estimates_upd_treg = np.asarray(estimates_upd_treg)\n",
    "estimates_naive = np.asarray(estimates_naive)\n",
    "\n",
    "print('True psi: ', true_psi)\n",
    "print('naive psi: ', estimates_naive.mean(), ' relative bias:',\n",
    "      (estimates_naive.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('updated TMLE psi: ', estimates_upd_treg.mean(), ' relative bias:',\n",
    "      (estimates_upd_treg.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('Reduction in bias:', np.abs(estimates_naive.mean() - true_psi)/true_psi * 100 - \n",
    "     np.abs(estimates_upd_treg.mean() - true_psi)/true_psi * 100, '%')\n",
    "\n",
    "# This takes the reduction in relative bias for each simulation first, then takes an average\n",
    "# (Owing to the nonlinearity of the ||x|| function, this gives different results which are\n",
    "# worth considering.)\n",
    "print('naive psi var:', estimates_naive.var())\n",
    "print('updated psi var:', estimates_upd_treg.var())\n",
    "errors_naive = (estimates_naive - true_psi)/true_psi *100\n",
    "errors_updated = (estimates_upd_treg - true_psi)/true_psi *100\n",
    "diff_errors = np.abs(errors_naive) - np.abs(errors_updated)\n",
    "print('Average of reductions:', diff_errors.mean(), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a92660",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3d5d74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.6162456, dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(np.asarray(val_loss_g_))\n",
    "best_model_test_loss_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fef13a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.5274386, dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(np.asarray(val_loss_q_))\n",
    "best_model_test_loss_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0eca62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b57e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03db0ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f47f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
