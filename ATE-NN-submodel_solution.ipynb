{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ec6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8ec6e",
   "metadata": {},
   "source": [
    "## Summary of Results:\n",
    "\n",
    "$\\hat Q$ is the outcome estimator, $\\hat G$ is the propensity score estimator. Their respective columns tell us which estimators are use e.g. NN means a neural network was used.\n",
    "\n",
    "'Reduction' is the relative percent error reduction when compared against the plug-in estimator using the outcome model alone. The results are averages over 60 simulations.\n",
    "\n",
    "\n",
    "| Method | $\\hat Q$ | $\\hat G$ | Reduction $\\%$ | Rel. Error $\\%$ |\n",
    "| --- | --- | --- | --- |--- |\n",
    "| Naive | $NN$ | - |- |  4.059|\n",
    "| TMLE | $NN$ | $NN$ | 1.450 | 2.608 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1217b",
   "metadata": {},
   "source": [
    "## Problem Setup:\n",
    "\n",
    "This example is taken from https://arxiv.org/abs/2107.00681 by Hines, Dukes, Diaz-Ordaz, and Vansteelandt (2021) and the empirical evaluation follows https://onlinelibrary.wiley.com/doi/full/10.1002/sim.7628 by Miguel Angel Luque-Fernandez, Michael Schomaker, Bernard Rachet, Mireille E. Schnitzer (2018).\n",
    "\n",
    "\n",
    "The following experiments are very similar to the ones in ATE.ipynb, but this time we will fit the estimators using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049bda1",
   "metadata": {},
   "source": [
    "## 1. Define the DGP and some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f134282",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-4\n",
    "def sigm(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def inv_sigm(x):\n",
    "    return np.log((x + eps) / (1 - x + eps))\n",
    "\n",
    "def generate_data(N, seed):\n",
    "    np.random.seed(seed=seed)\n",
    "    z1 = np.random.binomial(1, 0.5, (N,1))\n",
    "    z2 = np.random.binomial(1, 0.65, (N,1))\n",
    "    z3 = np.round(np.random.uniform(0, 4, (N,1)),3)\n",
    "    z4 = np.round(np.random.uniform(0, 5, (N,1)),3)\n",
    "    X = np.random.binomial(1, sigm(-0.4 + 0.2*z2 + 0.15*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y1 = np.random.binomial(1, sigm(-1 + 1 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y0 = np.random.binomial(1, sigm(-1 + 0 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y = Y1 * X + Y0 * (1-X)\n",
    "    Z = np.concatenate([z1,z2,z3,z4],1)\n",
    "    return Z, X, Y, Y1, Y0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed127b9",
   "metadata": {},
   "source": [
    "## 2. Define the Neural Network Objects/Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be134faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)     \n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layers_size, output_size, output_type, dropout):\n",
    "        super(QNet, self).__init__()      \n",
    "        \n",
    "        layers = []\n",
    "        layers.extend([nn.Linear(input_size, layers_size), nn.LeakyReLU()])\n",
    "        for i in range(num_layers-1):\n",
    "            layers.extend([nn.Linear(layers_size, layers_size), nn.LeakyReLU(), nn.Dropout(p=dropout)])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        pos_arm = []\n",
    "        pos_arm.extend([nn.Linear(layers_size, layers_size), nn.LeakyReLU()])\n",
    "        pos_arm.extend([nn.Linear(layers_size, output_size)])     \n",
    "        \n",
    "        neg_arm = []\n",
    "        neg_arm.extend([nn.Linear(layers_size, layers_size), nn.LeakyReLU()])\n",
    "        neg_arm.extend([nn.Linear(layers_size, output_size)])    \n",
    "        \n",
    "        if output_type == 'categorical':\n",
    "            pos_arm.append(nn.Sigmoid())\n",
    "            neg_arm.append(nn.Sigmoid())\n",
    "        elif output_type == 'continuous':\n",
    "            pass\n",
    "        self.pos_arm = nn.Sequential(*pos_arm)\n",
    "        self.neg_arm = nn.Sequential(*neg_arm)\n",
    "    \n",
    "        self.net.apply(init_weights) \n",
    "        self.neg_arm.apply(init_weights) \n",
    "        self.pos_arm.apply(init_weights) \n",
    "\n",
    "\n",
    "    def forward(self, X, Z):\n",
    "        \n",
    "        out = self.net(torch.cat([X,Z],1))\n",
    "        out0 = self.neg_arm(out)\n",
    "        out1 = self.pos_arm(out)\n",
    "        cond = X.bool()\n",
    "        return torch.where(cond, out1, out0)\n",
    "\n",
    "    \n",
    "    \n",
    "class GNet(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layers_size, output_size, output_type, dropout):\n",
    "        super(GNet, self).__init__()      \n",
    "        \n",
    "        layers = []\n",
    "        layers.extend([nn.Linear(input_size, layers_size), nn.LeakyReLU()])\n",
    "        for i in range(num_layers-1):\n",
    "            layers.extend([nn.Linear(layers_size, layers_size), nn.LeakyReLU(), nn.Dropout(p=dropout)])\n",
    "        layers.extend([nn.Linear(layers_size, output_size)])\n",
    "\n",
    "        if output_type == 'categorical':\n",
    "            layers.append(nn.Sigmoid())\n",
    "        elif output_type == 'continuous':\n",
    "            pass\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(init_weights) \n",
    "        \n",
    "    def forward(self, Z):\n",
    "        return self.net(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7283e3",
   "metadata": {},
   "source": [
    "## 3. Create a Neural Network training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5e57e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, qnet, gnet, iterations=None, batch_size=None, outcome_type='categorical', test_iter=None, lr=None):\n",
    "        self.qnet = qnet\n",
    "        self.gnet = gnet\n",
    "        self.iterations = iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.test_iter = test_iter\n",
    "        self.outcome_type = outcome_type\n",
    "        \n",
    "        if lr is not None:\n",
    "            self.q_optimizer = optim.Adam(qnet.parameters(), lr=lr)\n",
    "            self.g_optimizer = optim.Adam(gnet.parameters(), lr=lr)\n",
    "        self.bce_loss = nn.BCELoss(reduction='none')\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "        \n",
    "    def train(self, x, y, z):\n",
    "        \n",
    "        # create a small validation set\n",
    "        indices = np.arange(len(x))\n",
    "        np.random.shuffle(indices)\n",
    "        val_inds = indices[:len(x)//8]\n",
    "        train_inds = indices[len(x)//8:]\n",
    "        x_val, y_val, z_val = x[val_inds], y[val_inds], z[val_inds]\n",
    "        x_train, y_train, z_train = x[train_inds], y[train_inds], z[train_inds]\n",
    "        \n",
    "        indices = np.arange(len(x_train))\n",
    "        \n",
    "        train_losses_q = []\n",
    "        train_losses_g = []\n",
    "        test_losses_q = []\n",
    "        test_losses_g = []\n",
    "        \n",
    "        for it in range(self.iterations):\n",
    "            inds = np.random.choice(indices, self.batch_size)\n",
    "            x_batch, y_batch, z_batch = x_train[inds], y_train[inds], z_train[inds]\n",
    "            \n",
    "            x_pred = self.gnet(z_batch)\n",
    "            y_pred = self.qnet(x_batch, z_batch)\n",
    "            \n",
    "            if self.outcome_type == 'categorical':\n",
    "                q_loss = self.bce_loss(y_pred, y_batch).mean()\n",
    "            else:\n",
    "                q_loss = self.mse_loss(y_pred, y_batch)\n",
    "                       \n",
    "            weight = torch.tensor([0.7, 0.3])\n",
    "            weight_ = weight[x_batch.data.view(-1).long()].view_as(x_batch)\n",
    "            g_loss = (self.bce_loss(x_pred, x_batch) * weight_).mean()\n",
    "            \n",
    "            q_loss.backward()\n",
    "            g_loss.backward()\n",
    "            \n",
    "            self.q_optimizer.step()\n",
    "            self.g_optimizer.step()\n",
    "            self.q_optimizer.zero_grad()\n",
    "            self.g_optimizer.zero_grad()\n",
    "            \n",
    "            if (it % self.test_iter == 0) or (it == (self.iterations-1)):\n",
    "                self.qnet.eval()\n",
    "                self.gnet.eval()\n",
    "                x_pred = self.gnet(z_train[:800])\n",
    "                y_pred = self.qnet(x_train[:800], z_train[:800])\n",
    "                \n",
    "                if self.outcome_type == 'categorical':\n",
    "                    q_loss = self.bce_loss(y_pred, y_train[:800]).mean()\n",
    "                else:\n",
    "                    q_loss = self.mse_loss(y_pred, y_train[:800])\n",
    "                    \n",
    "                g_loss = self.bce_loss(x_pred, x_train[:800]).mean()\n",
    "                train_losses_q.append(q_loss.item())\n",
    "                train_losses_g.append(g_loss.item())\n",
    "                \n",
    "                q_loss_test, g_loss_test, _, _ = self.test(x_val, y_val, z_val)\n",
    "                test_losses_q.append(q_loss_test.item())\n",
    "                test_losses_g.append(g_loss_test.item())\n",
    "#                 print('== Iteration {} =='.format(it))\n",
    "#                 print('Test Loss Q:', q_loss_test.item(), '  Test Loss G:', g_loss_test.item())\n",
    "                \n",
    "                self.qnet.train()\n",
    "                self.gnet.train()\n",
    "        \n",
    "        return train_losses_q, train_losses_g, test_losses_q, test_losses_g\n",
    "    \n",
    "    \n",
    "    def test(self, x, y, z):\n",
    "        self.qnet.eval()\n",
    "        self.gnet.eval()\n",
    "        \n",
    "        x_pred = self.gnet(z)\n",
    "        y_pred = self.qnet(x,z)\n",
    "        \n",
    "        if self.outcome_type == 'categorical':\n",
    "            q_loss = self.bce_loss(y_pred, y).mean()\n",
    "        else:\n",
    "            q_loss = self.mse_loss(y_pred, y)\n",
    "            \n",
    "        g_loss = self.bce_loss(x_pred, x).mean()\n",
    "        \n",
    "        \n",
    "        return q_loss, g_loss, x_pred, y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e2e5e6",
   "metadata": {},
   "source": [
    "## 4. Create a hyperparameter tuning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0658fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tuner(object):\n",
    "    def __init__(self, x, y, z, trials, best_params=None):\n",
    "        self.best_params = best_params\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "        self.trials = trials\n",
    "        self.test_iter = 500\n",
    "        self.best_params = best_params\n",
    "        self.qnet = None\n",
    "        self.gnet = None\n",
    "        self.best_model_q = None\n",
    "        self.best_model_g = None\n",
    "        \n",
    "    def tune(self):\n",
    "\n",
    "        output_type_Q = 'categorical'\n",
    "        output_size_Q = 1\n",
    "        output_type_G = 'categorical'\n",
    "        output_size_G = 1\n",
    "        input_size_Q = z.shape[-1] + 1  # we will concatenate the treatment var inside the qnet class\n",
    "        input_size_G = z.shape[-1]\n",
    "\n",
    "        train_loss_q = []\n",
    "        train_loss_g = []\n",
    "        val_loss_q = []\n",
    "        val_loss_g = []\n",
    "        bs_ = []\n",
    "        iters_ = []\n",
    "        lr_ = []\n",
    "        layers_ = []\n",
    "        dropout_ = []\n",
    "        layer_size_ = []\n",
    "        best_loss = 1e10\n",
    "        for trial in range(self.trials):\n",
    "            # sample hyper params and store the history\n",
    "            bs = np.random.randint(30,120) if self.best_params == None else self.best_params['batch_size']\n",
    "            bs_.append(bs)\n",
    "            iters = np.random.randint(5000,100000) if self.best_params == None else self.best_params['iters']\n",
    "            iters_.append(iters)\n",
    "            lr = np.random.uniform(0.0001, 0.01) if self.best_params == None else self.best_params['lr']\n",
    "            lr_.append(lr)\n",
    "            layers = np.random.randint(2, 6) if self.best_params == None else self.best_params['layers']\n",
    "            layers_.append(layers)\n",
    "            dropout = np.random.uniform(0.1,0.4) if self.best_params == None else self.best_params['dropout']\n",
    "            dropout_.append(dropout)\n",
    "            layer_size = np.random.randint(16, 128) if self.best_params == None else self.best_params['layer_size']\n",
    "            layer_size_.append(layer_size)\n",
    "            print('======== Trial {} of {} ========='.format(trial, self.trials-1))\n",
    "            print('Batch size', bs, ' Iters', iters, ' Lr', lr, ' Layers', layers,\n",
    "                 ' Dropout', dropout, ' Layer Size', layer_size)\n",
    "\n",
    "            \n",
    "\n",
    "            self.qnet = QNet(input_size=input_size_Q, num_layers=layers,\n",
    "                      layers_size=layer_size, output_size=output_size_Q,\n",
    "                     output_type=output_type_Q, dropout=dropout)\n",
    "        \n",
    "            self.gnet = GNet(input_size=input_size_G, num_layers=layers,\n",
    "                      layers_size=layer_size, output_size=output_size_G,\n",
    "                     output_type=output_type_G, dropout=dropout)\n",
    "\n",
    "\n",
    "            trainer = Trainer(qnet=self.qnet, gnet=self.gnet, iterations=iters, outcome_type=output_type_Q,\n",
    "                          batch_size=bs, test_iter=self.test_iter, lr=lr)\n",
    "            train_loss_q_, train_loss_g_, val_loss_q_, val_loss_g_ = trainer.train(self.x,\n",
    "                                                                                  self.y,\n",
    "                                                                                  self.z)\n",
    "            train_loss_q.append(train_loss_q_[-1])\n",
    "            train_loss_g.append(train_loss_g_[-1])\n",
    "            val_loss_q.append(val_loss_q_[-1])\n",
    "            val_loss_g.append(val_loss_g_[-1])\n",
    "            \n",
    "            total_val_loss = val_loss_q_[-1] + val_loss_g_[-1]\n",
    "            \n",
    "            if total_val_loss < best_loss:\n",
    "                print('old loss:', best_loss)\n",
    "                print('new loss:', total_val_loss)\n",
    "                print('best model updated')\n",
    "                best_loss = total_val_loss\n",
    "                self.best_model_q = self.qnet\n",
    "                self.best_model_g = self.gnet\n",
    "\n",
    "        tuning_dict = {'batch_size': bs_, 'layers':layers_, 'dropout':dropout_,\n",
    "                      'layer_size':layer_size_,'lr':lr_, 'iters':iters_,\n",
    "                      'train_loss_q':train_loss_q, 'train_loss_g':train_loss_g,\n",
    "                      'val_loss_q':val_loss_q, 'val_loss_g':val_loss_g}\n",
    "        \n",
    "        return tuning_dict, self.best_model_q, self.best_model_g\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6fc57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "129eeea7",
   "metadata": {},
   "source": [
    "## 5. Run Hyperparameter Search\n",
    "\n",
    "Now we have everything we need, we can initialize the neural networks, run hyperparameter search to identify the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "475fa678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Trial 0 of 59 =========\n",
      "Batch size 78  Iters 43624  Lr 0.000652378214512967  Layers 4  Dropout 0.24005866222108652  Layer Size 62\n",
      "old loss: 10000000000.0\n",
      "new loss: 1.2859086394309998\n",
      "best model updated\n",
      "======== Trial 1 of 59 =========\n",
      "Batch size 45  Iters 48618  Lr 0.005221090611224163  Layers 3  Dropout 0.2993613094025411  Layer Size 100\n",
      "old loss: 1.2859086394309998\n",
      "new loss: 1.2085352540016174\n",
      "best model updated\n",
      "======== Trial 2 of 59 =========\n",
      "Batch size 97  Iters 39623  Lr 0.005642744726642037  Layers 5  Dropout 0.31929503500296186  Layer Size 81\n",
      "old loss: 1.2085352540016174\n",
      "new loss: 1.179295301437378\n",
      "best model updated\n",
      "======== Trial 3 of 59 =========\n",
      "Batch size 83  Iters 36428  Lr 0.004064598302245466  Layers 3  Dropout 0.38967239147412736  Layer Size 31\n",
      "======== Trial 4 of 59 =========\n",
      "Batch size 49  Iters 53707  Lr 0.004296106901307403  Layers 4  Dropout 0.13701222446235628  Layer Size 55\n",
      "======== Trial 5 of 59 =========\n",
      "Batch size 57  Iters 34124  Lr 0.0009893456171323955  Layers 5  Dropout 0.349807948126327  Layer Size 69\n",
      "======== Trial 6 of 59 =========\n",
      "Batch size 61  Iters 91262  Lr 0.009624812957643857  Layers 5  Dropout 0.13738362131548867  Layer Size 94\n",
      "======== Trial 7 of 59 =========\n",
      "Batch size 100  Iters 67242  Lr 0.0040096324246987405  Layers 2  Dropout 0.2007438173968017  Layer Size 67\n",
      "======== Trial 8 of 59 =========\n",
      "Batch size 52  Iters 17866  Lr 0.005265141413864855  Layers 2  Dropout 0.11562194594287083  Layer Size 111\n",
      "======== Trial 9 of 59 =========\n",
      "Batch size 51  Iters 78517  Lr 0.004662058267081518  Layers 4  Dropout 0.2206566314665658  Layer Size 89\n",
      "======== Trial 10 of 59 =========\n",
      "Batch size 37  Iters 46181  Lr 0.007978978506516752  Layers 5  Dropout 0.331866264189426  Layer Size 92\n",
      "======== Trial 11 of 59 =========\n",
      "Batch size 101  Iters 48654  Lr 0.003338341532886814  Layers 3  Dropout 0.15591124689620753  Layer Size 60\n",
      "======== Trial 12 of 59 =========\n",
      "Batch size 64  Iters 26432  Lr 0.008649168966430544  Layers 2  Dropout 0.20221539848918846  Layer Size 123\n",
      "======== Trial 13 of 59 =========\n",
      "Batch size 98  Iters 81778  Lr 0.0029103087158495427  Layers 4  Dropout 0.33816173090518553  Layer Size 109\n",
      "======== Trial 14 of 59 =========\n",
      "Batch size 87  Iters 59192  Lr 0.009672641809277963  Layers 3  Dropout 0.15292721890513816  Layer Size 72\n",
      "======== Trial 15 of 59 =========\n",
      "Batch size 113  Iters 40962  Lr 0.0013329421932287344  Layers 5  Dropout 0.14480214814074469  Layer Size 113\n",
      "======== Trial 16 of 59 =========\n",
      "Batch size 105  Iters 17674  Lr 0.0021208150838487316  Layers 2  Dropout 0.3170558972639248  Layer Size 62\n",
      "======== Trial 17 of 59 =========\n",
      "Batch size 119  Iters 22909  Lr 0.008818606517595952  Layers 4  Dropout 0.1788905949856382  Layer Size 17\n",
      "======== Trial 18 of 59 =========\n",
      "Batch size 116  Iters 75511  Lr 0.0007022248331946378  Layers 5  Dropout 0.3747638899833834  Layer Size 18\n",
      "======== Trial 19 of 59 =========\n",
      "Batch size 85  Iters 24762  Lr 0.00017467756877279918  Layers 2  Dropout 0.3200149194376548  Layer Size 45\n",
      "======== Trial 20 of 59 =========\n",
      "Batch size 111  Iters 66864  Lr 0.004699021009794957  Layers 4  Dropout 0.3299082855024259  Layer Size 57\n",
      "======== Trial 21 of 59 =========\n",
      "Batch size 60  Iters 55868  Lr 0.004357248191328349  Layers 3  Dropout 0.10111956570407399  Layer Size 98\n",
      "======== Trial 22 of 59 =========\n",
      "Batch size 60  Iters 73627  Lr 0.0037385352677504844  Layers 4  Dropout 0.3795804358568363  Layer Size 28\n",
      "======== Trial 23 of 59 =========\n",
      "Batch size 50  Iters 77539  Lr 0.00835623860287358  Layers 4  Dropout 0.1556149275710231  Layer Size 110\n",
      "old loss: 1.179295301437378\n",
      "new loss: 1.176097810268402\n",
      "best model updated\n",
      "======== Trial 24 of 59 =========\n",
      "Batch size 89  Iters 44811  Lr 0.005248067250637142  Layers 2  Dropout 0.2887837705019591  Layer Size 52\n",
      "======== Trial 25 of 59 =========\n",
      "Batch size 108  Iters 45749  Lr 0.004854233745952371  Layers 4  Dropout 0.1467633149021949  Layer Size 82\n",
      "======== Trial 26 of 59 =========\n",
      "Batch size 42  Iters 64454  Lr 0.008999360862562429  Layers 2  Dropout 0.18184474617409457  Layer Size 66\n",
      "======== Trial 27 of 59 =========\n",
      "Batch size 107  Iters 33660  Lr 0.002772859456624884  Layers 5  Dropout 0.1397398917351662  Layer Size 70\n",
      "======== Trial 28 of 59 =========\n",
      "Batch size 38  Iters 79159  Lr 0.003449366512289922  Layers 2  Dropout 0.30760078312006234  Layer Size 62\n",
      "======== Trial 29 of 59 =========\n",
      "Batch size 51  Iters 83679  Lr 0.008946593050508659  Layers 5  Dropout 0.35182017103673935  Layer Size 120\n",
      "======== Trial 30 of 59 =========\n",
      "Batch size 99  Iters 77307  Lr 0.005663190041941784  Layers 3  Dropout 0.265029740260669  Layer Size 113\n",
      "======== Trial 31 of 59 =========\n",
      "Batch size 39  Iters 70313  Lr 0.0056522033110067964  Layers 4  Dropout 0.2095134954929858  Layer Size 67\n",
      "======== Trial 32 of 59 =========\n",
      "Batch size 108  Iters 64748  Lr 0.003989423345670154  Layers 3  Dropout 0.18462686343473197  Layer Size 54\n",
      "======== Trial 33 of 59 =========\n",
      "Batch size 61  Iters 71862  Lr 0.0008792989041558125  Layers 4  Dropout 0.19357287485976293  Layer Size 103\n",
      "======== Trial 34 of 59 =========\n",
      "Batch size 96  Iters 65765  Lr 0.00016433921347059054  Layers 5  Dropout 0.23586817113676875  Layer Size 92\n",
      "======== Trial 35 of 59 =========\n",
      "Batch size 104  Iters 82583  Lr 0.0026461808590564346  Layers 4  Dropout 0.3049284260548968  Layer Size 92\n",
      "======== Trial 36 of 59 =========\n",
      "Batch size 110  Iters 79039  Lr 0.005488613596184957  Layers 3  Dropout 0.2477041050113293  Layer Size 87\n",
      "======== Trial 37 of 59 =========\n",
      "Batch size 55  Iters 40324  Lr 0.007523146557797651  Layers 2  Dropout 0.3630661693894205  Layer Size 83\n",
      "======== Trial 38 of 59 =========\n",
      "Batch size 89  Iters 52319  Lr 0.008468404148134873  Layers 4  Dropout 0.33785548429694  Layer Size 64\n",
      "======== Trial 39 of 59 =========\n",
      "Batch size 93  Iters 94176  Lr 0.003348099595893503  Layers 4  Dropout 0.3841581282399227  Layer Size 91\n",
      "======== Trial 40 of 59 =========\n",
      "Batch size 69  Iters 83204  Lr 0.0001667980459471015  Layers 5  Dropout 0.28189914550019496  Layer Size 42\n",
      "======== Trial 41 of 59 =========\n",
      "Batch size 45  Iters 47030  Lr 0.007845217887913335  Layers 5  Dropout 0.13947763731012888  Layer Size 44\n",
      "======== Trial 42 of 59 =========\n",
      "Batch size 69  Iters 84099  Lr 0.004066014849836972  Layers 4  Dropout 0.19005162804348863  Layer Size 46\n",
      "======== Trial 43 of 59 =========\n",
      "Batch size 119  Iters 59589  Lr 0.007317721372163117  Layers 3  Dropout 0.38825382328575875  Layer Size 116\n",
      "======== Trial 44 of 59 =========\n",
      "Batch size 96  Iters 97100  Lr 0.009380227953051108  Layers 2  Dropout 0.27864136328928885  Layer Size 31\n",
      "======== Trial 45 of 59 =========\n",
      "Batch size 109  Iters 76158  Lr 0.009998977067939977  Layers 5  Dropout 0.3449627056062375  Layer Size 75\n",
      "======== Trial 46 of 59 =========\n",
      "Batch size 99  Iters 27849  Lr 0.0074566554175397246  Layers 5  Dropout 0.18815258256630168  Layer Size 54\n",
      "======== Trial 47 of 59 =========\n",
      "Batch size 74  Iters 58290  Lr 0.0029407672122311905  Layers 4  Dropout 0.3531949755525855  Layer Size 121\n",
      "======== Trial 48 of 59 =========\n",
      "Batch size 51  Iters 49042  Lr 0.0011292340430233967  Layers 2  Dropout 0.37164237947269385  Layer Size 25\n",
      "======== Trial 49 of 59 =========\n",
      "Batch size 76  Iters 79008  Lr 0.008509254619778557  Layers 5  Dropout 0.16243105242746467  Layer Size 110\n",
      "======== Trial 50 of 59 =========\n",
      "Batch size 60  Iters 90691  Lr 0.009027402384193148  Layers 3  Dropout 0.3020097696187659  Layer Size 42\n",
      "======== Trial 51 of 59 =========\n",
      "Batch size 117  Iters 85450  Lr 0.006912196066155111  Layers 2  Dropout 0.10302393717693095  Layer Size 43\n",
      "======== Trial 52 of 59 =========\n",
      "Batch size 50  Iters 72174  Lr 0.003193344489642379  Layers 2  Dropout 0.15203899615641905  Layer Size 57\n",
      "======== Trial 53 of 59 =========\n",
      "Batch size 106  Iters 98577  Lr 0.0006292263386598998  Layers 5  Dropout 0.22603130329896598  Layer Size 41\n",
      "======== Trial 54 of 59 =========\n",
      "Batch size 72  Iters 81849  Lr 0.007688762951517617  Layers 2  Dropout 0.1085447448107915  Layer Size 94\n",
      "======== Trial 55 of 59 =========\n",
      "Batch size 54  Iters 23812  Lr 0.0017880385961088448  Layers 5  Dropout 0.31164306570701894  Layer Size 45\n",
      "======== Trial 56 of 59 =========\n",
      "Batch size 82  Iters 58105  Lr 0.003778389269811602  Layers 2  Dropout 0.24216991542876856  Layer Size 75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Trial 57 of 59 =========\n",
      "Batch size 84  Iters 82067  Lr 0.008625168192042103  Layers 4  Dropout 0.39576504862312833  Layer Size 113\n",
      "======== Trial 58 of 59 =========\n",
      "Batch size 76  Iters 20404  Lr 0.003991116597253959  Layers 3  Dropout 0.28379441335774314  Layer Size 66\n",
      "======== Trial 59 of 59 =========\n",
      "Batch size 70  Iters 77967  Lr 0.0016992870041747721  Layers 5  Dropout 0.3926752310771079  Layer Size 114\n"
     ]
    }
   ],
   "source": [
    "# First establish ground truth treatment effect:\n",
    "N = 5000000\n",
    "Z, x, y, Y1, Y0 = generate_data(N, seed=0)\n",
    "true_psi = (Y1-Y0).mean()\n",
    "\n",
    "\n",
    "# Set some params\n",
    "N = 10000\n",
    "seed = 0\n",
    "num_tuning_trials = 60\n",
    "\n",
    "# data generation:\n",
    "z, x, y, _, _ = generate_data(N, 0)\n",
    "x = torch.tensor(x).type(torch.float32)\n",
    "z = torch.tensor(z).type(torch.float32)\n",
    "y = torch.tensor(y).type(torch.float32)\n",
    "    \n",
    "tuner = Tuner(x=x,y=y,z=z,trials=num_tuning_trials)\n",
    "tuning_history, best_q, best_g = tuner.tune()\n",
    "\n",
    "total_losses = np.asarray(tuning_history['val_loss_g']) + np.asarray(tuning_history['val_loss_q'])\n",
    "best_index = np.argmin(total_losses)\n",
    "\n",
    "best_params = {}\n",
    "for key in tuning_history.keys():\n",
    "    best_params[key] = tuning_history[key][best_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4090404",
   "metadata": {},
   "source": [
    "## 6. Run Simulation\n",
    "\n",
    "Now we have the best hyperparameters, we will run the simulations accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9709b014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 50, 'layers': 4, 'dropout': 0.1556149275710231, 'layer_size': 110, 'lr': 0.00835623860287358, 'iters': 77539, 'train_loss_q': 0.5121873617172241, 'train_loss_g': 0.675466001033783, 'val_loss_q': 0.5054511427879333, 'val_loss_g': 0.6706466674804688}\n",
      "=====================RUN 0===================\n",
      "=====================RUN 1===================\n",
      "=====================RUN 2===================\n",
      "=====================RUN 3===================\n",
      "=====================RUN 4===================\n",
      "=====================RUN 5===================\n",
      "=====================RUN 6===================\n",
      "=====================RUN 7===================\n",
      "=====================RUN 8===================\n",
      "=====================RUN 9===================\n",
      "=====================RUN 10===================\n",
      "=====================RUN 11===================\n",
      "=====================RUN 12===================\n",
      "=====================RUN 13===================\n",
      "=====================RUN 14===================\n",
      "=====================RUN 15===================\n",
      "=====================RUN 16===================\n",
      "=====================RUN 17===================\n",
      "=====================RUN 18===================\n",
      "=====================RUN 19===================\n",
      "=====================RUN 20===================\n",
      "=====================RUN 21===================\n",
      "=====================RUN 22===================\n",
      "=====================RUN 23===================\n",
      "=====================RUN 24===================\n",
      "=====================RUN 25===================\n",
      "=====================RUN 26===================\n",
      "=====================RUN 27===================\n",
      "=====================RUN 28===================\n",
      "=====================RUN 29===================\n",
      "=====================RUN 30===================\n",
      "=====================RUN 31===================\n",
      "=====================RUN 32===================\n",
      "=====================RUN 33===================\n",
      "=====================RUN 34===================\n",
      "=====================RUN 35===================\n",
      "=====================RUN 36===================\n",
      "=====================RUN 37===================\n",
      "=====================RUN 38===================\n",
      "=====================RUN 39===================\n",
      "=====================RUN 40===================\n",
      "=====================RUN 41===================\n",
      "=====================RUN 42===================\n",
      "=====================RUN 43===================\n",
      "=====================RUN 44===================\n",
      "=====================RUN 45===================\n",
      "=====================RUN 46===================\n",
      "=====================RUN 47===================\n",
      "=====================RUN 48===================\n",
      "=====================RUN 49===================\n",
      "=====================RUN 50===================\n",
      "=====================RUN 51===================\n",
      "=====================RUN 52===================\n",
      "=====================RUN 53===================\n",
      "=====================RUN 54===================\n",
      "=====================RUN 55===================\n",
      "=====================RUN 56===================\n",
      "=====================RUN 57===================\n",
      "=====================RUN 58===================\n",
      "=====================RUN 59===================\n"
     ]
    }
   ],
   "source": [
    "print(best_params)\n",
    "N = 10000\n",
    "seed = 0\n",
    "num_runs = 60\n",
    "\n",
    "output_type_Q = 'categorical'\n",
    "output_size_Q = 1\n",
    "output_type_G = 'categorical'\n",
    "output_size_G = 1\n",
    "input_size_Q = z.shape[-1] + 1  # we will concatenate the treatment var inside the qnet class\n",
    "input_size_G = z.shape[-1]\n",
    "layers = best_params['layers']\n",
    "dropout = best_params['dropout']\n",
    "layer_size = best_params['layer_size']\n",
    "iters = best_params['iters']\n",
    "lr = best_params['lr']\n",
    "batch_size = best_params['batch_size']\n",
    "\n",
    "estimates_naive = []\n",
    "estimates_upd = []\n",
    "for i in range(num_runs):\n",
    "    print('=====================RUN {}==================='.format(i))\n",
    "    seed += 1\n",
    "    # data generation:\n",
    "    z, x, y, _, _ = generate_data(N, seed=seed)\n",
    "    x = torch.tensor(x).type(torch.float32)\n",
    "    z = torch.tensor(z).type(torch.float32)\n",
    "    y = torch.tensor(y).type(torch.float32)\n",
    "    x_int1 = torch.ones_like(x)  # this is the 'intervention data'\n",
    "    x_int0 = torch.zeros_like(x)    \n",
    "\n",
    "    qnet = QNet(input_size=input_size_Q, num_layers=layers,\n",
    "                          layers_size=layer_size, output_size=output_size_Q,\n",
    "                         output_type=output_type_Q, dropout=dropout)\n",
    "\n",
    "    gnet = GNet(input_size=input_size_G, num_layers=layers,\n",
    "                          layers_size=layer_size, output_size=output_size_G,\n",
    "                         output_type=output_type_G, dropout=dropout)\n",
    "\n",
    "\n",
    "    trainer = Trainer(qnet=qnet, gnet=gnet, iterations=iters, outcome_type=output_type_Q,\n",
    "                      batch_size=batch_size, test_iter=500, lr=lr)\n",
    "\n",
    "    train_loss_q_, train_loss_g_, val_loss_q_, val_loss_g_ = trainer.train(x, y, z)\n",
    "\n",
    "    _, _, x_pred, y_pred = trainer.test(x, y, z)\n",
    "    x_pred, y_pred = x_pred.detach().numpy(), y_pred.detach().numpy()\n",
    "    \n",
    "    _, _, _, Q10 = trainer.test(x, y, z)\n",
    "    _, _, G10, Q1 = trainer.test(x_int1, y, z)\n",
    "    _, _, _, Q0 = trainer.test(x_int0, y, z)\n",
    "    Q10 = Q10.detach().numpy()\n",
    "    Q1 = Q1.detach().numpy()\n",
    "    Q0 = Q0.detach().numpy()\n",
    "    biased_psi = (Q1-Q0).mean()\n",
    "\n",
    "    G10 = np.clip(G10.detach().numpy(), a_min=0.01, a_max=0.99)\n",
    "\n",
    "    x_ = x.detach().numpy()\n",
    "    y_ = y.detach().numpy()\n",
    "\n",
    "    H1 = x_/(G10)\n",
    "    H0 = (1-x_) / (1 - G10)\n",
    "\n",
    "    eps0, eps1 = sm.GLM(y_, np.concatenate([H0, H1], 1), offset=inv_sigm(Q10[:,0]),\n",
    "                        family=sm.families.Binomial()).fit().params\n",
    "\n",
    "    Q0_star = sigm(inv_sigm(Q0) + eps0 * H0)\n",
    "    Q1_star = sigm(inv_sigm(Q1) + eps1 * H1)\n",
    "\n",
    "    upd_psi = (Q1_star - Q0_star).mean()\n",
    "\n",
    "    estimates_naive.append(biased_psi)\n",
    "    estimates_upd.append(upd_psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e37c9e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True psi:  0.1956508\n",
      "naive psi:  0.20950554  relative bias: 7.081362873422857 %\n",
      "updated TMLE psi:  0.20527059  relative bias: 4.916815263714599 %\n",
      "Reduction in bias: 2.164547609708258 %\n"
     ]
    }
   ],
   "source": [
    "estimates_upd = np.asarray(estimates_upd)\n",
    "estimates_naive = np.asarray(estimates_naive)\n",
    "\n",
    "print('True psi: ', true_psi)\n",
    "print('naive psi: ', estimates_naive.mean(), ' relative bias:',\n",
    "      (estimates_naive.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('updated TMLE psi: ', estimates_upd.mean(), ' relative bias:',\n",
    "      (estimates_upd.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('Reduction in bias:', np.abs(estimates_naive.mean() - true_psi)/true_psi * 100 - \n",
    "     np.abs(estimates_upd.mean() - true_psi)/true_psi * 100, '%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4a92660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive psi var: 0.0018162876\n",
      "updated psi var: 0.0008233825\n",
      "Average of reductions: 6.0977077 %\n"
     ]
    }
   ],
   "source": [
    "# This takes the reduction in relative bias for each simulation first, then takes an average\n",
    "# (Owing to the nonlinearity of the ||x|| function, this gives different results which are\n",
    "# worth considering.)\n",
    "print('naive psi var:', estimates_naive.var())\n",
    "print('updated psi var:', estimates_upd.var())\n",
    "errors_naive = (estimates_naive - true_psi)/true_psi *100\n",
    "errors_updated = (estimates_upd - true_psi)/true_psi *100\n",
    "diff_errors = np.abs(errors_naive) - np.abs(errors_updated)\n",
    "print('Average of reductions:', diff_errors.mean(), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef13a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cec5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729b17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb6bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf78c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234013c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
