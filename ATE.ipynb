{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c43e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da050a0d",
   "metadata": {},
   "source": [
    "## Summary of Results:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f69909e",
   "metadata": {},
   "source": [
    "## Problem Setup:\n",
    "\n",
    "This example is taken from https://arxiv.org/abs/2107.00681 by Hines, Dukes, Diaz-Ordaz, and Vansteelandt (2021) and the empirical evaluation follows https://onlinelibrary.wiley.com/doi/full/10.1002/sim.7628 by Miguel Angel Luque-Fernandez, Michael Schomaker, Bernard Rachet, Mireille E. Schnitzer (2018).\n",
    "\n",
    "$\\psi(P_0) = \\mathbb{E}[\\mathbb{E}[Y|X=1, Z]] - \\mathbb{E}[\\mathbb{E}[Y|X=0, Z]]$  is our target estimand - it is the Average Treatment Effect, i.e. the difference in the average outcome $Y$ under treatment $X=1$ and the average outcome $Y$ under no treatment. $X=0$. This can be broken down into two potential outcome goals. So let's simplify it by considering the outcome under treatment:\n",
    "\n",
    "i.e. $$\\psi(P_0) = \\int y f(y|1,z)f(z) dy dz$$\n",
    "\n",
    "Which is the statistical estimand of the mean of $Y^1$ (the quantity is identifiable).\n",
    "\n",
    "If we assume the distribution from which the densities $f$ derive has been perturbed by a point mass at $(\\tilde x, \\tilde y, \\tilde z)$ then:\n",
    "\n",
    "$$\\psi(P_0) = \\int y f_t(y|1,z)f_t(z) dy dz$$\n",
    "\n",
    "$$ = \\int y \\frac{f_t(y,1,z)f_t(z)}{f_t(1,z)} dy dz$$\n",
    "\n",
    "Under the 'parametric submodel' $$P_t = t\\delta_x(\\tilde x) + (1-t)P_o$$ and where $\\delta_x(\\tilde x)$ denotes the Dirac delta function s.t. it gives the density of a point mass at $\\tilde x$, is zero everywhere else, and integrates to 1.\n",
    "\n",
    "For our densities we therefore have that:\n",
    "\n",
    "$$f_t(x,y,z) =  t \\delta_{x,y,z}(\\tilde x, \\tilde y, \\tilde z) +  (1-t)f(x,y,z)$$\n",
    "\n",
    "and, equivalently for $f_t(z)$ and $f_t(1,z)$.\n",
    "\n",
    "Following the derivation in the paper, the efficient influence function is given as:\n",
    "\n",
    "$$ \\phi_1 = \\left . \\frac{d\\psi (P_t)}{dt} \\right \\vert_{t=0} = \\frac{\\mathbb{1}_1(\\tilde x)}{\\pi(\\tilde z,  P)}\\{\\tilde y - \\mathbb{E}(Y|X=1, Z=z)\\} + \\mathbb{E}(Y|X=1, Z=z) - \\psi(P_0)$$\n",
    "\n",
    "This can be combined with the outcome under no treatment $x=0$ to derive the influence curve for the average treatment effect:\n",
    "\n",
    "$$\\phi_{ATE}=  \\phi_1 - \\phi_0 - \\psi_{ATE}$$\n",
    "\n",
    "In the expression above, there are a number of 'ground truth' quantities which we need to estimate. Using plug-in estimators we have:\n",
    "\n",
    "\n",
    "$$ \\phi_1(\\hat P_n) = \\frac{\\mathbb{1}_1(\\tilde x)}{\\pi(\\tilde z,  \\hat P)}\\{\\tilde y - m_1(Z, \\hat P_n) \\} + m_1(Z, \\hat P_n) - \\psi(\\hat P_n)$$\n",
    "\n",
    "i.e. we have estimators for the propensity score $\\pi$, and estimators for the expected outcome $m_1 \\approx \\mathbb{E}[Y|X=1, Z=z]$\n",
    "\n",
    "Finally, we want to update out estimate. This can be done a number of ways. E.g.:\n",
    "\n",
    "$$ \\psi(P_0) \\approx \\psi(\\hat P_n) + \\frac{1}{n}\\sum_{i=1}^n\\phi_1(O_i, \\hat P_n)$$\n",
    "\n",
    "Alternatively, we can follow the 'targeted learning' approach and tune the initial estimator for the outcome:\n",
    "\n",
    "$$m1^* = m1 + \\hat \\epsilon \\left ( \\frac{1}{\\pi(Z, \\hat P_n)} \\right ) $$\n",
    "\n",
    "which solves the influence curve to be equal to zero (i.e. the efficient influence curve):\n",
    "\n",
    "$$ 0 = \\frac{1}{n}\\sum_i^n \\frac{\\mathbb{1}_1(X_i)}{\\pi(Z_i, \\hat P_n)}\\left \\{ Y_i - m_1 - \\hat \\epsilon \\frac{1}{\\pi (Z_i, \\hat P_n)}\\right \\}$$\n",
    "\n",
    "This latter approach is the one used in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292cabfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigm(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def inv_sigm(x):\n",
    "    return np.log(x/(1-x))\n",
    "\n",
    "def generate_data(N, seed):\n",
    "    np.random.seed(seed=seed)\n",
    "    z1 = np.random.binomial(1, 0.5, (N,1))\n",
    "    z2 = np.random.binomial(1, 0.65, (N,1))\n",
    "    z3 = np.round(np.random.uniform(0, 4, (N,1)),3)\n",
    "    z4 = np.round(np.random.uniform(0, 5, (N,1)),3)\n",
    "    X = np.random.binomial(1, sigm(-0.4 + 0.2*z2 + 0.15*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y1 = np.random.binomial(1, sigm(-1 + 1 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y0 = np.random.binomial(1, sigm(-1 + 0 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    Y = Y1 * X + Y0 * (1-X)\n",
    "    Z = np.concatenate([z1,z2,z3,z4],1)\n",
    "    return Z, X, Y, Y1, Y0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea1a40",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 1. Misspecified Outcome and Treatment Models\n",
    "\n",
    "This first round demonstrates that targeted learning DOESNT work when both outcome and treatment models are misspecified. Specifically, the DGP contains interactions z2*z4 which we do not pre-specify in the logistic equation models.\n",
    "\n",
    "Note that each simulation will use the same random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a411d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First establish ground truth treatment effect:\n",
    "N = 5000000\n",
    "Z, x, y, Y1, Y0 = generate_data(N, seed=0)\n",
    "true_psi = (Y1-Y0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9418adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some params\n",
    "num_runs = 1000\n",
    "N = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1bf4776",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now start simulations\n",
    "estimates_naive = []\n",
    "estimates_upd = []\n",
    "seed = 0\n",
    "for i in range(num_runs):\n",
    "    seed += 1\n",
    "    Z, x, y, Y1, Y0 = generate_data(N, seed)\n",
    "    x_int1 = np.ones_like(x)  # this is the 'intervention data'\n",
    "    x_int0 = np.zeros_like(x)  # this is the 'intervention data'\n",
    "    \n",
    "    QZ = Z # misspecify the outcome model by not including interaction term\n",
    "    GZ = Z[:, :-1]  # misspecify the treatment model in the same way\n",
    "    \n",
    "    Q = LogisticRegression().fit(np.concatenate([x,QZ], 1), y[:,0])\n",
    "    Q1 = np.clip(Q.predict_proba((np.concatenate([x_int1, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "    Q0 = np.clip(Q.predict_proba((np.concatenate([x_int0, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "    Q10 = np.clip(Q.predict_proba((np.concatenate([x, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "\n",
    "    biased_psi = (Q1-Q0).mean()\n",
    "\n",
    "    G = LogisticRegression().fit(GZ, x[:,0])\n",
    "    G10 = np.clip(G.predict_proba(GZ), a_min=0, a_max=1)[:, 1:]\n",
    "\n",
    "    H1 = x/(G10)\n",
    "    H0 = (1-x) / (1 - G10)\n",
    "\n",
    "    eps0, eps1 = sm.GLM(y, np.concatenate([H0, H1], 1), offset=inv_sigm(Q10[:,0]),\n",
    "                        family=sm.families.Binomial()).fit().params\n",
    "\n",
    "    Q0_star = sigm(inv_sigm(Q0) + eps0 * H0)\n",
    "    Q1_star = sigm(inv_sigm(Q1) + eps1 * H1)\n",
    "    \n",
    "    upd_psi = (Q1_star - Q0_star).mean()\n",
    "    estimates_naive.append(biased_psi)\n",
    "    estimates_upd.append(upd_psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc5076f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for misspecified outcome and treatment models....\n",
      "True psi:  0.1956508\n",
      "naive psi:  0.19777339178968617  relative bias: 1.0848878663854973 %\n",
      "updated TMLE psi:  0.19802285261944388  relative bias: 1.2123909636167451 %\n",
      "Reduction in bias: -0.12750309723124786 %\n"
     ]
    }
   ],
   "source": [
    "print('Results for misspecified outcome and treatment models....')\n",
    "\n",
    "estimates_naive = np.asarray(estimates_naive)\n",
    "estimates_upd = np.asarray(estimates_upd)\n",
    "print('True psi: ', true_psi)\n",
    "print('naive psi: ', estimates_naive.mean(), ' relative bias:',\n",
    "      (estimates_naive.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('updated TMLE psi: ', estimates_upd.mean(), ' relative bias:',\n",
    "      (estimates_upd.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('Reduction in bias:', np.abs(estimates_naive.mean() - true_psi)/true_psi * 100 - \n",
    "     np.abs(estimates_upd.mean() - true_psi)/true_psi * 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f1b55b",
   "metadata": {},
   "source": [
    "As can be seen in the boxplot above, no bias reduction was achieved. In fact, the update made things worse. This is because the models for *both* the outcome and treatment were misspecified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbdb694",
   "metadata": {},
   "source": [
    "## 2. Misspecified Outcome Model, Correct Treatment Model\n",
    "\n",
    "Let's create some features accounting for the interaction terms z2*z4 and feed these into the model for the propensity score, thus resulting in misspecification of the outcome model only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c68f143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimates_naive2 = []\n",
    "estimates_upd2 = []\n",
    "seed = 0\n",
    "for i in range(num_runs):\n",
    "    seed += 1\n",
    "    Z, x, y, Y1, Y0 = generate_data(N, seed)\n",
    "    Z_int = Z[:, 1:2] * Z[:,3:4]\n",
    "    Z = np.concatenate([Z, Z_int], 1)\n",
    "    x_int1 = np.ones_like(x)  # this is the 'intervention data'\n",
    "    x_int0 = np.zeros_like(x)  # this is the 'intervention data'\n",
    "    \n",
    "    # Reminder of DGP:\n",
    "    # X = np.random.binomial(1, sigm(-0.4 + 0.2*z2 + 0.15*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    # Y1 = np.random.binomial(1, sigm(-1 + 1 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    \n",
    "    QZ = Z[:,:-1]  # misspecify the outcome model by not including interaction term\n",
    "    GZ = Z[:, 1:]  # correctly specify the treatment model\n",
    "\n",
    "    Q = LogisticRegression().fit(np.concatenate([x,QZ], 1), y[:,0])\n",
    "    Q1 = np.clip(Q.predict_proba((np.concatenate([x_int1, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "    Q0 = np.clip(Q.predict_proba((np.concatenate([x_int0, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "    Q10 = np.clip(Q.predict_proba((np.concatenate([x, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "\n",
    "    biased_psi = (Q1-Q0).mean()\n",
    "\n",
    "    G = LogisticRegression().fit(GZ, x[:,0])\n",
    "    G10 = np.clip(G.predict_proba(GZ), a_min=0, a_max=1)[:, 1:]\n",
    "\n",
    "    H1 = x/(G10)\n",
    "    H0 = (1-x) / (1 - G10)\n",
    "\n",
    "    eps0, eps1 = sm.GLM(y, np.concatenate([H0, H1], 1), offset=inv_sigm(Q10[:,0]),\n",
    "                        family=sm.families.Binomial()).fit().params\n",
    "\n",
    "    Q0_star = sigm(inv_sigm(Q0) + eps0 * H0)\n",
    "    Q1_star = sigm(inv_sigm(Q1) + eps1 * H1)\n",
    "\n",
    "    upd_psi = (Q1_star - Q0_star).mean()\n",
    "\n",
    "    estimates_naive2.append(biased_psi)\n",
    "    estimates_upd2.append(upd_psi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41833ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for misspecified outcome model and correctly specified treatment model....\n",
      "True psi:  0.1956508\n",
      "naive psi:  0.19777339178968617  relative bias: 1.0848878663854973 %\n",
      "updated TMLE psi:  0.19713150698900234  relative bias: 0.7568111088747517 %\n",
      "Reduction in bias: 0.3280767575107456 %\n"
     ]
    }
   ],
   "source": [
    "print('Results for misspecified outcome model and correctly specified treatment model....')\n",
    "\n",
    "estimates_naive2 = np.asarray(estimates_naive2)\n",
    "estimates_upd2 = np.asarray(estimates_upd2)\n",
    "print('True psi: ', true_psi)\n",
    "print('naive psi: ', estimates_naive2.mean(), ' relative bias:',\n",
    "      (estimates_naive2.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('updated TMLE psi: ', estimates_upd2.mean(), ' relative bias:',\n",
    "      (estimates_upd2.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('Reduction in bias:', np.abs(estimates_naive2.mean() - true_psi)/true_psi * 100 - \n",
    "     np.abs(estimates_upd2.mean() - true_psi)/true_psi * 100, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed7cdd",
   "metadata": {},
   "source": [
    "The results above show that even if one of the models is misspecified (in the previous case it was the outcome model), we still get a reduction in bias - this is the double robustness property.  \n",
    "\n",
    "## 3. Outcome and Treatment Models Correctly Specified\n",
    "\n",
    "Now, let's try again but correctly specify *both* models, whilst comparing against the correctly specified outcome model without the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8a591c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates_naive3 = []\n",
    "estimates_upd3 = []\n",
    "seed = 0\n",
    "for i in range(num_runs):\n",
    "    seed += 1\n",
    "    Z, x, y, Y1, Y0 = generate_data(N, seed)\n",
    "    Z_int = Z[:, 1:2] * Z[:,3:4]\n",
    "    Z = np.concatenate([Z, Z_int], 1)\n",
    "    x_int1 = np.ones_like(x)  # this is the 'intervention data'\n",
    "    x_int0 = np.zeros_like(x)  # this is the 'intervention data'\n",
    "    \n",
    "    # Reminder of DGP:\n",
    "    # X = np.random.binomial(1, sigm(-0.4 + 0.2*z2 + 0.15*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    # Y1 = np.random.binomial(1, sigm(-1 + 1 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "           \n",
    "    QZ = Z # outcome model correctly specified\n",
    "    GZ = Z[:, 1:]  # propensity model correctly specified\n",
    "\n",
    "    Q = LogisticRegression().fit(np.concatenate([x,QZ], 1), y[:,0])\n",
    "    Q1 = np.clip(Q.predict_proba((np.concatenate([x_int1, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "    Q0 = np.clip(Q.predict_proba((np.concatenate([x_int0, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "    Q10 = np.clip(Q.predict_proba((np.concatenate([x, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "    # note the new reference point is the correctly specified outcome model on its own:\n",
    "    biased_psi = (Q1-Q0).mean() \n",
    "    \n",
    "    G = LogisticRegression().fit(GZ, x[:,0])\n",
    "    G10 = np.clip(G.predict_proba(GZ), a_min=0, a_max=1)[:, 1:]\n",
    "\n",
    "    H1 = x/(G10)\n",
    "    H0 = (1-x) / (1 - G10)\n",
    "\n",
    "    eps0, eps1 = sm.GLM(y, np.concatenate([H0, H1], 1), offset=inv_sigm(Q10[:,0]),\n",
    "                        family=sm.families.Binomial()).fit().params\n",
    "\n",
    "    Q0_star = sigm(inv_sigm(Q0) + eps0 * H0)\n",
    "    Q1_star = sigm(inv_sigm(Q1) + eps1 * H1)\n",
    "\n",
    "    upd_psi = (Q1_star - Q0_star).mean()\n",
    "\n",
    "    estimates_naive3.append(biased_psi)\n",
    "    estimates_upd3.append(upd_psi)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3bd35c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for correctly specified outcome and treatment models....\n",
      "Note that the naive model is now actually the correct specified outcome model.\n",
      "True psi:  0.1956508\n",
      "naive psi:  0.19537547440087769  relative bias: -0.14072296107264984 %\n",
      "updated TMLE psi:  0.19555759236038875  relative bias: -0.04763979478298308 %\n",
      "Reduction in bias: 0.09308316628966676 %\n"
     ]
    }
   ],
   "source": [
    "print('Results for correctly specified outcome and treatment models....')\n",
    "print('Note that the naive model is now actually the correct specified outcome model.')\n",
    "estimates_naive3 = np.asarray(estimates_naive3)\n",
    "estimates_upd3 = np.asarray(estimates_upd3)\n",
    "print('True psi: ', true_psi)\n",
    "print('naive psi: ', estimates_naive3.mean(), ' relative bias:',\n",
    "      (estimates_naive3.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('updated TMLE psi: ', estimates_upd3.mean(), ' relative bias:',\n",
    "      (estimates_upd3.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('Reduction in bias:', np.abs(estimates_naive3.mean() - true_psi)/true_psi * 100 - \n",
    "     np.abs(estimates_upd3.mean() - true_psi)/true_psi * 100, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35468fff",
   "metadata": {},
   "source": [
    "As can be seen, the reduction in bias is not much because we were using the correct outcome model already.\n",
    "\n",
    "## 4. Outcome Model Correctly Specified, Treatment Model Misspecified \n",
    "Let's see what happens when the outcome model is correctly specified, but the treatment model is not, still comparing against the naive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2052b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "estimates_naive4 = []\n",
    "estimates_upd4 = []\n",
    "seed = 0\n",
    "for i in range(num_runs):\n",
    "    seed += 1\n",
    "    Z, x, y, Y1, Y0 = generate_data(N, seed)\n",
    "    Z_int = Z[:, 1:2] * Z[:,3:4]\n",
    "    Z = np.concatenate([Z, Z_int], 1)\n",
    "    x_int1 = np.ones_like(x)  # this is the 'intervention data'\n",
    "    x_int0 = np.zeros_like(x)  # this is the 'intervention data'\n",
    "    \n",
    "    # Reminder of DGP:\n",
    "    # X = np.random.binomial(1, sigm(-0.4 + 0.2*z2 + 0.15*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    # Y1 = np.random.binomial(1, sigm(-1 + 1 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    \n",
    "    QZ = Z # outcome correctly specified\n",
    "    GZ = Z[:, :-1]  # propensity model misspecified\n",
    "\n",
    "    Q = LogisticRegression().fit(np.concatenate([x,QZ], 1), y[:,0])\n",
    "    Q1 = np.clip(Q.predict_proba((np.concatenate([x_int1, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "    Q0 = np.clip(Q.predict_proba((np.concatenate([x_int0, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "    Q10 = np.clip(Q.predict_proba((np.concatenate([x, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "\n",
    "    biased_psi = (Q1-Q0).mean()\n",
    "\n",
    "    G = LogisticRegression().fit(GZ, x[:,0])\n",
    "    G10 = np.clip(G.predict_proba(GZ), a_min=0, a_max=1)[:, 1:]\n",
    "\n",
    "    H1 = x/(G10)\n",
    "    H0 = (1-x) / (1 - G10)\n",
    "\n",
    "    eps0, eps1 = sm.GLM(y, np.concatenate([H0, H1], 1), offset=inv_sigm(Q10[:,0]),\n",
    "                        family=sm.families.Binomial()).fit().params\n",
    "\n",
    "    Q0_star = sigm(inv_sigm(Q0) + eps0 * H0)\n",
    "    Q1_star = sigm(inv_sigm(Q1) + eps1 * H1)\n",
    "\n",
    "    upd_psi = (Q1_star - Q0_star).mean()\n",
    "\n",
    "    estimates_naive4.append(biased_psi)\n",
    "    estimates_upd4.append(upd_psi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e5a73b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for correctly specified outcome model and misspecified treatment model....\n",
      "Note that the naive model is now actually the correct specified outcome model.\n",
      "True psi:  0.1956508\n",
      "naive psi:  0.19537547440087769  relative bias: -0.14072296107264984 %\n",
      "updated TMLE psi:  0.1955532743796033  relative bias: -0.04984677823792245 %\n",
      "Reduction in bias: 0.0908761828347274 %\n"
     ]
    }
   ],
   "source": [
    "print('Results for correctly specified outcome model and misspecified treatment model....')\n",
    "print('Note that the naive model is now actually the correct specified outcome model.')\n",
    "estimates_naive4 = np.asarray(estimates_naive4)\n",
    "estimates_upd4 = np.asarray(estimates_upd4)\n",
    "print('True psi: ', true_psi)\n",
    "print('naive psi: ', estimates_naive4.mean(), ' relative bias:',\n",
    "      (estimates_naive4.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('updated TMLE psi: ', estimates_upd4.mean(), ' relative bias:',\n",
    "      (estimates_upd4.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('Reduction in bias:', np.abs(estimates_naive4.mean() - true_psi)/true_psi * 100 - \n",
    "     np.abs(estimates_upd4.mean() - true_psi)/true_psi * 100, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913dcd5d",
   "metadata": {},
   "source": [
    "Once again, not much bias reduction is achieved because we already started with the correct outcome model.\n",
    "\n",
    "Note that in practice, we need machine learning algorithms or ensembles (e.g. see super learning https://pubmed.ncbi.nlm.nih.gov/17910531/) which specify a family of models such that the true model is likely to be within that family. Otherwise, we will obviously be unable to specify the correct model without knowing it *a priori*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceea119",
   "metadata": {},
   "source": [
    "## 5. Correct Specification, One-Step update Process\n",
    "Now, we re-run the last examples, but using the one-step update process to de-bias our initial estimate, instead of estimating estimating epsilon etc. This is therefore different from the typical targeted learning procedure. Let's start with the situation where both outcome and treatment models are correctly specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d269681",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates_naive5 = []\n",
    "estimates_upd5 = []\n",
    "seed = 0\n",
    "for i in range(num_runs):\n",
    "    seed += 1\n",
    "    Z, x, y, Y1, Y0 = generate_data(N, seed)\n",
    "    Z_int = Z[:, 1:2] * Z[:,3:4]\n",
    "    Z = np.concatenate([Z, Z_int], 1)\n",
    "    x_int1 = np.ones_like(x)  # this is the 'intervention data'\n",
    "    x_int0 = np.zeros_like(x)  # this is the 'intervention data'\n",
    "\n",
    "    # Reminder of DGP:\n",
    "    # X = np.random.binomial(1, sigm(-0.4 + 0.2*z2 + 0.15*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    # Y1 = np.random.binomial(1, sigm(-1 + 1 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "       \n",
    "    QZ = Z # outcome correctly specified\n",
    "    GZ = Z[:, 1:]  # propensity model correctly specified\n",
    "\n",
    "    Q = LogisticRegression().fit(np.concatenate([x,QZ], 1), y[:,0])\n",
    "    Q1 = np.clip(Q.predict_proba((np.concatenate([x_int1, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "    Q0 = np.clip(Q.predict_proba((np.concatenate([x_int0, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "    Q10 = np.clip(Q.predict_proba((np.concatenate([x, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "    \n",
    "    biased_psi = (Q1-Q0).mean()\n",
    "\n",
    "    G = LogisticRegression().fit(GZ, x[:,0])\n",
    "    G10 = np.clip(G.predict_proba(GZ), a_min=0, a_max=1)[:, 1:]\n",
    "\n",
    "    H1 = 1/(G10)\n",
    "    H0 = 1 / (1 - G10)\n",
    "    \n",
    "    D1 = x * H1 * (y - Q1) + Q1 - Q1.mean()\n",
    "    D0 = (1 - x) * H0 * (y - Q0) + Q0 - Q0.mean()\n",
    "\n",
    "    Q1_star = Q1 + D1\n",
    "    Q0_star = Q0 + D0\n",
    "\n",
    "    upd_psi = (Q1_star - Q0_star).mean()\n",
    "\n",
    "    estimates_naive5.append(biased_psi)\n",
    "    estimates_upd5.append(upd_psi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2ef24f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for correctly specified outcome model and treatment models using the one-step update process....\n",
      "Note that the naive model is now actually the correct specified outcome model.\n",
      "True psi:  0.1956508\n",
      "naive psi:  0.19537547440087769  relative bias: -0.14072296107264984 %\n",
      "updated TMLE psi:  0.19580529903385285  relative bias: 0.07896672738002583 %\n",
      "Reduction in bias: 0.06175623369262401 %\n"
     ]
    }
   ],
   "source": [
    "print('Results for correctly specified outcome model and treatment models using the one-step update process....')\n",
    "print('Note that the naive model is now actually the correct specified outcome model.')\n",
    "estimates_naive5 = np.asarray(estimates_naive5)\n",
    "estimates_upd5 = np.asarray(estimates_upd5)\n",
    "print('True psi: ', true_psi)\n",
    "print('naive psi: ', estimates_naive5.mean(), ' relative bias:',\n",
    "      (estimates_naive5.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('updated TMLE psi: ', estimates_upd5.mean(), ' relative bias:',\n",
    "      (estimates_upd5.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('Reduction in bias:', np.abs(estimates_naive5.mean() - true_psi)/true_psi * 100 - \n",
    "     np.abs(estimates_upd5.mean() - true_psi)/true_psi * 100, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb6f530",
   "metadata": {},
   "source": [
    "It can be seen that the results are not quite as good as they were in example number 3 above when using the targeted learning update procedure. Nonetheless, some bias reduction is achieved even when the outcome model is correctly specified to begin with.\n",
    "\n",
    "## 6. Correctly specified treatment model, Misspecified Outcome Model, one-step update\n",
    "\n",
    "Nearly there... to cover all bases, we run the situation with the correctly specified treatment model but misspecified outcome model, with the one-step update process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6353988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates_naive6 = []\n",
    "estimates_upd6 = []\n",
    "seed = 0\n",
    "for i in range(num_runs):\n",
    "    seed += 1\n",
    "    Z, x, y, Y1, Y0 = generate_data(N, seed)\n",
    "    Z_int = Z[:, 1:2] * Z[:,3:4]\n",
    "    Z = np.concatenate([Z, Z_int], 1)\n",
    "    x_int1 = np.ones_like(x)  # this is the 'intervention data'\n",
    "    x_int0 = np.zeros_like(x)  # this is the 'intervention data'\n",
    "\n",
    "    # Reminder of DGP:\n",
    "    # X = np.random.binomial(1, sigm(-0.4 + 0.2*z2 + 0.15*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    # Y1 = np.random.binomial(1, sigm(-1 + 1 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    \n",
    "    QZ = Z[:, :-1]  # Misspecify the outcome model\n",
    "    GZ = Z[:, 1:]  # propensity model correctly specified\n",
    "    \n",
    "    Q = LogisticRegression().fit(np.concatenate([x,QZ], 1), y[:,0])\n",
    "    Q1 = np.clip(Q.predict_proba((np.concatenate([x_int1, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "    Q0 = np.clip(Q.predict_proba((np.concatenate([x_int0, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "    Q10 = np.clip(Q.predict_proba((np.concatenate([x, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "\n",
    "    biased_psi = (Q1-Q0).mean()\n",
    "    \n",
    "    G = LogisticRegression().fit(GZ, x[:,0])\n",
    "    G10 = np.clip(G.predict_proba(GZ), a_min=0, a_max=1)[:, 1:]\n",
    "\n",
    "    H1 = 1/(G10)\n",
    "    H0 = 1 / (1 - G10)\n",
    "    \n",
    "    D1 = x * H1 * (y - Q1) + Q1 - Q1.mean()\n",
    "    D0 = (1 - x) * H0 * (y - Q0) + Q0 - Q0.mean()\n",
    "\n",
    "    Q1_star = Q1 + D1\n",
    "    Q0_star = Q0 + D0\n",
    "\n",
    "    upd_psi = (Q1_star - Q0_star).mean()\n",
    "\n",
    "    estimates_naive6.append(biased_psi)\n",
    "    estimates_upd6.append(upd_psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6523e5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for correctly specified treatment model and misspecified outcome model using the one-step update process....\n",
      "True psi:  0.1956508\n",
      "naive psi:  0.19777339178968617  relative bias: 1.0848878663854973 %\n",
      "updated TMLE psi:  0.19580091277418638  relative bias: 0.07672484558527831 %\n",
      "Reduction in bias: 1.008163020800219 %\n"
     ]
    }
   ],
   "source": [
    "print('Results for correctly specified treatment model and misspecified outcome model using the one-step update process....')\n",
    "\n",
    "estimates_naive6 = np.asarray(estimates_naive6)\n",
    "estimates_upd6 = np.asarray(estimates_upd6)\n",
    "print('True psi: ', true_psi)\n",
    "print('naive psi: ', estimates_naive6.mean(), ' relative bias:',\n",
    "      (estimates_naive6.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('updated TMLE psi: ', estimates_upd6.mean(), ' relative bias:',\n",
    "      (estimates_upd6.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('Reduction in bias:', np.abs(estimates_naive6.mean() - true_psi)/true_psi * 100 - \n",
    "     np.abs(estimates_upd6.mean() - true_psi)/true_psi * 100, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d3017a",
   "metadata": {},
   "source": [
    "Interestingly, there was more bias reduction than in the corresponding example number 2 which used the targeted learning update process.\n",
    "\n",
    "## 7. Correctly specified outcome, misspecified treatment model, one-step update\n",
    "\n",
    "Finally, we consider the case where the treatment model is misspecified but the outcome model is correctly specified, using the one-step update process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0168a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates_naive7 = []\n",
    "estimates_upd7 = []\n",
    "seed = 0\n",
    "for i in range(num_runs):\n",
    "    seed += 1\n",
    "    Z, x, y, Y1, Y0 = generate_data(N, seed)\n",
    "    Z_int = Z[:, 1:2] * Z[:,3:4]\n",
    "    Z = np.concatenate([Z, Z_int], 1)\n",
    "    x_int1 = np.ones_like(x)  # this is the 'intervention data'\n",
    "    x_int0 = np.zeros_like(x)  # this is the 'intervention data'\n",
    "\n",
    "    # Reminder of DGP:\n",
    "    # X = np.random.binomial(1, sigm(-0.4 + 0.2*z2 + 0.15*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    # Y1 = np.random.binomial(1, sigm(-1 + 1 - 0.1*z1 + 0.3*z2 + 0.25*z3 + 0.2*z4 + 0.15*z2*z4), (N,1))\n",
    "    \n",
    "    QZ = Z  # Correctly specify the outcome model\n",
    "    GZ = Z[:, 0:-1]  # propensity model misspecified\n",
    "    \n",
    "    Q = LogisticRegression().fit(np.concatenate([x,QZ], 1), y[:,0])\n",
    "    Q1 = np.clip(Q.predict_proba((np.concatenate([x_int1, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "    Q0 = np.clip(Q.predict_proba((np.concatenate([x_int0, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "    Q10 = np.clip(Q.predict_proba((np.concatenate([x, QZ], 1)))[:, 1:], a_min=0, a_max=1)\n",
    "   \n",
    "    biased_psi = (Q1-Q0).mean()\n",
    "\n",
    "    G = LogisticRegression().fit(GZ, x[:,0])\n",
    "    G10 = np.clip(G.predict_proba(GZ), a_min=0, a_max=1)[:, 1:]\n",
    "\n",
    "    H1 = 1/(G10)\n",
    "    H0 = 1 / (1 - G10)\n",
    "    \n",
    "    D1 = x * H1 * (y - Q1) + Q1 - Q1.mean()\n",
    "    D0 = (1 - x) * H0 * (y - Q0) + Q0 - Q0.mean()\n",
    "\n",
    "    Q1_star = Q1 + D1\n",
    "    Q0_star = Q0 + D0\n",
    "\n",
    "    upd_psi = (Q1_star - Q0_star).mean()\n",
    "\n",
    "    estimates_naive7.append(biased_psi)\n",
    "    estimates_upd7.append(upd_psi)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "299bf7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for misspecified treatment model and correctly specified outcome model using the one-step update process....\n",
      "Note that the naive model is now actually the correct specified outcome model.\n",
      "True psi:  0.1956508\n",
      "naive psi:  0.19537547440087769  relative bias: -0.14072296107264984 %\n",
      "updated TMLE psi:  0.1957941411294404  relative bias: 0.07326375841059382 %\n",
      "Reduction in bias: 0.06745920266205602 %\n"
     ]
    }
   ],
   "source": [
    "print('Results for misspecified treatment model and correctly specified outcome model using the one-step update process....')\n",
    "print('Note that the naive model is now actually the correct specified outcome model.')\n",
    "estimates_naive7 = np.asarray(estimates_naive7)\n",
    "estimates_upd7 = np.asarray(estimates_upd7)\n",
    "print('True psi: ', true_psi)\n",
    "print('naive psi: ', estimates_naive7.mean(), ' relative bias:',\n",
    "      (estimates_naive7.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('updated TMLE psi: ', estimates_upd7.mean(), ' relative bias:',\n",
    "      (estimates_upd7.mean() - true_psi)/true_psi * 100, '%')\n",
    "print('Reduction in bias:', np.abs(estimates_naive7.mean() - true_psi)/true_psi * 100 - \n",
    "     np.abs(estimates_upd7.mean() - true_psi)/true_psi * 100, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c43618",
   "metadata": {},
   "source": [
    "It can be seen that a little bias reduction was achieved, similar in magnitude to that for example number 4 above which used the targeted learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a19b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
