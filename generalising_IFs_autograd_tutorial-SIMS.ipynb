{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451559bc",
   "metadata": {},
   "source": [
    "# Generalising Influence Functions with Autograd\n",
    "\n",
    "This notebook is an implementation of the data splitting estimator for the Shannon Entropy in 'Influence Functions for Machine Learning: Nonparametric Estimators for Entropies, Divergences and Mutual Informations' by Kandasamy et al. 2015 https://arxiv.org/abs/1411.4342\n",
    "\n",
    "The process is the same as theirs except, instead of implementing the analytical form derived in the paper, we use autograd.\n",
    "\n",
    "#### Acknowledgements\n",
    "I'd like to thank Nic Ford, Sina Akbari, and Jalal Etesami for their patience in helping me work through this topic.\n",
    "\n",
    "### Form\n",
    "\n",
    "This work is concerned with functions of the form $$T(p) = \\phi \\left( \\int \\nu(p) d\\nu \\right)$$\n",
    "\n",
    "where $T(p)$ is the target functional we wish to estimate, and $p$ is a density.\n",
    "\n",
    "Notice already that Shannon entropy $-\\int p \\log p$ can be expressed in the same form. There are a couple of ways to do this, but one way is with $\\phi(p) = p$ (i.e. the identity function) and $\\nu(p) = p \\log p$. \n",
    "\n",
    "### Pathwise Derivative\n",
    "\n",
    "The data we collect enables us to estimate $T(p)$ but not the true population quantity $T(q)$. i.e. we have access to $P$ but not to $Q$. We assume that $T(p)$ is 'close enough' and lies on a path to $T(q)$. This allows us to define the pathwise or 'Gateaux' derivative as:\n",
    "\n",
    "$$\n",
    "T'(H; P) = \\left. \\frac{\\partial T(P+tH)}{\\partial t} \\right \\vert_{t=0}\n",
    "$$\n",
    "\n",
    "### Influence Function\n",
    "\n",
    "Assuming that $T$ is Gateaux differentiable at $P$ then a function $\\psi:\\mathcal{X} \\rightarrow \\mathbb{R}$ which satisfies $T'(Q-P;P) = \\int \\psi(x; P)dQ(x)$ is known as the influence function:\n",
    "\n",
    "$$ \\psi(x, P) = T'(\\delta_x - P, P) =\\left. \\frac{\\partial T((1-t)P+t\\delta_x)}{\\partial t} \\right \\vert_{t=0}$$\n",
    "\n",
    "### Von Mises\n",
    "\n",
    "Following a generalization of the Taylor expansion to functionals, the true target quantity $T(Q)$ which we wish to estimate can be expressed as:\n",
    "\n",
    "$$\n",
    "T(Q) = T(P) + T'(Q-P;P) + R_2 = T(P) + \\int \\psi(x;P)dQ(x) + R_2\n",
    "$$\n",
    "\n",
    "In words, the true quantity is equal to the estimatable quantity plus the integral of the influence function and some higher order error term(s).\n",
    "\n",
    "Following a little substitution, the expression can be written as:\n",
    "\n",
    "$$\n",
    "T(q) = T(p) + \\phi ' \\left( \\int \\nu(p)\\right) \\int (q-p)\\nu ' (p) + R_2\n",
    "$$\n",
    "\n",
    "expanding the second term...\n",
    "\n",
    "$$\n",
    "T(q) = T(p) + \\phi ' \\left( \\int \\nu(p)\\right) \\left(  \\int q\\nu ' (p) - \\int p \\nu ' (p)  \\right)+ R_2\n",
    "$$\n",
    "\n",
    "\n",
    "### Estimating $T(q)$\n",
    "\n",
    "As we do not have access to $Q$ we can approximate it using samples from our dataset. This is where our data splitting will come in handy.\n",
    "\n",
    "The rest of the process is described in line with the code below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f5eb7f",
   "metadata": {},
   "source": [
    "###  Make some imports and define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b5ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd.functional as func\n",
    "import torch.autograd as grad\n",
    "from sklearn.neighbors import KernelDensity\n",
    "torch.pi = torch.tensor(torch.acos(torch.zeros(1)).item() * 2)\n",
    "\n",
    "# Set up an example for finding the Shannon Entropy of a Gaussian\n",
    "n = 10000\n",
    "true_mu = 0\n",
    "true_sigma = 1\n",
    "dx = 0.01  # as we are estimating densities with sums we will multiply by dx\n",
    "runs = 500\n",
    "\n",
    "def nu(a, dx):\n",
    "    return - a * torch.log(a/dx) \n",
    "\n",
    "def phi(b):\n",
    "    return b\n",
    "\n",
    "def entropy(p, dx):\n",
    "    return phi((nu(p, dx)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba9762d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Run 0 ======\n",
      "===== Run 10 ======\n",
      "===== Run 20 ======\n",
      "===== Run 30 ======\n",
      "===== Run 40 ======\n",
      "===== Run 50 ======\n",
      "===== Run 60 ======\n",
      "===== Run 70 ======\n",
      "===== Run 80 ======\n",
      "===== Run 90 ======\n",
      "===== Run 100 ======\n",
      "===== Run 110 ======\n",
      "===== Run 120 ======\n",
      "===== Run 130 ======\n",
      "===== Run 140 ======\n",
      "===== Run 150 ======\n",
      "===== Run 160 ======\n",
      "===== Run 170 ======\n",
      "===== Run 180 ======\n",
      "===== Run 190 ======\n",
      "===== Run 200 ======\n",
      "===== Run 210 ======\n",
      "===== Run 220 ======\n",
      "===== Run 230 ======\n",
      "===== Run 240 ======\n",
      "===== Run 250 ======\n",
      "===== Run 260 ======\n",
      "===== Run 270 ======\n",
      "===== Run 280 ======\n",
      "===== Run 290 ======\n",
      "===== Run 300 ======\n",
      "===== Run 310 ======\n",
      "===== Run 320 ======\n",
      "===== Run 330 ======\n",
      "===== Run 340 ======\n",
      "===== Run 350 ======\n",
      "===== Run 360 ======\n",
      "===== Run 370 ======\n",
      "===== Run 380 ======\n",
      "===== Run 390 ======\n",
      "===== Run 400 ======\n",
      "===== Run 410 ======\n",
      "===== Run 420 ======\n",
      "===== Run 430 ======\n",
      "===== Run 440 ======\n",
      "===== Run 450 ======\n",
      "===== Run 460 ======\n",
      "===== Run 470 ======\n",
      "===== Run 480 ======\n",
      "===== Run 490 ======\n"
     ]
    }
   ],
   "source": [
    "# calculate true entropy\n",
    "GT_psi = 0.5 * torch.log(2 * torch.pi * torch.exp(torch.tensor([1])) * true_sigma**2)\n",
    "    \n",
    "updated_psis = []\n",
    "naive_psis = []\n",
    "for i in range(runs):\n",
    "    if i % 10 == 0:\n",
    "        print('===== Run {} ======'.format(i))\n",
    "    x = ((torch.randn(n) + true_mu) * true_sigma).reshape(-1,1)\n",
    "\n",
    "    # data splits\n",
    "    x1 = x[:len(x)//2]\n",
    "    x2 = x[len(x)//2:]\n",
    "\n",
    "    # estimate density using first half of data\n",
    "    kde_ds1 = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(x1)\n",
    "\n",
    "    # estimate density using second half of data\n",
    "    kde_ds2 = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(x2)\n",
    "\n",
    "    # define domain x\n",
    "    r_ds1 = np.arange(-10, 10, dx).reshape(-1,1)\n",
    "    # get density of domain\n",
    "    p_r_ds1 = np.exp(kde_ds1.score_samples(r_ds1)) * dx\n",
    "    p_r_ds1 = torch.tensor(p_r_ds1)\n",
    "\n",
    "    # define domain x\n",
    "    r_ds2 = np.arange(-10, 10, dx).reshape(-1,1)\n",
    "    # get density of domain\n",
    "    p_r_ds2 = np.exp(kde_ds1.score_samples(r_ds2)) * dx\n",
    "    p_r_ds2 = torch.tensor(p_r_ds2)\n",
    "\n",
    "    # calculate estimated entropy\n",
    "    est_ds1 = entropy(p_r_ds1, dx)\n",
    "    est_ds2 = entropy(p_r_ds2, dx)\n",
    "    est = (est_ds1 + est_ds2) / 2\n",
    "    naive_psis.append(est)\n",
    "    \n",
    "    # A\n",
    "    int_nu_p_ds1 = nu(p_r_ds1, dx).sum()  # 1.\n",
    "    int_nu_p_ds1.requires_grad_(True)   # 2.\n",
    "    phi_int_nu_p_ds1 = phi(int_nu_p_ds1)  # 3.\n",
    "    phi_int_nu_p_ds1.backward(torch.ones(phi_int_nu_p_ds1.shape))  # 4.\n",
    "    A_ds1 = int_nu_p_ds1.grad.data  # 5.\n",
    "\n",
    "    p_xi_ds2 = torch.tensor(np.exp(kde_ds1.score_samples(x2))) * dx  # 1.\n",
    "    p_xi_ds2.requires_grad_(True)  # 2. \n",
    "    nu_p_xi_ds2 = nu(p_xi_ds2, dx)  # 3.\n",
    "    nu_p_xi_ds2.backward(torch.ones(nu_p_xi_ds2.shape))  # 4.\n",
    "    nu_pr_p_xi_ds2 = p_xi_ds2.grad.data  # 5.\n",
    "    B_ds1 = nu_pr_p_xi_ds2.mean()  # 6.\n",
    "\n",
    "    p_r_ds1.requires_grad_(True)  # 1.\n",
    "    nu_p_ds1 = nu(p_r_ds1, dx)  # 2.\n",
    "    nu_p_ds1.backward(torch.ones(nu_p_ds1.shape))  # 3.\n",
    "    nu_pr_p_ds1 = p_r_ds1.grad.data  # 4. \n",
    "    C_ds1 = (p_r_ds1 * nu_pr_p_ds1).sum()  # 5.\n",
    "\n",
    "    psi_ds1 = A_ds1 * (B_ds1 - C_ds1)\n",
    "\n",
    "    # PART 2\n",
    "\n",
    "    # A\n",
    "    int_nu_p_ds2 = nu(p_r_ds2, dx).sum()\n",
    "    int_nu_p_ds2.requires_grad_(True)\n",
    "    phi_int_nu_p_ds2 = phi(int_nu_p_ds2)\n",
    "    phi_int_nu_p_ds2.backward(torch.ones(phi_int_nu_p_ds2.shape))\n",
    "    A_ds2 = int_nu_p_ds2.grad.data\n",
    "\n",
    "    # B -> 1/n * sum ( nu_pr(p(x_i))) using ds 2\n",
    "\n",
    "    p_xi_ds1 = torch.tensor(np.exp(kde_ds2.score_samples(x1))) * dx\n",
    "    p_xi_ds1.requires_grad_(True)\n",
    "    nu_p_xi_ds1 = nu(p_xi_ds1, dx)\n",
    "    nu_p_xi_ds1.backward(torch.ones(nu_p_xi_ds1.shape))\n",
    "    nu_pr_p_xi_ds1 = p_xi_ds1.grad.data\n",
    "    B_ds2 = nu_pr_p_xi_ds1.mean()\n",
    "\n",
    "    # C\n",
    "    p_r_ds2.requires_grad_(True)\n",
    "    nu_p_ds2 = nu(p_r_ds2, dx)\n",
    "    nu_p_ds2.backward(torch.ones(nu_p_ds2.shape))\n",
    "    nu_pr_p_ds2 = p_r_ds2.grad.data\n",
    "    C_ds2 = (p_r_ds2 * nu_pr_p_ds2).sum()\n",
    "\n",
    "    psi_ds2 = A_ds2 * (B_ds2 - C_ds2)\n",
    "\n",
    "    psi = (psi_ds1 + psi_ds2) / 2\n",
    "\n",
    "    updated_est = est + psi\n",
    "    updated_psis.append(updated_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c54b4dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True psi:  1.4189385\n",
      "naive psi:  1.4375673514037324  relative bias: 1.312871107701906 %\n",
      "updated TMLE psi:  1.4203782  relative bias: 0.10146250715479255 %\n",
      "Reduction in bias: 1.2114086005471134 %\n"
     ]
    }
   ],
   "source": [
    "naive_psis = np.asarray(naive_psis)\n",
    "updated_psis = torch.FloatTensor(updated_psis).detach().numpy()\n",
    "GT_psi = GT_psi.numpy()[0]\n",
    "print('True psi: ', GT_psi)\n",
    "print('naive psi: ', naive_psis.mean(), ' relative bias:',\n",
    "      (naive_psis.mean() - GT_psi)/GT_psi * 100, '%')\n",
    "print('updated TMLE psi: ', updated_psis.mean(), ' relative bias:',\n",
    "      (updated_psis.mean() - GT_psi)/GT_psi * 100, '%')\n",
    "print('Reduction in bias:', np.abs(naive_psis.mean() - GT_psi)/GT_psi * 100 - \n",
    "     np.abs(updated_psis.mean() - GT_psi)/GT_psi * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb2e6605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive psi var: 9.849999785692589e-05\n",
      "updated psi var: 5.5809996e-05\n",
      "Average of reductions: 0.9019158318410758 %\n"
     ]
    }
   ],
   "source": [
    "# This takes the reduction in relative bias for each simulation first, then takes an average\n",
    "# (Owing to the nonlinearity of the ||x|| function, this gives different results which are\n",
    "# worth considering.)\n",
    "print('naive psi var:', naive_psis.var())\n",
    "print('updated psi var:', updated_psis.var())\n",
    "errors_naive = (naive_psis - GT_psi)/GT_psi *100\n",
    "errors_updated = (updated_psis - GT_psi)/GT_psi *100\n",
    "diff_errors = np.abs(errors_naive) - np.abs(errors_updated)\n",
    "print('Average of reductions:', diff_errors.mean(), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea11e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
