{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451559bc",
   "metadata": {},
   "source": [
    "# Generalising Influence Functions with Autograd\n",
    "\n",
    "This notebook is an implementation of the data splitting estimator for the Shannon Entropy in 'Influence Functions for Machine Learning: Nonparametric Estimators for Entropies, Divergences and Mutual Informations' by Kandasamy et al. 2015 https://arxiv.org/abs/1411.4342\n",
    "\n",
    "The process is the same as theirs except, instead of implementing the analytical form derived in the paper, we use autograd.\n",
    "\n",
    "#### Acknowledgements\n",
    "I'd like to thank Nic Ford, Sina Akbari, and Jalal Etesami for their patience in helping me work through this topic.\n",
    "\n",
    "### Form\n",
    "\n",
    "This work is concerned with functions of the form $$T(p) = \\phi \\left( \\int \\nu(p) d\\nu \\right)$$\n",
    "\n",
    "where $T(p)$ is the target functional we wish to estimate, and $p$ is a density.\n",
    "\n",
    "Notice already that Shannon entropy $-\\int p \\log p$ can be expressed in the same form. There are a couple of ways to do this, but one way is with $\\phi(p) = p$ (i.e. the identity function) and $\\nu(p) = p \\log p$. \n",
    "\n",
    "### Pathwise Derivative\n",
    "\n",
    "The data we collect enables us to estimate $T(p)$ but not the true population quantity $T(q)$. i.e. we have access to $P$ but not to $Q$. We assume that $T(p)$ is 'close enough' and lies on a path to $T(q)$. This allows us to define the pathwise or 'Gateaux' derivative as:\n",
    "\n",
    "$$\n",
    "T'(H; P) = \\left. \\frac{\\partial T(P+tH)}{\\partial t} \\right \\vert_{t=0}\n",
    "$$\n",
    "\n",
    "### Influence Function\n",
    "\n",
    "Assuming that $T$ is Gateaux differentiable at $P$ then a function $\\psi:\\mathcal{X} \\rightarrow \\mathbb{R}$ which satisfies $T'(Q-P;P) = \\int \\psi(x; P)dQ(x)$ is known as the influence function:\n",
    "\n",
    "$$ \\psi(x, P) = T'(\\delta_x - P, P) =\\left. \\frac{\\partial T((1-t)P+t\\delta_x)}{\\partial t} \\right \\vert_{t=0}$$\n",
    "\n",
    "### Von Mises\n",
    "\n",
    "Following a generalization of the Taylor expansion to functionals, the true target quantity $T(Q)$ which we wish to estimate can be expressed as:\n",
    "\n",
    "$$\n",
    "T(Q) = T(P) + T'(Q-P;P) + R_2 = T(P) + \\int \\psi(x;P)dQ(x) + R_2\n",
    "$$\n",
    "\n",
    "In words, the true quantity is equal to the estimatable quantity plus the integral of the influence function and some higher order error term(s).\n",
    "\n",
    "Following a little substitution, the expression can be written as:\n",
    "\n",
    "$$\n",
    "T(q) = T(p) + \\phi ' \\left( \\int \\nu(p)\\right) \\int (q-p)\\nu ' (p) + R_2\n",
    "$$\n",
    "\n",
    "expanding the second term...\n",
    "\n",
    "$$\n",
    "T(q) = T(p) + \\phi ' \\left( \\int \\nu(p)\\right) \\left(  \\int q\\nu ' (p) - \\int p \\nu ' (p)  \\right)+ R_2\n",
    "$$\n",
    "\n",
    "\n",
    "### Estimating $T(q)$\n",
    "\n",
    "As we do not have access to $Q$ we can approximate it using samples from our dataset. This is where our data splitting will come in handy.\n",
    "\n",
    "The rest of the process is described in line with the code below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f5eb7f",
   "metadata": {},
   "source": [
    "###  Make some imports and define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ada09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd.functional as func\n",
    "import torch.autograd as grad\n",
    "from sklearn.neighbors import KernelDensity\n",
    "torch.pi = torch.tensor(torch.acos(torch.zeros(1)).item() * 2)\n",
    "\n",
    "# Set up an example for finding the Shannon Entropy of a Gaussian\n",
    "n = 10000\n",
    "true_mu = 0\n",
    "true_sigma = 1\n",
    "dx = 0.01  # as we are estimating densities with sums we will multiply by dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5909cbab",
   "metadata": {},
   "source": [
    "### Create the data and the two data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c9abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ((torch.randn(n) + true_mu) * true_sigma).reshape(-1,1)\n",
    "\n",
    "# data splits\n",
    "x1 = x[:len(x)//2]\n",
    "x2 = x[len(x)//2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d43d1e2",
   "metadata": {},
   "source": [
    "### Derive KDEs for the two data splits\n",
    "\n",
    "Note that after fitting the kde functions, we also create a 'domain' for x 'r' and use it generate the density 'p_r' by taking the exponential of the sample scores for this domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8255e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check estimated density sums to 1:  0.9999999999998879\n",
      "check estimated density sums to 1:  0.9999999999998879\n"
     ]
    }
   ],
   "source": [
    "# estimate density using first half of data\n",
    "kde_ds1 = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(x1)\n",
    "\n",
    "# estimate density using second half of data\n",
    "kde_ds2 = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(x2)\n",
    "\n",
    "# define domain x\n",
    "r_ds1 = np.arange(-10, 10, dx).reshape(-1,1)\n",
    "# get density of domain\n",
    "p_r_ds1 = np.exp(kde_ds1.score_samples(r_ds1)) * dx\n",
    "print('check estimated density sums to 1: ', p_r_ds1.sum())\n",
    "p_r_ds1 = torch.tensor(p_r_ds1)\n",
    "\n",
    "# define domain x\n",
    "r_ds2 = np.arange(-10, 10, dx).reshape(-1,1)\n",
    "# get density of domain\n",
    "p_r_ds2 = np.exp(kde_ds1.score_samples(r_ds2)) * dx\n",
    "print('check estimated density sums to 1: ', p_r_ds2.sum())\n",
    "p_r_ds2 = torch.tensor(p_r_ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584f2567",
   "metadata": {},
   "source": [
    "### Define our functions\n",
    "Here we define $\\nu$, $\\phi$ and their combination $T$ which in this example is the Shannon entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e81e013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nu(a, dx):\n",
    "    return - a * torch.log(a/dx) \n",
    "\n",
    "def phi(b):\n",
    "    return b\n",
    "\n",
    "def entropy(p, dx):\n",
    "    return phi((nu(p, dx)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987d3a8",
   "metadata": {},
   "source": [
    "### Calculate the true entropy and the initial estimate\n",
    "The 'true entropy' is our ground truth for the example, and we calculate it using the analytically derived expression for the entropy of a Gaussian distribution.\n",
    "\n",
    "The initial estimates are greated using the data split density estimates derived above, and then averaged to provide 'est' which is the initial 'naive' estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6480001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True value:  1.4189385175704956\n",
      "Estimate:  1.4338736315801988\n",
      "Relative estimation error: 1.0525569915771484 %\n"
     ]
    }
   ],
   "source": [
    "# calculate true entropy\n",
    "true = 0.5 * torch.log(2 * torch.pi * torch.exp(torch.tensor([1])) * true_sigma**2)\n",
    "print('True value: ', true.item())\n",
    "\n",
    "# calculate estimated entropy\n",
    "est_ds1 = entropy(p_r_ds1, dx)\n",
    "est_ds2 = entropy(p_r_ds2, dx)\n",
    "est = (est_ds1 + est_ds2) / 2\n",
    "print('Estimate: ', est.item())\n",
    "print('Relative estimation error:', (torch.abs(est - true)/true * 100).item(), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134ec5b0",
   "metadata": {},
   "source": [
    "### Calculating the Integral of the Influence Function\n",
    "\n",
    "As above, the integral of the IF is defined as \n",
    "\n",
    "$$\n",
    "\\phi ' \\left(\\int(\\nu(p)\\right) \\left( \\int(q \\nu'(p)) - \\int(p\\nu'(p))\\right)\n",
    "$$\n",
    "\n",
    "We write this as $A(B-C)$ and compute A, B, and C one at a time.\n",
    "\n",
    "The process is repeated twice, once for the first datasplit, using the second data split in place of $Q$ where needed, and then we repeat this. The second time we swap the splits around, using the first data split in place of $Q$. If this sounds confusing, I have written further details below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c19d0",
   "metadata": {},
   "source": [
    "### Part 1A\n",
    "1. Here we begin by computing $\\int \\nu(p)$ using the density derived from the first datasplit.\n",
    "\n",
    "2. We then specify that the resulting estimate requires grad, so that it appears in the computational graph when taking the chain rule using pytorch's autograd library.\n",
    "\n",
    "3. When then run the estimate for $\\int \\nu(p)$ through our function $\\phi(.)$ (which in our case is the identity function but which has been included here for completeness and generality.\n",
    "\n",
    "4. Then we compute the gradient of the output w.r.t. the input\n",
    "\n",
    "5. The pytorch .grad method gives us the resulting value for $\\phi'(\\int \\nu (p) = A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41087562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A\n",
    "int_nu_p_ds1 = nu(p_r_ds1, dx).sum()  # 1.\n",
    "int_nu_p_ds1.requires_grad_(True)   # 2.\n",
    "phi_int_nu_p_ds1 = phi(int_nu_p_ds1)  # 3.\n",
    "phi_int_nu_p_ds1.backward(torch.ones(phi_int_nu_p_ds1.shape))  # 4.\n",
    "A_ds1 = int_nu_p_ds1.grad.data  # 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01417238",
   "metadata": {},
   "source": [
    "### Part 1B\n",
    "1. We need to compute $\\int q \\nu'(p)$. This is in the form of an expectation which we approximate using samples from the *other* datasplit. i.e. we use the kde density derived on the *first* datasplit, to compute sample probabilities for samples from the *second* datasplit. This gives us an approximation for $q$. A similar process is, for example, used in the computation of cross-entropy https://en.wikipedia.org/wiki/Cross_entropy#Estimation\n",
    "\n",
    "The first step in doing so is therefore to compute the probabilities of these samples. This is what happens in step 1. below.\n",
    "\n",
    "2. As before, specify that the resulting quantity from step 1. requires a gradient.\n",
    "\n",
    "3. Now we can compute the expectation for $\\nu()$ of the probabilities of these samples\n",
    "\n",
    "4. ... and compute the gradients through the graph fro $\\nu$. Note that we still haven't done anything with $\\nu'$, we need $\\nu$ first in order to then compute the gradient of the function evaluated at the respective sample probabilities\n",
    "\n",
    "5. Now we actulaly have $\\nu'(.)$ evaluated at the sample probabilities.\n",
    "\n",
    "6. We take an empirical average of this quantity to get B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b09013f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_xi_ds2 = torch.tensor(np.exp(kde_ds1.score_samples(x2))) * dx  # 1.\n",
    "p_xi_ds2.requires_grad_(True)  # 2. \n",
    "nu_p_xi_ds2 = nu(p_xi_ds2, dx)  # 3.\n",
    "nu_p_xi_ds2.backward(torch.ones(nu_p_xi_ds2.shape))  # 4.\n",
    "nu_pr_p_xi_ds2 = p_xi_ds2.grad.data  # 5.\n",
    "B_ds1 = nu_pr_p_xi_ds2.mean()  # 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0307abf",
   "metadata": {},
   "source": [
    "### Part 1C\n",
    "1. We need to compute $\\int p \\nu'(p)$. Some of these steps will be the same as above for the quantity $A$. Once again, we start by computing $\\nu (p)$ in order to then get the gradient at $p$. We therefore use the KDE estimated using the first data split, and specify that this requires a gradient.\n",
    "\n",
    "2. Now we compute $\\nu$ of this quantity \n",
    "\n",
    "3. ... and compute the gradients\n",
    "\n",
    "4. And derive the value for $\\nu'(.)$ evaluated at our density estimate for $p$.\n",
    "\n",
    "5. Finally we need to multiply the above quantity by our density estimate for $p$ and sum to acquire the integral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "024179e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_r_ds1.requires_grad_(True)  # 1.\n",
    "nu_p_ds1 = nu(p_r_ds1, dx)  # 2.\n",
    "nu_p_ds1.backward(torch.ones(nu_p_ds1.shape))  # 3.\n",
    "nu_pr_p_ds1 = p_r_ds1.grad.data  # 4. \n",
    "C_ds1 = (p_r_ds1 * nu_pr_p_ds1).sum()  # 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b7e78",
   "metadata": {},
   "source": [
    "### Part 1 - Conclusion\n",
    "Finally we put A, B, and C together to get the first data split estimate for the integral of the influence function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a09571d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_ds1 = A_ds1 * (B_ds1 - C_ds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23a47b4",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "We now repeat the whole process, but swapping instances of data split 1 for data split 2 and vice versa..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f4413c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2\n",
    "\n",
    "# A\n",
    "int_nu_p_ds2 = nu(p_r_ds2, dx).sum()\n",
    "int_nu_p_ds2.requires_grad_(True)\n",
    "phi_int_nu_p_ds2 = phi(int_nu_p_ds2)\n",
    "phi_int_nu_p_ds2.backward(torch.ones(phi_int_nu_p_ds2.shape))\n",
    "A_ds2 = int_nu_p_ds2.grad.data\n",
    "\n",
    "# B -> 1/n * sum ( nu_pr(p(x_i))) using ds 2\n",
    "\n",
    "p_xi_ds1 = torch.tensor(np.exp(kde_ds2.score_samples(x1))) * dx\n",
    "p_xi_ds1.requires_grad_(True)\n",
    "nu_p_xi_ds1 = nu(p_xi_ds1, dx)\n",
    "nu_p_xi_ds1.backward(torch.ones(nu_p_xi_ds1.shape))\n",
    "nu_pr_p_xi_ds1 = p_xi_ds1.grad.data\n",
    "B_ds2 = nu_pr_p_xi_ds1.mean()\n",
    "\n",
    "# C\n",
    "p_r_ds2.requires_grad_(True)\n",
    "nu_p_ds2 = nu(p_r_ds2, dx)\n",
    "nu_p_ds2.backward(torch.ones(nu_p_ds2.shape))\n",
    "nu_pr_p_ds2 = p_r_ds2.grad.data\n",
    "C_ds2 = (p_r_ds2 * nu_pr_p_ds2).sum()\n",
    "\n",
    "psi_ds2 = A_ds2 * (B_ds2 - C_ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ee2f56",
   "metadata": {},
   "source": [
    "### Part 3 - the final estimate of the integral of the Influence Function \n",
    "We can now put our two estimates of the influence function together to get the average over the full dataset. This is simply $\\hat \\psi_{DS} = 0.5(\\hat \\psi_{DS}^1 + \\hat \\psi_{DS}^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a073cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi = (psi_ds1 + psi_ds2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc7b2ca",
   "metadata": {},
   "source": [
    "### Updating our original Estimate\n",
    "Following the Von Mises expansion, our updated esimated is simply our old estimate plus the integral of the influence function. We should see that our relative estimation error has decreased..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4499e089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated estimate : 1.412238771407718\n",
      "Relative (naive) estimation error: 1.0525569915771484 %\n",
      "Relative (updated) estimation error: 0.4721698760986328 %\n"
     ]
    }
   ],
   "source": [
    "updated_est = est + psi\n",
    "print('Updated estimate :', updated_est.item())\n",
    "print('Relative (naive) estimation error:', (torch.abs(est - true)/true * 100).item(), '%')\n",
    "print('Relative (updated) estimation error:', (torch.abs(updated_est - true)/true * 100).item(), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b4dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
