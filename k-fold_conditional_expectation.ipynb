{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8cc07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b5048e",
   "metadata": {},
   "source": [
    "## Problem Setup:\n",
    "\n",
    "This example is taken from https://arxiv.org/abs/2107.00681 by Hines, Dukes, Diaz-Ordaz, and Vansteelandt (2021).\n",
    "\n",
    "$\\psi(P_0) = \\mathbb{E}[Y|X=x]$  is our target estimand - it is the conditional outcome mean.\n",
    "\n",
    "i.e. $$\\psi(P_0) = \\int y \\frac{f_{y,x}(y,x)}{f_x(x)} dy $$\n",
    "\n",
    "\n",
    "If we assume the distribution from which the densities $f$ derive has been perturbed by a point mass at $(\\tilde x, \\tilde y)$ then:\n",
    "\n",
    "i.e. $$\\psi(P_t) = \\int y \\frac{f_{y,x,t}(y,x)}{f_{x,t}(x)} dy $$\n",
    "\n",
    "where $f_{y,x,t}(y,x)$ and $f_{x,t}(x)$ are the joint and marginal densities of $(Y,X)$ and $(X)$ respectively, under the 'parametric submodel' $$P_t = t\\delta_x(\\tilde x) + (1-t)P_o$$ and where $\\delta_x(\\tilde x)$ denotes the Dirac delta function s.t. it gives the density of a point mass at $\\tilde x$, is zero everywhere else, and integrates to 1.\n",
    "\n",
    "For our densities we therefore have that:\n",
    "\n",
    "$$f_{y,x,t}(x,y) =  t \\delta_{x,y}(\\tilde x, \\tilde y) +  (1-t)f_{x,y}(x,y)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$f_{x,t}(x) =  t \\delta_{x}(\\tilde x) +  (1-t)f_{x}(x)$$\n",
    "\n",
    "For some single observation $\\tilde o$, the influence function can be written as:\n",
    "\n",
    "$$\\phi(\\tilde o, P) = \\left. \\frac{d\\psi(P_t)}{dt} \\right \\vert_{t=0} = \\int y \\left . \\frac{d}{dt} \\right \\vert_{t=0}\\frac{f_{x,y,t}(x,y)}{f_{x,t}(x)}dy$$\n",
    "\n",
    "Following the quotient rule:\n",
    "$$\\phi(\\tilde o, P) = \\int\\left .\\frac{y}{f(x)} \\frac{df_{x,y,t}(x,y)}{dt} \\right \\vert_{t=0} dy  - \\int\\left .\\frac{ydf_{x,t}(x)}{dt} \\frac{f_{x,y,t}(x,y)}{f(x)^2} \\right \\vert_{t=0} dy$$\n",
    "\n",
    "Now $$\\left. \\frac{df_{y,x,t}(x,y)}{dt}\\right \\vert_{t=0} =  \\delta_{x,y}(\\tilde x, \\tilde y) -f_{x,y}(x,y)$$\n",
    "\n",
    "and \n",
    "$$\\left. \\frac{df_{x,t}(x)}{dt} \\right \\vert_{t=0} =  \\delta_{x}(\\tilde x) -f_{x}(x)$$\n",
    "\n",
    "Therefore, with some manipulation and remembering that $\\delta_{x,y}(\\tilde x, \\tilde y) = \\delta_{x}(\\tilde x) \\delta_{y}(\\tilde y) $\n",
    "\n",
    "$$ \\phi(\\tilde o, P) = \\frac{\\delta_{x}(\\tilde x) }{f(\\tilde x)} \\left( \\tilde y - \\mathbb{E}[Y|X=\\tilde x]\\right )$$\n",
    "\n",
    "This is fine if $X$ is discrete, since in this case $\\delta_{x}(\\tilde x)$ is the indicator function $\\mathbb{1}_x(\\tilde x)$ which equals 1 when $x=\\tilde x$.\n",
    "\n",
    "Finally we want to update our initial estimate as:\n",
    "\n",
    "$$ \\psi(P) \\approx \\psi(P_n) + \\frac{1}{N}\\sum_{i=0}^N\\phi(\\tilde o_i, P_n)$$\n",
    "\n",
    "which follows the Von Mises process for approximating a functional at an unknown distribution as a sum of terms involving the functional at the current distribution plus some derivatives (in our case, pathwise derivative) and higher order terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41138c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that returns p(X=x) given the PMF p(X) and space of possible X\n",
    "def pmf(x, cats, p_cats):\n",
    "    ''' takes in set of possible outcomes 'cats' and corresponding probabilitys p_cats\n",
    "    output is probability of specified event 'x' '''\n",
    "    ind = np.where((cats==x).all(axis=1))[0][0]\n",
    "    return p_cats[ind]\n",
    "\n",
    "\n",
    "def data_gen(N, beta, seed):\n",
    "    np.random.seed(seed)\n",
    "    X0 = np.array([1.0]).repeat(N).reshape(-1, 1)\n",
    "    X1 = np.random.binomial(1, 0.5, N).reshape(-1, 1)\n",
    "    X2 = np.random.binomial(1, 0.5, N).reshape(-1, 1)\n",
    "    X3 = np.random.binomial(1, 0.5, N).reshape(-1, 1)\n",
    "    X4 = np.random.binomial(1, 0.5, N).reshape(-1, 1)\n",
    "    X = np.concatenate([X0, X1, X2, X3, X4, X2*X3], 1)\n",
    "    u =  0.5*np.random.randn(N)\n",
    "    y = np.dot(X, beta) + u\n",
    "    \n",
    "    return X[:,1:-1], X, y, u  # first output is the variables without the constant and interaction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4ffef",
   "metadata": {},
   "source": [
    "First intentionally misspecify the model by excluding the interaction term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb13489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loop: 1\n",
      "Running loop: 51\n",
      "Running loop: 101\n",
      "Running loop: 151\n",
      "Running loop: 201\n",
      "Running loop: 251\n",
      "Running loop: 301\n",
      "Running loop: 351\n",
      "Running loop: 401\n",
      "Running loop: 451\n"
     ]
    }
   ],
   "source": [
    "num_loops = 500\n",
    "k = 6\n",
    "N = 5000\n",
    "seed = 0\n",
    "# set our query value of X=x:\n",
    "cond_X_GT = np.array([1., 1., 0., 1., 0., 0.])  # note the last var has to be X2*X3\n",
    "\n",
    "cond_X_est = cond_X_GT[1:-1]  # excludes offset and interaction. \n",
    "beta = np.array([3.3, 0.6, 0.5, 0.9, 0.6, 1.0])  # set beta/coefficients to some values\n",
    "# we run multiple loops over a k-fold procedure to get an understanding of the variance\n",
    "# in bias reduction.\n",
    "\n",
    "biased_psis = []\n",
    "upd_psis = []\n",
    "for i in range(num_loops):\n",
    "    if i % 50 == 0:\n",
    "        print('Running loop:', i+1)\n",
    "    seed += 1\n",
    "    X, X_all, y, u = data_gen(N=N, beta=beta, seed=seed)  # generate the data\n",
    "    # simulated data gen process (without noise) for purposes of finding E(Y|X=x)\n",
    "    Psi_GT = np.dot(cond_X_GT, beta)\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    kf.get_n_splits(X)\n",
    "    psis = []\n",
    "    phis = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "\n",
    "        reg = LinearRegression().fit(X_train, y_train)\n",
    "        psi = reg.predict((cond_X_est).reshape(1, -1))\n",
    "        psis.append(psi)\n",
    "\n",
    "        # return the set of possible outcomes, and their counts for X_ds1\n",
    "        cats_train, counts_train = np.unique(X_train, axis=0,return_counts=True)\n",
    "        p_cats_train = counts_train / len(X_train)  # this returns the normalized probabilities for each possible outcome\n",
    "\n",
    "        all_phi = []\n",
    "        for i in range(len(X_test)):\n",
    "            y_tilde = y_test[i]\n",
    "            x_tilde = X_test[i]\n",
    "            EY_tildeX = reg.predict(x_tilde.reshape(1, -1))\n",
    "            y_Ey = y_tilde - EY_tildeX\n",
    "            if (x_tilde == cond_X_est).all():\n",
    "                delta_over_fx = 1 / pmf(x_tilde, cats_train, p_cats_train)\n",
    "            else:\n",
    "                delta_over_fx = 0\n",
    "            all_phi.append(delta_over_fx * y_Ey)\n",
    "\n",
    "        phis = np.asarray(all_phi)\n",
    "    phi = phis.mean()\n",
    "    psi = np.asarray(psis).mean()    \n",
    "    psi_est_updated = psi + phi\n",
    "    biased_psis.append(psi)\n",
    "    upd_psis.append(psi_est_updated)\n",
    "biased_psis = np.asarray(biased_psis)\n",
    "upd_psis = np.asarray(upd_psis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67bb441c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for misspecified outcome model....\n",
      "True psi:  4.8\n",
      "naive psi:  5.050779973366795  relative bias: 5.224582778474892 %\n",
      "updated TMLE psi:  4.798730222584823  relative bias: -0.026453696149508765 %\n",
      "Reduction in bias: 5.198129082325384 %\n"
     ]
    }
   ],
   "source": [
    "print('Results for misspecified outcome model....')\n",
    "\n",
    "print('True psi: ', Psi_GT)\n",
    "print('naive psi: ', biased_psis.mean(), ' relative bias:',\n",
    "      (biased_psis.mean() - Psi_GT)/Psi_GT * 100, '%')\n",
    "print('updated TMLE psi: ', upd_psis.mean(), ' relative bias:',\n",
    "      (upd_psis.mean() - Psi_GT)/Psi_GT * 100, '%')\n",
    "print('Reduction in bias:', np.abs(biased_psis.mean() - Psi_GT)/Psi_GT * 100 - \n",
    "     np.abs(upd_psis.mean() - Psi_GT)/Psi_GT * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0604caf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive psi var: 0.00030687451683603507\n",
      "updated psi var: 0.007279790011345534\n",
      "Average of reductions: 3.836451852456813 %\n"
     ]
    }
   ],
   "source": [
    "# This takes the reduction in relative bias for each simulation first, then takes an average\n",
    "# (Owing to the nonlinearity of the ||x|| function, this gives different results which are\n",
    "# worth considering.)\n",
    "print('naive psi var:', biased_psis.var())\n",
    "print('updated psi var:', upd_psis.var())\n",
    "errors_naive = (biased_psis - Psi_GT)/Psi_GT *100\n",
    "errors_updated = (upd_psis - Psi_GT)/Psi_GT *100\n",
    "diff_errors = np.abs(errors_naive) - np.abs(errors_updated)\n",
    "print('Average of reductions:', diff_errors.mean(), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558fc542",
   "metadata": {},
   "source": [
    "Now let's try again, but correctly specifying the outcome model by including the interaction terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "874354e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loop: 1\n",
      "Running loop: 51\n",
      "Running loop: 101\n",
      "Running loop: 151\n",
      "Running loop: 201\n",
      "Running loop: 251\n",
      "Running loop: 301\n",
      "Running loop: 351\n",
      "Running loop: 401\n",
      "Running loop: 451\n"
     ]
    }
   ],
   "source": [
    "\n",
    "biased_psis = []\n",
    "upd_psis = []\n",
    "for i in range(num_loops):\n",
    "    if i % 50 == 0:\n",
    "        print('Running loop:', i+1)\n",
    "    seed += 1\n",
    "    X, X_all, y, u = data_gen(N=N, beta=beta, seed=seed) \n",
    "    # simulated data gen process (without noise) for purposes of finding E(Y|X=x)\n",
    "    Psi_GT = np.dot(cond_X_GT, beta)\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    kf.get_n_splits(X_all)\n",
    "    psis = []\n",
    "    phis = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = X_all[train_index, 1:]\n",
    "        X_test = X_all[test_index, 1:]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "\n",
    "        reg = LinearRegression().fit(X_train, y_train)\n",
    "        psi = reg.predict((cond_X_GT[1:]).reshape(1, -1))\n",
    "        psis.append(psi)\n",
    "\n",
    "        # return the set of possible outcomes, and their counts for X_ds1\n",
    "        cats_train, counts_train = np.unique(X_train, axis=0,return_counts=True)\n",
    "        p_cats_train = counts_train / len(X_train)  # this returns the normalized probabilities for each possible outcome\n",
    "\n",
    "        all_phi = []\n",
    "        for i in range(len(X_test)):\n",
    "            y_tilde = y_test[i]\n",
    "            x_tilde = X_test[i]\n",
    "            EY_tildeX = reg.predict(x_tilde.reshape(1, -1))\n",
    "            y_Ey = y_tilde - EY_tildeX\n",
    "            if (x_tilde == cond_X_GT[1:]).all():\n",
    "                delta_over_fx = 1 / pmf(x_tilde, cats_train, p_cats_train)\n",
    "            else:\n",
    "                delta_over_fx = 0\n",
    "            all_phi.append(delta_over_fx * y_Ey)\n",
    "\n",
    "        phis = np.asarray(all_phi)\n",
    "    phi = phis.mean()\n",
    "    psi = np.asarray(psis).mean()    \n",
    "    psi_est_updated = psi + phi\n",
    "    biased_psis.append(psi)\n",
    "    upd_psis.append(psi_est_updated)\n",
    "biased_psis = np.asarray(biased_psis)\n",
    "upd_psis = np.asarray(upd_psis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad07e132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for correctly specified outcome model....\n",
      "True psi:  4.8\n",
      "naive psi:  4.80026775343152  relative bias: 0.005578196489997023 %\n",
      "updated TMLE psi:  4.7990347537751585  relative bias: -0.020109296350860156 %\n",
      "Reduction in bias: -0.014531099860863133 %\n"
     ]
    }
   ],
   "source": [
    "print('Results for correctly specified outcome model....')\n",
    "\n",
    "print('True psi: ', Psi_GT)\n",
    "print('naive psi: ', biased_psis.mean(), ' relative bias:',\n",
    "      (biased_psis.mean() - Psi_GT)/Psi_GT * 100, '%')\n",
    "print('updated TMLE psi: ', upd_psis.mean(), ' relative bias:',\n",
    "      (upd_psis.mean() - Psi_GT)/Psi_GT * 100, '%')\n",
    "print('Reduction in bias:', np.abs(biased_psis.mean() - Psi_GT)/Psi_GT * 100 - \n",
    "     np.abs(upd_psis.mean() - Psi_GT)/Psi_GT * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7638cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive psi var: 0.00031138339366269997\n",
      "updated psi var: 0.005507836869718758\n",
      "Average of reductions: -0.9150817980484894 %\n"
     ]
    }
   ],
   "source": [
    "# This takes the reduction in relative bias for each simulation first, then takes an average\n",
    "# (Owing to the nonlinearity of the ||x|| function, this gives different results which are\n",
    "# worth considering.)\n",
    "print('naive psi var:', biased_psis.var())\n",
    "print('updated psi var:', upd_psis.var())\n",
    "errors_naive = (biased_psis - Psi_GT)/Psi_GT *100\n",
    "errors_updated = (upd_psis - Psi_GT)/Psi_GT *100\n",
    "diff_errors = np.abs(errors_naive) - np.abs(errors_updated)\n",
    "print('Average of reductions:', diff_errors.mean(), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9b96d2",
   "metadata": {},
   "source": [
    "Note that when the models have been correctly specified, the IF actually makes things worse, and the variance of the updated estimator is also higher than the naive estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78117033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
